{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install -Uq catalyst==20.11"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from catalyst.utils import set_global_seed, get_device\n",
    "\n",
    "\n",
    "set_global_seed(42)\n",
    "device = get_device()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Seminar\n",
    "\n",
    "Hey! Today we are going to learn a recommendation system basis. We'll introduce metrics, an example dataset and couple of recommendation systems. \n",
    "\n",
    "Move on!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Metrics\n",
    "\n",
    "Our example will be this. We have 6 documents, and our model predict some order on it. For example, we gave some users to say how relevant were these documents. Model prediction is `order`, and human score is `rel_score`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "order = np.array([1, 2, 3, 4, 5, 6])\n",
    "rel_score = np.array([3, 2, 3, 0, 1, 2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How good is our model? Check by Discounted Cumulative Gain and HitRate."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DCG\n",
    "\n",
    "It's most popular way to understand system perfomance. It's computed by formula:\n",
    "\n",
    "$$\n",
    "\\mathrm{DCG_{p}} = rel_1 + \\sum_{i=2}^{p} \\frac{rel_{i}}{\\log_{2}(i+1)}\n",
    "$$\n",
    "\n",
    "Implement it!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DCG_3: 5.7618595071429155, DCG_6: 6.861126688593501\n"
     ]
    }
   ],
   "source": [
    "DCG_3 = rel_score[0] + np.sum(rel_score[1:3] / np.log2(order[1:3] + 1))\n",
    "DCG_6 = rel_score[0] + np.sum(rel_score[1:] / np.log2(order[1:] + 1))\n",
    "assert np.isclose(DCG_3, 5.7618595)\n",
    "assert np.isclose(DCG_6, 6.8611266)\n",
    "print(f\"DCG_3: {DCG_3}, DCG_6: {DCG_6}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The gain formula can be changed to the exponantial form. And we will get another DCG formulation.\n",
    "\n",
    "$$\n",
    "\\mathrm{DCG_{p}} = \\sum_{i=1}^{p} \\frac{2^{\\text{rel}_{i}} - 1}{\\log_{2}(i+1)}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exponantial DCG_6: 13.848263629272981\n"
     ]
    }
   ],
   "source": [
    "DCG_6 = np.sum((2**rel_score - 1) / np.log2(order + 1))\n",
    "assert np.isclose(DCG_6, 13.8482636)\n",
    "print(f\"Exponantial DCG_6: {DCG_6}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Usually Normal DCG is used. Formula:\n",
    "\n",
    "$$ \n",
    "\\mathrm{nDCG_{p}} = \\frac{DCG_{p}}{IDCG_{p}}\n",
    "$$\n",
    "\n",
    "IDCG is ideal DCG. It's calculated when system order is gotten by human relevance score:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "ideal_order = np.array([1, 4, 2, 6, 5, 3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "IDCG_6: 14.595390756454924\n"
     ]
    }
   ],
   "source": [
    "IDCG_6 = np.sum((2**rel_score - 1) / np.log2(ideal_order + 1))\n",
    "assert np.isclose(IDCG_6, 14.59539075)\n",
    "print(f\"IDCG_6: {IDCG_6}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NDCG_6: 0.9488107485678985\n"
     ]
    }
   ],
   "source": [
    "NDCG_6 = DCG_6 / IDCG_6\n",
    "assert np.isclose(NDCG_6, 0.9488107)\n",
    "print(f\"NDCG_6: {NDCG_6}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "There is a implemented function to calculate ndcg in Catalyst."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "from catalyst.metrics import ndcg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "our_score = 1 / order # Higher score – higher raiting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NDCG: (tensor([0.7789]), tensor([0.9595]), tensor([0.9488]))\n"
     ]
    }
   ],
   "source": [
    "t_our_score = torch.tensor([our_score])\n",
    "t_rel_score = torch.tensor([rel_score])\n",
    "\n",
    "print(f\"NDCG: {ndcg(t_our_score, t_rel_score, top_k=[2, 3, 6])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Hit Rate\n",
    "\n",
    "Another way to get the system performance is HitRate. To calculate it, we need to count how many times an item from the system order is relevent for user. Example:"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "one_user_rel_score = rel_score // 3 # <-- only two documents are relevent for one user\n",
    "print(f\"New rel_score: {one_user_rel_score}\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "hitrate = one_user_rel_score.mean()\n",
    "assert np.isclose(hitrate, 0.33333)\n",
    "print(f\"HitRate: {hitrate}\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from catalyst.metrics import hitrate"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "one_user_t_rel_score = t_rel_score // 3"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "print(f\"HitRate: {hitrate(t_our_score, one_user_t_rel_score)}\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Movie Lens Dataset\n",
    "\n",
    "\n",
    "MovieLens Dataset contains users score of some movies. `0` means that an user hasn't set raiting. An user can set raiting from `1` to `5`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New rel_score: [1 0 1 0 0 0]\n"
     ]
    }
   ],
   "source": [
    "one_user_rel_score = rel_score // 3 # <-- only two documents are relevent for one user\n",
    "print(f\"New rel_score: {one_user_rel_score}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HitRate: 0.3333333333333333\n"
     ]
    }
   ],
   "source": [
    "hitrate = one_user_rel_score.mean()\n",
    "assert np.isclose(hitrate, 0.33333)\n",
    "print(f\"HitRate: {hitrate}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "from catalyst.metrics import hitrate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "one_user_t_rel_score = t_rel_score // 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HitRate: tensor([0.3333])\n"
     ]
    }
   ],
   "source": [
    "print(f\"HitRate: {hitrate(t_our_score, one_user_t_rel_score)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Movie Lens Dataset\n",
    "\n",
    "\n",
    "MovieLens Dataset contains users score of some movies. `0` means that an user hasn't set raiting. An user can set raiting from `1` to `5`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from catalyst.contrib.datasets import MovieLens\n",
    "\n",
    "\n",
    "train_dataset = MovieLens(root=\".\", train=True, download=True)\n",
    "test_dataset = MovieLens(root=\".\", train=False, download=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll try to train model to find high scored unseed movies. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import typing as tp\n",
    "\n",
    "from catalyst.utils import get_loader\n",
    "\n",
    "\n",
    "def dist_transform(row: tp.Dict[str, tp.Any]) -> tp.Dict[str, tp.Any]:\n",
    "    raitings = row[\"raitings\"]\n",
    "    movie_ids = torch.arange(raitings.size(0))[raitings > 0]\n",
    "    user_ids = (\n",
    "        torch.zeros_like(movie_ids).type(torch.LongTensor) + row[\"user_id\"]\n",
    "    )\n",
    "    targets = (raitings[raitings > 0] / 5.0).type(torch.FloatTensor)\n",
    "    return {\"user_ids\": user_ids, \"movie_ids\": movie_ids, \"targets\": targets}\n",
    "\n",
    "\n",
    "def collate_fn(\n",
    "    batch: tp.Sequence[tp.Dict[str, torch.Tensor]]\n",
    ") -> tp.Dict[str, torch.Tensor]:\n",
    "    user_ids = torch.cat([b[\"user_ids\"] for b in batch])\n",
    "    movie_ids = torch.cat([b[\"movie_ids\"] for b in batch])\n",
    "    targets = torch.cat([b[\"targets\"] for b in batch])\n",
    "    return {\"user_ids\": user_ids, \"movie_ids\": movie_ids, \"targets\": targets}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_indexes = torch.arange(len(train_dataset))\n",
    "\n",
    "train_dataloader = get_loader(\n",
    "    user_indexes,\n",
    "    open_fn=lambda x: {\"user_id\": x, \"raitings\": train_dataset[x]},\n",
    "    dict_transform=dist_transform,\n",
    "    batch_size=1,\n",
    "    num_workers=4,\n",
    "    shuffle=True,\n",
    "    drop_last=True,\n",
    "    collate_fn=collate_fn\n",
    ")\n",
    "\n",
    "valid_dataloader = get_loader(\n",
    "    user_indexes,\n",
    "    open_fn=lambda x: {\"user_id\": x, \"raitings\": test_dataset[x]},\n",
    "    dict_transform=dist_transform,\n",
    "    batch_size=1,\n",
    "    num_workers=4,\n",
    "    shuffle=True,\n",
    "    drop_last=True,\n",
    "    collate_fn=collate_fn\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Funk SVD\n",
    "\n",
    "\n",
    "A first method it's SVD base. Instead of calculating true SVD matrices, we will find them by fitting!\n",
    "\n",
    "These implementation based on this [medium post](https://medium.com/datadriveninvestor/how-funk-singular-value-decomposition-algorithm-work-in-recommendation-engines-36f2fbf62cac)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "\n",
    "class FunkSVD(nn.Module):\n",
    "    def __init__(self, user_num: int, item_num: int, embedding_dim: int):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.user_embeddings = nn.Embedding(user_num, embedding_dim)\n",
    "        self.item_embeddings = nn.Embedding(item_num, embedding_dim)\n",
    "        \n",
    "        self.user_bias = nn.Embedding(user_num, 1)\n",
    "        self.item_bias = nn.Embedding(item_num, 1)\n",
    "        \n",
    "        self.bias = torch.nn.Parameter(torch.zeros(1))\n",
    "        self.embedding_dim = embedding_dim\n",
    "            \n",
    "    def forward(\n",
    "        self, user_ids: torch.Tensor, movie_ids: torch.Tensor\n",
    "    ) -> torch.Tensor:\n",
    "        user_embedding = self.user_embeddings(user_ids)\n",
    "        user_bias = self.user_bias(user_ids).reshape(-1)\n",
    "        item_embedding = self.item_embeddings(movie_ids)\n",
    "        item_bias = self.item_bias(movie_ids).reshape(-1)\n",
    "        dot = torch.einsum(\"oi,oj->o\", user_embedding, item_embedding)\n",
    "        output = dot + user_bias + item_bias + self.bias\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from catalyst.contrib.nn import RAdam\n",
    "\n",
    "\n",
    "model = FunkSVD(len(train_dataset), len(train_dataset[0]), 16)\n",
    "optimizer = RAdam(model.parameters(), lr=1e-1)\n",
    "criterion = nn.BCEWithLogitsLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from catalyst.dl import SupervisedRunner, Callback, CallbackOrder, SupervisedRunner, IRunner\n",
    "\n",
    "\n",
    "class RankMetricCallback(Callback):\n",
    "    def __init__(self):\n",
    "        super().__init__(CallbackOrder.Metric)\n",
    "        \n",
    "        self.top_k = [1, 3, 5]\n",
    "\n",
    "    def on_batch_end(self, runner: IRunner):\n",
    "        # In every batch we have only one user.\n",
    "        targets = runner.input[\"targets\"].to(\"cpu\")\n",
    "        logits = runner.output[\"logits\"].to(\"cpu\")\n",
    "        \n",
    "        sorted_indeces = torch.argsort(logits, descending=True)\n",
    "        targets = torch.take(targets, sorted_indeces).reshape(1, -1)\n",
    "        logits = torch.take(logits, sorted_indeces).reshape(1, -1)\n",
    "        \n",
    "        ndcg_values = ndcg(logits, targets, top_k=self.top_k)\n",
    "        runner.batch_metrics.update(\n",
    "            {f\"ndcg_{k}\": v for k, v in zip(self.top_k, ndcg_values)}\n",
    "        )\n",
    "\n",
    "        \n",
    "runner = SupervisedRunner(input_key=[\"user_ids\", \"movie_ids\"], )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from catalyst.callbacks import ControlFlowCallback, OptimizerCallback\n",
    "\n",
    "\n",
    "callbacks = [\n",
    "    RankMetricCallback(),\n",
    "    OptimizerCallback(\n",
    "        accumulation_steps=64\n",
    "    ),\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/10 * Epoch (train):   8% 74/943 [00:22<02:39,  5.44it/s, loss=9.876, ndcg_1=0.516, ndcg_3=0.431, ndcg_5=0.431]   "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/adchumachenko/.local/lib/python3.6/site-packages/catalyst/contrib/nn/optimizers/radam.py:85: UserWarning:\n",
      "\n",
      "This overload of addcmul_ is deprecated:\n",
      "\taddcmul_(Number value, Tensor tensor1, Tensor tensor2)\n",
      "Consider using one of the following signatures instead:\n",
      "\taddcmul_(Tensor tensor1, Tensor tensor2, *, Number value) (Triggered internally at  /pytorch/torch/csrc/utils/python_arg_parser.cpp:882.)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/10 * Epoch (train): 100% 943/943 [00:37<00:00, 25.19it/s, loss=1.049, ndcg_1=0.741, ndcg_3=0.741, ndcg_5=0.712] \n",
      "1/10 * Epoch (valid): 100% 943/943 [00:10<00:00, 86.37it/s, loss=1.217, ndcg_1=0.516, ndcg_3=0.613, ndcg_5=0.667] \n",
      "[2020-11-20 19:00:29,891] \n",
      "1/10 * Epoch 1 (_base): lr=0.1000 | momentum=0.9000\n",
      "1/10 * Epoch 1 (train): loss=3.7382 | ndcg_1=0.6218 | ndcg_3=0.6231 | ndcg_5=0.6316\n",
      "1/10 * Epoch 1 (valid): loss=2.6834 | ndcg_1=0.6920 | ndcg_3=0.7292 | ndcg_5=0.7717\n",
      "2/10 * Epoch (train): 100% 943/943 [00:17<00:00, 54.72it/s, loss=1.242, ndcg_1=0.741, ndcg_3=0.662, ndcg_5=0.671] \n",
      "2/10 * Epoch (valid): 100% 943/943 [00:10<00:00, 90.29it/s, loss=1.550, ndcg_1=0.516, ndcg_3=0.653, ndcg_5=0.656] \n",
      "[2020-11-20 19:00:57,602] \n",
      "2/10 * Epoch 2 (_base): lr=0.1000 | momentum=0.9000\n",
      "2/10 * Epoch 2 (train): loss=2.5161 | ndcg_1=0.6204 | ndcg_3=0.6233 | ndcg_5=0.6289\n",
      "2/10 * Epoch 2 (valid): loss=1.8496 | ndcg_1=0.6961 | ndcg_3=0.7335 | ndcg_5=0.7771\n",
      "3/10 * Epoch (train): 100% 943/943 [00:17<00:00, 55.47it/s, loss=1.507, ndcg_1=0.741, ndcg_3=0.642, ndcg_5=0.559]\n",
      "3/10 * Epoch (valid): 100% 943/943 [00:10<00:00, 86.73it/s, loss=1.648, ndcg_1=1.000, ndcg_3=0.743, ndcg_5=0.705] \n",
      "[2020-11-20 19:01:25,570] \n",
      "3/10 * Epoch 3 (_base): lr=0.1000 | momentum=0.9000\n",
      "3/10 * Epoch 3 (train): loss=1.7650 | ndcg_1=0.6172 | ndcg_3=0.6199 | ndcg_5=0.6281\n",
      "3/10 * Epoch 3 (valid): loss=1.3667 | ndcg_1=0.6964 | ndcg_3=0.7391 | ndcg_5=0.7801\n",
      "4/10 * Epoch (train): 100% 943/943 [00:17<00:00, 54.85it/s, loss=1.593, ndcg_1=0.149, ndcg_3=0.338, ndcg_5=0.417]\n",
      "4/10 * Epoch (valid): 100% 943/943 [00:10<00:00, 87.65it/s, loss=1.606, ndcg_1=0.696, ndcg_3=0.634, ndcg_5=0.712] \n",
      "[2020-11-20 19:01:53,624] \n",
      "4/10 * Epoch 4 (_base): lr=0.1000 | momentum=0.9000\n",
      "4/10 * Epoch 4 (train): loss=1.3350 | ndcg_1=0.6258 | ndcg_3=0.6287 | ndcg_5=0.6306\n",
      "4/10 * Epoch 4 (valid): loss=1.1308 | ndcg_1=0.6813 | ndcg_3=0.7358 | ndcg_5=0.7781\n",
      "5/10 * Epoch (train): 100% 943/943 [00:16<00:00, 56.90it/s, loss=0.498, ndcg_1=0.741, ndcg_3=0.879, ndcg_5=0.807]\n",
      "5/10 * Epoch (valid): 100% 943/943 [00:10<00:00, 86.69it/s, loss=0.535, ndcg_1=0.516, ndcg_3=0.712, ndcg_5=0.792] \n",
      "[2020-11-20 19:02:21,178] \n",
      "5/10 * Epoch 5 (_base): lr=0.1000 | momentum=0.9000\n",
      "5/10 * Epoch 5 (train): loss=1.0991 | ndcg_1=0.6385 | ndcg_3=0.6319 | ndcg_5=0.6360\n",
      "5/10 * Epoch 5 (valid): loss=0.9744 | ndcg_1=0.6885 | ndcg_3=0.7374 | ndcg_5=0.7795\n",
      "6/10 * Epoch (train): 100% 943/943 [00:17<00:00, 55.20it/s, loss=0.356, ndcg_1=1.000, ndcg_3=1.000, ndcg_5=0.962]\n",
      "6/10 * Epoch (valid): 100% 943/943 [00:10<00:00, 87.16it/s, loss=0.405, ndcg_1=1.000, ndcg_3=0.800, ndcg_5=0.822]\n",
      "[2020-11-20 19:02:49,214] \n",
      "6/10 * Epoch 6 (_base): lr=0.1000 | momentum=0.9000\n",
      "6/10 * Epoch 6 (train): loss=0.9466 | ndcg_1=0.6347 | ndcg_3=0.6356 | ndcg_5=0.6396\n",
      "6/10 * Epoch 6 (valid): loss=0.8683 | ndcg_1=0.6957 | ndcg_3=0.7436 | ndcg_5=0.7846\n",
      "7/10 * Epoch (train): 100% 943/943 [00:17<00:00, 54.83it/s, loss=0.589, ndcg_1=0.741, ndcg_3=0.674, ndcg_5=0.727]\n",
      "7/10 * Epoch (valid): 100% 943/943 [00:10<00:00, 87.78it/s, loss=0.646, ndcg_1=0.320, ndcg_3=0.620, ndcg_5=0.626]\n",
      "[2020-11-20 19:03:17,272] \n",
      "7/10 * Epoch 7 (_base): lr=0.1000 | momentum=0.9000\n",
      "7/10 * Epoch 7 (train): loss=0.8448 | ndcg_1=0.6375 | ndcg_3=0.6349 | ndcg_5=0.6406\n",
      "7/10 * Epoch 7 (valid): loss=0.7896 | ndcg_1=0.7000 | ndcg_3=0.7470 | ndcg_5=0.7889\n",
      "8/10 * Epoch (train): 100% 943/943 [00:16<00:00, 55.83it/s, loss=0.852, ndcg_1=1.000, ndcg_3=0.657, ndcg_5=0.619]\n",
      "8/10 * Epoch (valid): 100% 943/943 [00:10<00:00, 86.74it/s, loss=1.001, ndcg_1=0.516, ndcg_3=0.825, ndcg_5=0.821] \n",
      "[2020-11-20 19:03:45,147] \n",
      "8/10 * Epoch 8 (_base): lr=0.1000 | momentum=0.9000\n",
      "8/10 * Epoch 8 (train): loss=0.7757 | ndcg_1=0.6456 | ndcg_3=0.6412 | ndcg_5=0.6478\n",
      "8/10 * Epoch 8 (valid): loss=0.7402 | ndcg_1=0.7034 | ndcg_3=0.7530 | ndcg_5=0.7930\n",
      "9/10 * Epoch (train): 100% 943/943 [00:17<00:00, 55.07it/s, loss=0.737, ndcg_1=0.516, ndcg_3=0.569, ndcg_5=0.584]\n",
      "9/10 * Epoch (valid): 100% 943/943 [00:10<00:00, 85.94it/s, loss=0.573, ndcg_1=0.741, ndcg_3=0.871, ndcg_5=0.901]\n",
      "[2020-11-20 19:04:13,367] \n",
      "9/10 * Epoch 9 (_base): lr=0.1000 | momentum=0.9000\n",
      "9/10 * Epoch 9 (train): loss=0.7271 | ndcg_1=0.6522 | ndcg_3=0.6458 | ndcg_5=0.6569\n",
      "9/10 * Epoch 9 (valid): loss=0.7089 | ndcg_1=0.7065 | ndcg_3=0.7573 | ndcg_5=0.7977\n",
      "10/10 * Epoch (train): 100% 943/943 [00:16<00:00, 56.06it/s, loss=0.836, ndcg_1=1.000, ndcg_3=0.847, ndcg_5=0.846]\n",
      "10/10 * Epoch (valid): 100% 943/943 [00:10<00:00, 87.56it/s, loss=1.353, ndcg_1=0.431, ndcg_3=0.692, ndcg_5=0.761]\n",
      "[2020-11-20 19:04:41,084] \n",
      "10/10 * Epoch 10 (_base): lr=0.1000 | momentum=0.9000\n",
      "10/10 * Epoch 10 (train): loss=0.7009 | ndcg_1=0.6573 | ndcg_3=0.6524 | ndcg_5=0.6644\n",
      "10/10 * Epoch 10 (valid): loss=0.6898 | ndcg_1=0.7203 | ndcg_3=0.7665 | ndcg_5=0.8022\n",
      "Top best models:\n",
      "logs/20201120-185938/checkpoints/train.10.pth\t0.6898\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "\n",
    "runner.train(\n",
    "    model=model,\n",
    "    optimizer=optimizer,\n",
    "    loaders={\"train\": train_dataloader, \"valid\": valid_dataloader},\n",
    "    criterion=criterion,\n",
    "    callbacks=callbacks,\n",
    "    logdir=Path(\"logs\") / datetime.now().strftime(\"%Y%m%d-%H%M%S\"),\n",
    "    num_epochs=10,\n",
    "    verbose=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural Collaborative Filtering\n",
    "\n",
    "\n",
    "Second method it's calculating user and item embeddings. To score user-item pair relevance, we aare going to concatinating vectors and pass forward through a neural network.\n",
    "\n",
    "This method based on NCF article: [arxiv](https://arxiv.org/pdf/1708.05031.pdf). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NСF(nn.Module):\n",
    "    def __init__(\n",
    "        self, user_num: int, item_num: int, embedding_dim: int, hidden_dim: int\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        self.user_embeddings = nn.Embedding(user_num, embedding_dim)\n",
    "        self.item_embeddings = nn.Embedding(item_num, embedding_dim)\n",
    "\n",
    "        self.layers = nn.Sequential(\n",
    "            nn.Linear(2 * embedding_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, 1),\n",
    "        )\n",
    "\n",
    "    def forward(\n",
    "        self, user_ids: torch.Tensor, movie_ids: torch.Tensor\n",
    "    ) -> torch.Tensor:\n",
    "        user_embedding = self.user_embeddings(user_ids)\n",
    "        item_embedding = self.item_embeddings(movie_ids)\n",
    "        concat = torch.cat((user_embedding, item_embedding), -1)\n",
    "        return self.layers(concat).view(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from catalyst.contrib.nn import RAdam\n",
    "\n",
    "\n",
    "model = NСF(len(train_dataset), len(train_dataset[0]), 64, 64)\n",
    "optimizer = RAdam(model.parameters(), lr=1e-2)\n",
    "criterion = nn.BCEWithLogitsLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from catalyst.dl import SupervisedRunner\n",
    "\n",
    "\n",
    "runner = SupervisedRunner(input_key=[\"user_ids\", \"movie_ids\"], )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/5 * Epoch (train): 100% 943/943 [00:19<00:00, 49.54it/s, loss=0.533, ndcg_1=0.741, ndcg_3=0.735, ndcg_5=0.775]\n",
      "1/5 * Epoch (valid): 100% 943/943 [00:12<00:00, 77.02it/s, loss=0.530, ndcg_1=0.516, ndcg_3=0.696, ndcg_5=0.763]\n",
      "[2020-11-20 19:05:12,723] \n",
      "1/5 * Epoch 1 (_base): lr=0.0100 | momentum=0.9000\n",
      "1/5 * Epoch 1 (train): loss=0.6310 | ndcg_1=0.6027 | ndcg_3=0.6224 | ndcg_5=0.6310\n",
      "1/5 * Epoch 1 (valid): loss=0.6001 | ndcg_1=0.7138 | ndcg_3=0.7496 | ndcg_5=0.7884\n",
      "2/5 * Epoch (train): 100% 943/943 [00:19<00:00, 49.40it/s, loss=0.585, ndcg_1=0.320, ndcg_3=0.604, ndcg_5=0.705]\n",
      "2/5 * Epoch (valid): 100% 943/943 [00:12<00:00, 78.57it/s, loss=0.642, ndcg_1=0.516, ndcg_3=0.608, ndcg_5=0.659]\n",
      "[2020-11-20 19:05:43,853] \n",
      "2/5 * Epoch 2 (_base): lr=0.0100 | momentum=0.9000\n",
      "2/5 * Epoch 2 (train): loss=0.6091 | ndcg_1=0.6339 | ndcg_3=0.6519 | ndcg_5=0.6596\n",
      "2/5 * Epoch 2 (valid): loss=0.5948 | ndcg_1=0.7349 | ndcg_3=0.7720 | ndcg_5=0.8050\n",
      "3/5 * Epoch (train): 100% 943/943 [00:18<00:00, 49.84it/s, loss=0.660, ndcg_1=0.149, ndcg_3=0.601, ndcg_5=0.673]\n",
      "3/5 * Epoch (valid): 100% 943/943 [00:12<00:00, 77.02it/s, loss=0.552, ndcg_1=0.516, ndcg_3=0.712, ndcg_5=0.727]\n",
      "[2020-11-20 19:06:15,164] \n",
      "3/5 * Epoch 3 (_base): lr=0.0100 | momentum=0.9000\n",
      "3/5 * Epoch 3 (train): loss=0.6047 | ndcg_1=0.7499 | ndcg_3=0.7327 | ndcg_5=0.7272\n",
      "3/5 * Epoch 3 (valid): loss=0.5911 | ndcg_1=0.7608 | ndcg_3=0.7893 | ndcg_5=0.8142\n",
      "4/5 * Epoch (train): 100% 943/943 [00:18<00:00, 49.97it/s, loss=0.690, ndcg_1=0.741, ndcg_3=0.879, ndcg_5=0.840]\n",
      "4/5 * Epoch (valid): 100% 943/943 [00:12<00:00, 78.00it/s, loss=0.735, ndcg_1=0.696, ndcg_3=0.786, ndcg_5=0.752]\n",
      "[2020-11-20 19:06:46,211] \n",
      "4/5 * Epoch 4 (_base): lr=0.0100 | momentum=0.9000\n",
      "4/5 * Epoch 4 (train): loss=0.6026 | ndcg_1=0.8056 | ndcg_3=0.7759 | ndcg_5=0.7739\n",
      "4/5 * Epoch 4 (valid): loss=0.5886 | ndcg_1=0.7754 | ndcg_3=0.7991 | ndcg_5=0.8246\n",
      "5/5 * Epoch (train): 100% 943/943 [00:18<00:00, 49.93it/s, loss=0.519, ndcg_1=0.741, ndcg_3=0.879, ndcg_5=0.811]\n",
      "5/5 * Epoch (valid): 100% 943/943 [00:11<00:00, 80.46it/s, loss=0.503, ndcg_1=1.000, ndcg_3=0.939, ndcg_5=0.918]\n",
      "[2020-11-20 19:07:16,926] \n",
      "5/5 * Epoch 5 (_base): lr=0.0100 | momentum=0.9000\n",
      "5/5 * Epoch 5 (train): loss=0.5995 | ndcg_1=0.8165 | ndcg_3=0.8026 | ndcg_5=0.7980\n",
      "5/5 * Epoch 5 (valid): loss=0.5857 | ndcg_1=0.7937 | ndcg_3=0.8107 | ndcg_5=0.8337\n",
      "Top best models:\n",
      "logs/20201120-190441/checkpoints/train.5.pth\t0.5857\n"
     ]
    }
   ],
   "source": [
    "runner.train(\n",
    "    model=model,\n",
    "    optimizer=optimizer,\n",
    "    loaders={\"train\": train_dataloader, \"valid\": valid_dataloader},\n",
    "    criterion=criterion,\n",
    "    callbacks=callbacks,\n",
    "    logdir=Path(\"logs\") / datetime.now().strftime(\"%Y%m%d-%H%M%S\"),\n",
    "    num_epochs=5,\n",
    "    verbose=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}