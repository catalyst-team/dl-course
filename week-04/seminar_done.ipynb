{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "seminar_done",
      "provenance": [],
      "collapsed_sections": [
        "YoZUN7H0h8X5",
        "bQcWQq6-yDeh",
        "1XQFtAKeh8Vf",
        "1WEChB2vnL87"
      ]
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BlJW0uN7h8aC"
      },
      "source": [
        "# 4 Seminar\n",
        "\n",
        "Hi! Today we are going to learn about image segmentation and object detection. We'll write and train some simple neural models for image segmentation, and look how object detection models work.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LB63wvFvW66a"
      },
      "source": [
        "!pip install catalyst\n",
        "from catalyst.utils import set_global_seed\n",
        "set_global_seed(42)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uKfCJVszWolP"
      },
      "source": [
        "## Segmentation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YoZUN7H0h8X5"
      },
      "source": [
        "### Dataset\n",
        "\n",
        "Look at the today task: cell segmentation! It's the dataset [ADDI](https://www.fc.up.pt/addi/ph2%20database.html). The dataset contained medical photo of skin with a mole or a melonoma. We'll just segment them on photo, without classification.\n",
        "\n",
        "<table><tr><td><img src=\"data:image/jpeg;base64,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\"></td><td><img src=\"data:image/jpeg;base64,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\"></td></tr></table>\n",
        "\n",
        "Download the archive and uncompess it.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qVGsvRA8m14_"
      },
      "source": [
        "!wget https://www.dropbox.com/s/k88qukc20ljnbuo/PH2Dataset.rar"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dLVEctpWm2GS"
      },
      "source": [
        "get_ipython().system_raw(\"unrar x PH2Dataset.rar\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gfjhBOdVxqER"
      },
      "source": [
        "// File structure\n",
        "\n",
        "// Let's show the images"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QrTSCMaYm2Du"
      },
      "source": [
        "import os\n",
        "from pathlib import Path\n",
        "\n",
        "from cv2 import erode\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from skimage.transform import rotate, rescale\n",
        "\n",
        "import catalyst\n",
        "import catalyst.dl as dl\n",
        "from catalyst.utils import imread\n",
        "\n",
        "import torch.nn.functional as F"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZBYFMYF1clIH"
      },
      "source": [
        "images = []\n",
        "lesions = []\n",
        "\n",
        "root = \"PH2Dataset\"\n",
        "\n",
        "for root, dirs, files in os.walk(os.path.join(root, \"PH2 Dataset images\")):\n",
        "    if root.endswith(\"_Dermoscopic_Image\"):\n",
        "        images.append(imread(os.path.join(root, files[0])))\n",
        "    if root.endswith(\"_lesion\"):\n",
        "        lesions.append(imread(os.path.join(root, files[0])))\n",
        "\n",
        "images = np.array(images)\n",
        "lesions = np.array(lesions)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fNtKdrNbeOC3"
      },
      "source": [
        "%matplotlib inline\n",
        "_, ax = plt.subplots(1, 2, figsize=(15, 10))\n",
        "\n",
        "ax[0].imshow(images[0])\n",
        "ax[1].imshow(lesions[0])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "foqbxeyWeN-w"
      },
      "source": [
        "thr = 0.5\n",
        "\n",
        "example_lesion = (lesions[0] / 255).mean(2)\n",
        "transformed_lesion = (rotate(example_lesion, 40) > thr) * 1.0\n",
        "\n",
        "_, ax = plt.subplots(1, 2, figsize=(15, 10))\n",
        "\n",
        "ax[0].imshow(example_lesion, cmap=\"gray\")\n",
        "ax[1].imshow(transformed_lesion, cmap=\"gray\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bQcWQq6-yDeh"
      },
      "source": [
        "### IoU\n",
        "\n",
        "![iou](https://pyimagesearch.com/wp-content/uploads/2016/09/iou_equation.png)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y8EVe03mfYb4"
      },
      "source": [
        "intersection = transformed_lesion*example_lesion\n",
        "\n",
        "_, ax = plt.subplots(1, 3, figsize=(15, 10))\n",
        "\n",
        "red_true = np.concatenate(\n",
        "    [example_lesion[:, :, np.newaxis], np.zeros(example_lesion.shape + (2,))],\n",
        "    axis=2,\n",
        ")\n",
        "blue_fake = np.concatenate(\n",
        "    [\n",
        "        np.zeros(example_lesion.shape + (2,)),\n",
        "        transformed_lesion[:, :, np.newaxis],\n",
        "    ],\n",
        "    axis=2,\n",
        ")\n",
        "\n",
        "ax[0].imshow(red_true)\n",
        "ax[1].imshow((intersection[:,:,np.newaxis] * 0.5 + red_true + blue_fake))\n",
        "ax[2].imshow(blue_fake)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DiiINYPifYY7"
      },
      "source": [
        "union = transformed_lesion+example_lesion-intersection\n",
        "\n",
        "_, ax = plt.subplots(1, 3, figsize=(15, 10))\n",
        "\n",
        "red_true = np.concatenate(\n",
        "    [example_lesion[:, :, np.newaxis], np.zeros(example_lesion.shape + (2,))],\n",
        "    axis=2,\n",
        ")\n",
        "blue_fake = np.concatenate(\n",
        "    [\n",
        "        np.zeros(example_lesion.shape + (2,)),\n",
        "        transformed_lesion[:, :, np.newaxis],\n",
        "    ],\n",
        "    axis=2,\n",
        ")\n",
        "\n",
        "ax[0].imshow(red_true)\n",
        "ax[1].imshow(union, cmap=\"gray\")\n",
        "ax[2].imshow(blue_fake)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6-iG09QmfYR7"
      },
      "source": [
        "iou = np.sum(intersection) / np.sum(union)\n",
        "print(f\"Current IoU: {iou}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c9g4nVsQy3pf"
      },
      "source": [
        "# Is it loss?\n",
        "intersection_l = transformed_lesion*example_lesion\n",
        "union_l = transformed_lesion+example_lesion-intersection_l\n",
        "iou_l = np.sum(intersection_l) / np.sum(union_l)\n",
        "\n",
        "print(f\"IoU: {iou_l}, Current IoU Loss: {1 - iou_l}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lHL5XDwhuVQI"
      },
      "source": [
        "import torch\n",
        "from catalyst.metrics import iou\n",
        "\n",
        "\n",
        "print(f\"IoU {iou(torch.from_numpy(transformed_lesion), torch.from_numpy(example_lesion), mode='micro')}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hcHqpB1uyDbr"
      },
      "source": [
        "### Dice\n",
        "\n",
        "![](https://miro.medium.com/max/858/1*yUd5ckecHjWZf6hGrdlwzA.png)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u2F2a8f5y4Or"
      },
      "source": [
        "intersection = transformed_lesion * example_lesion\n",
        "\n",
        "recall = intersection.sum() / example_lesion.sum()\n",
        "precision = intersection.sum() / transformed_lesion.sum()\n",
        "print(f\"Recall: {recall}. Precision: {precision}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2SZuMEV-nPCf"
      },
      "source": [
        "f1 = 2 / (1 / recall + 1 / precision)\n",
        "print(f\"F1: {f1}\")\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bfzTr5ownPAU"
      },
      "source": [
        "f1 = 2 * intersection.sum() / (transformed_lesion.sum() + example_lesion.sum())\n",
        "print(f\"Another F1: {f1}. It's DICE!\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LL4XcKaNv0x_"
      },
      "source": [
        "from catalyst.metrics import dice\n",
        "\n",
        "\n",
        "print(f\"Dice {dice(torch.from_numpy(transformed_lesion), torch.from_numpy(example_lesion), mode='micro')}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PNsNmI6kyDWe"
      },
      "source": [
        "### Morphological Transformations\n",
        "\n",
        "\n",
        "Morphological transformation is a useful toolkit to enhance maskes or semantic maps. If a mask is noisy, we can remove error points by erosion."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cPO3IfjGp_bl"
      },
      "source": [
        "thr = 0.5\n",
        "\n",
        "transformed_lesion = rotate(example_lesion, 40) * 1.0\n",
        "noise = np.random.randn(*(s // 20 for s in example_lesion.shape))\n",
        "noise = rescale(noise, 30)[\n",
        "    : example_lesion.shape[0], : example_lesion.shape[1]\n",
        "]\n",
        "noised_mask = 0.7 * transformed_lesion + 0.3 * noise\n",
        "noised_mask = (noised_mask - noised_mask.min()) / (\n",
        "    noised_mask.max() - noised_mask.min()\n",
        ")\n",
        "\n",
        "_, ax = plt.subplots(1, 3, figsize=(15, 10))\n",
        "\n",
        "ax[0].imshow(example_lesion, cmap=\"gray\")\n",
        "ax[1].imshow(transformed_lesion > thr, cmap=\"gray\")\n",
        "ax[2].imshow(noised_mask > thr, cmap=\"gray\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GnKzII3jyvUm"
      },
      "source": [
        "How erosion works?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J4X_AkVZyuzI"
      },
      "source": [
        "kernel = np.ones((3, 3)).astype(np.uint8)\n",
        "print(f\"Kernel:\\n{kernel}\")\n",
        "\n",
        "example = np.ones((10, 10))\n",
        "example[3, 4] = 0\n",
        "print(f\"Example:\\n{example}\")\n",
        "\n",
        "padded_example = np.zeros((12, 12))\n",
        "padded_example[1:11, 1:11] = example\n",
        "result = np.zeros((10, 10))\n",
        "for i in range(10):\n",
        "    for j in range(10):\n",
        "        result[i, j] = np.min(padded_example[i : i + 3, j : j + 3] * kernel)\n",
        "\n",
        "print(f\"Result:\\n{result}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G3q7WmZpxDvu"
      },
      "source": [
        "Let's use it.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sD9di6NcwxVi"
      },
      "source": [
        "kernel = np.ones((5, 5)).astype(np.uint8)\n",
        "eroded = (\n",
        "    erode((noised_mask * 255).astype(np.uint8), kernel, iterations=5) / 255\n",
        ")\n",
        "\n",
        "_, ax = plt.subplots(1, 3, figsize=(15, 10))\n",
        "\n",
        "ax[0].imshow(example_lesion, cmap=\"gray\")\n",
        "ax[1].imshow(noised_mask > thr, cmap=\"gray\")\n",
        "ax[2].imshow(eroded > thr, cmap=\"gray\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sY4hzUjpMfuA"
      },
      "source": [
        "## Data Augmentation\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RTcVtI0dMkgk"
      },
      "source": [
        "from torch.utils.data import Dataset\n",
        "\n",
        "\n",
        "class SegmentationDataset(Dataset):\n",
        "    def __init__(self, images=None, masks=None, transforms=None) -> None:\n",
        "        self.images = images\n",
        "        self.masks = masks\n",
        "        self.transforms = transforms\n",
        "\n",
        "    def __len__(self) -> int:\n",
        "        return len(self.images)\n",
        "\n",
        "    def __getitem__(self, idx: int) -> dict:\n",
        "        image = self.images[idx]\n",
        "\n",
        "        result = {\"image\": image}\n",
        "\n",
        "        if self.masks is not None:\n",
        "            result[\"mask\"] = self.masks[idx].mean(2).astype(int) // 255\n",
        "\n",
        "        if self.transforms is not None:\n",
        "            result = self.transforms(**result)\n",
        "\n",
        "        return result"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EL_uxA1sMkfU"
      },
      "source": [
        "import albumentations as albu\n",
        "from albumentations.pytorch import ToTensor\n",
        "\n",
        "\n",
        "def pre_transform(image_size: int = 224):\n",
        "    return albu.Resize(224, 224, p=1)\n",
        "\n",
        "\n",
        "def augmentations(image_size: int = 224):\n",
        "    channel_augs = [\n",
        "        albu.HueSaturationValue(p=0.5),\n",
        "        albu.ChannelShuffle(p=0.5),\n",
        "    ]\n",
        "\n",
        "    result = [\n",
        "        albu.OneOf(\n",
        "            [albu.IAAAdditiveGaussianNoise(), albu.GaussNoise(),], p=0.5\n",
        "        ),\n",
        "        albu.OneOf(\n",
        "            [\n",
        "                albu.MotionBlur(blur_limit=3, p=0.7),\n",
        "                albu.MedianBlur(blur_limit=3, p=1.0),\n",
        "                albu.Blur(blur_limit=3, p=0.7),\n",
        "            ],\n",
        "            p=0.5,\n",
        "        ),\n",
        "        albu.OneOf(channel_augs),\n",
        "        albu.OneOf(\n",
        "            [albu.CLAHE(clip_limit=2), albu.IAASharpen(), albu.IAAEmboss(),],\n",
        "            p=0.5,\n",
        "        ),\n",
        "        albu.RandomBrightnessContrast(\n",
        "            brightness_limit=0.5, contrast_limit=0.5, p=0.5\n",
        "        ),\n",
        "        albu.RandomGamma(p=0.5),\n",
        "        albu.OneOf([albu.MedianBlur(p=0.5), albu.MotionBlur(p=0.5)]),\n",
        "        albu.RandomGamma(gamma_limit=(85, 115), p=0.5),\n",
        "    ]\n",
        "    return albu.Compose(result)\n",
        "\n",
        "\n",
        "def post_transform():\n",
        "    return albu.Compose([albu.Normalize(), ToTensor()])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XGZ3fTI0OJWA"
      },
      "source": [
        "train_transformation = albu.Compose(\n",
        "    [pre_transform(), augmentations(), post_transform()]\n",
        ")\n",
        "\n",
        "valid_transformation = albu.Compose([pre_transform(), post_transform()])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BYz5COQSMkby"
      },
      "source": [
        "for i in range(4):\n",
        "    aug_image = (\n",
        "        train_transformation(image=images[0])[\"image\"].permute(1, 2, 0).numpy()\n",
        "    )\n",
        "    ax[i % 2][i // 2].imshow(aug_image)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FH56ygxjPOdm"
      },
      "source": [
        "from torch.utils.data import DataLoader, random_split\n",
        "\n",
        "\n",
        "batch_size = 8\n",
        "num_workers = 4\n",
        "\n",
        "\n",
        "indexes = np.arange(len(images))\n",
        "np.random.shuffle(indexes)\n",
        "train_indexes = indexes[: int(0.8 * len(images))]\n",
        "valid_indexes = indexes[int(0.8 * len(images)) :]\n",
        "\n",
        "\n",
        "train_loader = DataLoader(\n",
        "    SegmentationDataset(\n",
        "        images[train_indexes], lesions[train_indexes], train_transformation\n",
        "    ),\n",
        "    batch_size=batch_size,\n",
        "    num_workers=num_workers,\n",
        "    drop_last=True,\n",
        "    shuffle=True,\n",
        ")\n",
        "\n",
        "valid_loader = DataLoader(\n",
        "    SegmentationDataset(\n",
        "        images[valid_indexes], lesions[valid_indexes], valid_transformation\n",
        "    ),\n",
        "    batch_size=batch_size,\n",
        "    num_workers=num_workers,\n",
        "    drop_last=False,\n",
        "    shuffle=False,\n",
        ")\n",
        "\n",
        "loaders = {\"train\": train_loader, \"valid\": valid_loader}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sXVEL1AHUuXP"
      },
      "source": [
        "### U-Net\n",
        "\n",
        "![](https://www.researchgate.net/profile/Alan_Jackson9/publication/323597886/figure/fig2/AS:601386504957959@1520393124691/Convolutional-neural-network-CNN-architecture-based-on-UNET-Ronneberger-et-al.png)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "My3AHjPSy-AB"
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "\n",
        "class UNet(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "\n",
        "        self.down_1 = self.make_down_layer_(3, 64)\n",
        "        self.down_2 = self.make_down_layer_(64, 128)\n",
        "        self.down_3 = self.make_down_layer_(128, 256)\n",
        "        self.down_4 = self.make_down_layer_(256, 512)\n",
        "\n",
        "        self.up_1 = self.make_up_layer_(512, 256)\n",
        "        self.up_2 = self.make_up_layer_(256, 128)\n",
        "        self.up_3 = self.make_up_layer_(128, 64)\n",
        "        self.up_4 = nn.ConvTranspose2d(\n",
        "            64, 1, kernel_size=3, padding=1, stride=2, output_padding=1\n",
        "        )\n",
        "\n",
        "    def make_down_layer_(self, in_channels, out_channels):\n",
        "        return nn.Sequential(\n",
        "            nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1),\n",
        "            nn.BatchNorm2d(out_channels),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(out_channels, out_channels, kernel_size=3, padding=1),\n",
        "            nn.BatchNorm2d(out_channels),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(kernel_size=3, stride=2, padding=1),\n",
        "        )\n",
        "\n",
        "    def make_up_layer_(self, in_channels, out_channels):\n",
        "        return nn.Sequential(\n",
        "            nn.ConvTranspose2d(\n",
        "                in_channels,\n",
        "                out_channels,\n",
        "                kernel_size=3,\n",
        "                stride=2,\n",
        "                padding=1,\n",
        "                output_padding=1,\n",
        "            ),\n",
        "            nn.BatchNorm2d(2 * out_channels),\n",
        "            nn.ReLU(),\n",
        "            nn.ConvTranspose2d(\n",
        "                2 * out_channels,\n",
        "                out_channels,\n",
        "                kernel_size=3,\n",
        "                stride=1,\n",
        "                padding=1,\n",
        "            ),\n",
        "            nn.BatchNorm2d(out_channels),\n",
        "            nn.ReLU(),\n",
        "        )\n",
        "\n",
        "    def forward(self, image):\n",
        "        x_1 = self.down_1(image)\n",
        "        x_2 = self.down_2(x_1)\n",
        "        x_3 = self.down_3(x_2)\n",
        "        x_4 = self.down_4(x_3)\n",
        "\n",
        "        u_1 = self.up_1[0](x_4)\n",
        "        u_1 = torch.cat([x_3, u_1], axis=1)\n",
        "        for m in self.up_1[1:]:\n",
        "            u_1 = m(u_1)\n",
        "\n",
        "        u_2 = self.up_2[0](u_1)\n",
        "        u_2 = torch.cat([x_2, u_2], axis=1)\n",
        "        for m in self.up_2[1:]:\n",
        "            u_2 = m(u_2)\n",
        "\n",
        "        u_3 = self.up_3[0](u_2)\n",
        "        u_3 = torch.cat([x_1, u_3], axis=1)\n",
        "        for m in self.up_3[1:]:\n",
        "            u_3 = m(u_3)\n",
        "\n",
        "        return self.up_4(u_3)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pX5ycIj0lr6N"
      },
      "source": [
        "model = UNet()\n",
        "criterion = {\n",
        "    \"dice\": DiceLoss(),\n",
        "    \"iou\": IoULoss(),\n",
        "    \"bce\": nn.BCEWithLogitsLoss(),\n",
        "}\n",
        "optimizer = RAdam(model.parameters(), lr=5e-3)\n",
        "callbacks = [\n",
        "    dl.BatchTransformCallback(\n",
        "        scope=\"on_batch_end\",\n",
        "        transform=\"torch.sigmoid\",\n",
        "        input_key=\"logits\",\n",
        "        output_key=\"pred_mask\",\n",
        "    ),\n",
        "    dl.CriterionCallback(\n",
        "        \"pred_mask\", \"mask\", \"loss_dice\", criterion_key=\"dice\"\n",
        "    ),\n",
        "    dl.CriterionCallback(\"pred_mask\", \"mask\", \"loss_iou\", criterion_key=\"iou\"),\n",
        "    dl.CriterionCallback(\"pred_mask\", \"mask\", \"loss_bce\", criterion_key=\"bce\"),\n",
        "    dl.MetricAggregationCallback(\n",
        "        \"loss\",\n",
        "        mode=\"weighted_sum\",\n",
        "        metrics={\"loss_dice\": 1.0, \"loss_iou\": 1.0, \"loss_bce\": 0.8},\n",
        "    ),\n",
        "    dl.DiceCallback(\"pred_mask\", \"mask\"),\n",
        "    dl.IOUCallback(\"pred_mask\", \"mask\"),\n",
        "]\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VtqVHvu0lzlt"
      },
      "source": [
        "from datetime import datetime\n",
        "\n",
        "\n",
        "runner = dl.SupervisedRunner(\n",
        "    input_key=\"image\", target_key=\"mask\", output_key=\"logits\"\n",
        ")\n",
        "\n",
        "\n",
        "runner.train(\n",
        "    model=model,\n",
        "    criterion=criterion,\n",
        "    optimizer=optimizer,\n",
        "    loaders=loaders,\n",
        "    callbacks=callbacks,\n",
        "    logdir=Path(\"logs\") / datetime.now().strftime(\"%Y%m%d-%H%M%S\"),\n",
        "    num_epochs=10,\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rEggRn3XUvs7"
      },
      "source": [
        "Let's look at the mask!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VHPpZJzvy919"
      },
      "source": [
        "from torch.nn.functional import interpolate\n",
        "\n",
        "\n",
        "pt_image = valid_transformation(image=images[0])[\"image\"].unsqueeze(0).cuda()\n",
        "model.eval()\n",
        "\n",
        "pt_mask = torch.sigmoid(model(pt_image).detach().cpu())\n",
        "pt_mask = interpolate(pt_mask, images[0].shape[:-1]).squeeze()\n",
        "example_mask = pt_mask.squeeze().numpy()\n",
        "\n",
        "\n",
        "_, ax = plt.subplots(1, 3, figsize=(15, 10))\n",
        "\n",
        "ax[0].imshow(images[0])\n",
        "ax[1].imshow(lesions[0])\n",
        "ax[2].imshow(example_mask, cmap=\"gray\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "41MnzMWKUrgr"
      },
      "source": [
        "### FCN & Transfer Learning\n",
        "\n",
        "Using a pretrained ResNet model as a backbone for our segmentation model.\n",
        "\n",
        "![FCN](http://deeplearning.net/tutorial/_images/cat_segmentation.png)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L4hcth0lzFzs"
      },
      "source": [
        "class FCN(nn.Module):\n",
        "    def __init__(\n",
        "        self, pretrained_model, output_channel, image_size=224, p=0.1\n",
        "    ):\n",
        "        super().__init__()\n",
        "\n",
        "        self.pretrained_model = pretrained_model\n",
        "        self.output_channel = output_channel\n",
        "        self.image_size = image_size\n",
        "        self.p = p\n",
        "\n",
        "        self.deconv = nn.Sequential(\n",
        "            *(\n",
        "                self.make_up_block_(self.output_channel // d)\n",
        "                for d in [1, 4, 16, 64, 256]\n",
        "            )\n",
        "        )\n",
        "\n",
        "        self.output = nn.Conv2d(\n",
        "            1, 1, kernel_size=7, stride=1, padding=3, bias=False\n",
        "        )\n",
        "\n",
        "    def make_up_block_(self, output_channel):\n",
        "        return nn.Sequential(\n",
        "            nn.ConvTranspose2d(\n",
        "                output_channel,\n",
        "                output_channel // 4,\n",
        "                kernel_size=3,\n",
        "                stride=2,\n",
        "                padding=1,\n",
        "                output_padding=1,\n",
        "            ),\n",
        "            nn.BatchNorm2d(output_channel // 4),\n",
        "            nn.Dropout(p=self.p),\n",
        "            nn.ReLU(),\n",
        "        )\n",
        "\n",
        "    def forward(self, image):\n",
        "        features = self.pretrained_model(image)\n",
        "        mask = self.deconv(features)\n",
        "        return self.output(mask)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "htgMsTNpL5Nj"
      },
      "source": [
        "from torchvision.models import mobilenet_v2\n",
        "\n",
        "\n",
        "pretrained_model = mobilenet_v2(pretrained=True)\n",
        "model = FCN(pretrained_model.features, 1280, 512)\n",
        "optimizer = RAdam(model.parameters(), lr=5e-3)\n",
        "\n",
        "runner = dl.SupervisedRunner(\n",
        "    input_key=\"image\", target_key=\"mask\", output_key=\"logits\"\n",
        ")\n",
        "runner.train(\n",
        "    model=model,\n",
        "    criterion=criterion,\n",
        "    optimizer=optimizer,\n",
        "    loaders=loaders,\n",
        "    callbacks=callbacks,\n",
        "    logdir=Path(\"logs\") / datetime.now().strftime(\"%Y%m%d-%H%M%S\"),\n",
        "    num_epochs=10,\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yh0U_xmqL0Zl"
      },
      "source": [
        "pt_image = valid_transformation(image=images[0])[\"image\"].unsqueeze(0).cuda()\n",
        "model.eval()\n",
        "\n",
        "pt_mask = torch.sigmoid(model(pt_image).detach().cpu())\n",
        "pt_mask = interpolate(pt_mask, images[0].shape[:-1]).squeeze()\n",
        "example_mask = pt_mask.squeeze().numpy()\n",
        "\n",
        "\n",
        "_, ax = plt.subplots(1, 3, figsize=(15, 10))\n",
        "\n",
        "ax[0].imshow(images[0])\n",
        "ax[1].imshow(lesions[0])\n",
        "ax[2].imshow(example_mask, cmap=\"gray\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "owh-2zWePrEo"
      },
      "source": [
        "### Catalyst.Contrib\n",
        "\n",
        "Catalyst already has a segmentation models generator.\n",
        "There is a huge variety of model architectures, let's try one of them!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "deSZVEWFPujw"
      },
      "source": [
        "from catalyst.contrib.models.cv.segmentation.unet import Unet\n",
        "\n",
        "model = Unet()\n",
        "optimizer = RAdam(model.parameters(), lr=5e-3)\n",
        "\n",
        "runner = dl.SupervisedRunner(\n",
        "    input_key=\"image\", target_key=\"mask\", output_key=\"logits\"\n",
        ")\n",
        "runner.train(\n",
        "    model=model,\n",
        "    criterion=criterion,\n",
        "    optimizer=optimizer,\n",
        "    loaders=loaders,\n",
        "    callbacks=callbacks,\n",
        "    logdir=Path(\"logs\") / datetime.now().strftime(\"%Y%m%d-%H%M%S\"),\n",
        "    num_epochs=10,\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U7liNuzqQqd0"
      },
      "source": [
        "pt_image = valid_transformation(image=images[0])[\"image\"].unsqueeze(0).cuda()\n",
        "model.eval()\n",
        "\n",
        "pt_mask = torch.sigmoid(model(pt_image).detach().cpu())\n",
        "pt_mask = interpolate(pt_mask, images[0].shape[:-1]).squeeze()\n",
        "example_mask = pt_mask.squeeze().numpy()\n",
        "\n",
        "\n",
        "_, ax = plt.subplots(1, 3, figsize=(15, 10))\n",
        "\n",
        "ax[0].imshow(images[0])\n",
        "ax[1].imshow(lesions[0])\n",
        "ax[2].imshow(example_mask, cmap=\"gray\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1XQFtAKeh8Vf"
      },
      "source": [
        "## Object Detection\n",
        "\n",
        "This part based on tutorial from [github](https://github.com/sgrvinod/a-PyTorch-Tutorial-to-Object-Detection).\n",
        "\n",
        " Firstly download all necessary files. They contain a codebase, finetuned model's state dict and fonts for showing bounding boxes.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1b8Fy32vpanC"
      },
      "source": [
        "!git clone https://github.com/sgrvinod/a-PyTorch-Tutorial-to-Object-Detection\n",
        "!mv a-PyTorch-Tutorial-to-Object-Detection/* .\n",
        "!gdown https://drive.google.com/uc?id=1zmx5FXcnE9WC_rIZhh0qsy8Zom32YwNz&export=download\n",
        "!unzip Calibri_Font_Family.zip\n",
        "!gdown https://drive.google.com/uc?id=1bvJfF6r_zYl2xZEpYXxgb7jLQHFZ01Qe&export=download"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9_o1ZJA6_cci"
      },
      "source": [
        "Choose the victim. Object detection usualy is used for self-driving cars, so try to detect pedectrians on a  photo."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UhuJCS-qtu9W"
      },
      "source": [
        "!wget https://www1.nyc.gov/html/dot/images/pedestrians/ped-ramps-bay-ridge-ave.jpg"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aBsYYNFp4FSn"
      },
      "source": [
        "from PIL import Image\n",
        "import matplotlib.pyplot as plt\n",
        "from copy import deepcopy\n",
        "\n",
        "original_image = Image.open(\"ped-ramps-bay-ridge-ave.jpg\", mode=\"r\")\n",
        "original_image = original_image.convert(\"RGB\")\n",
        "\n",
        "plt.figure(figsize=(15, 10))\n",
        "plt.imshow(original_image)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KluwJXe2Appe"
      },
      "source": [
        "Prepare the image and the model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dAYq3WOW4XKe"
      },
      "source": [
        "from torchvision import transforms\n",
        "from utils import *\n",
        "from PIL import Image, ImageDraw, ImageFont\n",
        "\n",
        "from model import SSD300\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Load model checkpoint\n",
        "checkpoint = \"checkpoint_ssd300.pth.tar\"\n",
        "checkpoint = torch.load(checkpoint, map_location=device)\n",
        "start_epoch = checkpoint[\"epoch\"] + 1\n",
        "print(\"\\nLoaded checkpoint from epoch %d.\\n\" % start_epoch)\n",
        "model = SSD300(checkpoint[\"model\"].n_classes)\n",
        "model.load_state_dict(checkpoint[\"model\"].state_dict())\n",
        "model = model.to(device)\n",
        "model.eval()\n",
        "\n",
        "# Transforms\n",
        "resize = transforms.Resize((300, 300))\n",
        "to_tensor = transforms.ToTensor()\n",
        "normalize = transforms.Normalize(\n",
        "    mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r0wmChLe4pQO"
      },
      "source": [
        "image = normalize(to_tensor(resize(original_image)))\n",
        "\n",
        "# Move to default device\n",
        "image = image.to(device)\n",
        "\n",
        "# Forward prop.\n",
        "predicted_locs, predicted_scores = model(image.unsqueeze(0))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vIFpdihEAvvB"
      },
      "source": [
        "Let's look into predicted locs and labels."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EDoQ42F0AzmC"
      },
      "source": [
        "print(\n",
        "    f\"Locs shape: {predicted_locs.size()[1:]}, scores shape: {predicted_scores.size()[1:]}\"\n",
        ")\n",
        "\n",
        "annotated_image = deepcopy(original_image)\n",
        "draw = ImageDraw.Draw(annotated_image)\n",
        "\n",
        "decoded_locs = (\n",
        "    cxcy_to_xy(gcxgcy_to_cxcy(predicted_locs[0], model.priors_cxcy))\n",
        "    .cpu()\n",
        "    .detach()\n",
        ")\n",
        "\n",
        "original_dims = torch.FloatTensor(\n",
        "    [\n",
        "        original_image.width,\n",
        "        original_image.height,\n",
        "        original_image.width,\n",
        "        original_image.height,\n",
        "    ]\n",
        ").unsqueeze(0)\n",
        "decoded_locs = decoded_locs * original_dims\n",
        "\n",
        "for box_loc in decoded_locs[::10]:\n",
        "    box_loc = box_loc.numpy()\n",
        "    draw.rectangle(xy=[l + 1.0 for l in box_loc], outline=\"black\")\n",
        "\n",
        "plt.figure(figsize=(15, 10))\n",
        "plt.imshow(annotated_image)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hALE76JBM_71"
      },
      "source": [
        "Filter boxes by on model confidence."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jz7uaxnfGKDt"
      },
      "source": [
        "probs = F.softmax(predicted_scores[0], dim=1)\n",
        "# First class – background\n",
        "best_scores, best_cls = torch.max(probs[:, 1:], 1)\n",
        "\n",
        "filtered = decoded_locs[best_scores >= 0.2]\n",
        "print(f\"Filtered locations: {len(filtered)}\")\n",
        "\n",
        "annotated_image = deepcopy(original_image)\n",
        "draw = ImageDraw.Draw(annotated_image)\n",
        "\n",
        "for box_loc in filtered:\n",
        "    box_loc = box_loc.numpy()\n",
        "    draw.rectangle(xy=[l for l in box_loc], outline=\"black\")\n",
        "\n",
        "\n",
        "plt.figure(figsize=(15, 10))\n",
        "plt.imshow(annotated_image)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eBSy1ne2NEtV"
      },
      "source": [
        "Calculate IoU(or Jaccard index, same things) to find overlapping boxes.\n",
        "This is the Non-Maximum Suppression algorithm."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PTdMj2HlHPPP"
      },
      "source": [
        "annotated_image = deepcopy(original_image)\n",
        "draw = ImageDraw.Draw(annotated_image)\n",
        "\n",
        "filtered_scores = best_scores[best_scores > 0.2]\n",
        "filtered_cls = best_cls[best_scores > 0.2]\n",
        "\n",
        "result_box = []\n",
        "result_cls = []\n",
        "\n",
        "for c in range(0, model.n_classes):\n",
        "    _, indeces = torch.sort(\n",
        "        filtered_scores[filtered_cls == c], descending=True\n",
        "    )\n",
        "    cls_boxes = filtered[filtered_cls == c]\n",
        "    cls_boxes = cls_boxes[indeces]\n",
        "    overlap = find_jaccard_overlap(cls_boxes, cls_boxes)\n",
        "\n",
        "    suppress = torch.zeros(cls_boxes.size(0), dtype=torch.uint8)\n",
        "    for i in range(cls_boxes.size(0)):\n",
        "        if suppress[i] == 1:\n",
        "            continue\n",
        "\n",
        "        condition = overlap[i] > 0.5\n",
        "        condition = torch.tensor(condition, dtype=torch.uint8)\n",
        "        suppress = torch.max(suppress, condition)\n",
        "\n",
        "        suppress[i] = 0\n",
        "\n",
        "    for s, box in zip(suppress, cls_boxes):\n",
        "        if s < 1:\n",
        "            result_box.append(box)\n",
        "            result_cls.append(c)\n",
        "\n",
        "print(f\"After another filter: {len(result_box)} boxes\")\n",
        "for box_loc in result_box:\n",
        "    box_loc = box_loc.numpy()\n",
        "    draw.rectangle(xy=[l for l in box_loc], outline=\"black\")\n",
        "\n",
        "plt.figure(figsize=(15, 10))\n",
        "plt.imshow(annotated_image)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ftltg40iNZq9"
      },
      "source": [
        "And the last task is creating annotations!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ePd73OrE6wbb"
      },
      "source": [
        "annotated_image = deepcopy(original_image)\n",
        "draw = ImageDraw.Draw(annotated_image)\n",
        "font = ImageFont.truetype(\"./Calibri 400.ttf\", 15)\n",
        "\n",
        "for i in range(len(result_box)):\n",
        "    box_location = result_box[i].numpy()\n",
        "    label = rev_label_map[result_cls[i] + 1]\n",
        "    draw.rectangle(xy=box_location, outline=label_color_map[label])\n",
        "    draw.rectangle(\n",
        "        xy=[l + 1.0 for l in box_location], outline=label_color_map[label]\n",
        "    )\n",
        "\n",
        "    text_size = font.getsize(label.upper())\n",
        "    text_location = [box_location[0] + 2.0, box_location[1] - text_size[1]]\n",
        "    textbox_location = [\n",
        "        box_location[0],\n",
        "        box_location[1] - text_size[1],\n",
        "        box_location[0] + text_size[0] + 4.0,\n",
        "        box_location[1],\n",
        "    ]\n",
        "    draw.rectangle(xy=textbox_location, fill=label_color_map[label])\n",
        "    draw.text(xy=text_location, text=label.upper(), fill=\"white\", font=font)\n",
        "\n",
        "plt.figure(figsize=(15, 10))\n",
        "plt.imshow(annotated_image)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "id": "1WEChB2vnL87"
      },
      "source": [
        "### Extras\n",
        "Change model from the old SSD300 to the modern EfficientDet.\n",
        "Implementation and pretrained model's weights you can find in [@toandaominh1997/EfficientDet.Pytorch](https://github.com/toandaominh1997/EfficientDet.Pytorch)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9gQWAMbeGxvF"
      },
      "source": [
        "!pip install effdet"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N2dfUxj_rUf0"
      },
      "source": [
        "from effdet import create_model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Gorc2OX8rXN0"
      },
      "source": [
        "model = create_model(\n",
        "    \"efficientdet_d0\", pretrained=True, bench_task=\"predict\"\n",
        ").to(device)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P0vFLkmDulBP"
      },
      "source": [
        "resize = transforms.Resize((512, 512))\n",
        "image = normalize(to_tensor(resize(original_image))).to(device)\n",
        "\n",
        "pred = model(image.unsqueeze(0))[0].cpu().detach()\n",
        "\n",
        "boxes = pred[:, :4]\n",
        "probs, classes = pred[:, -2], pred[:, -1]\n",
        "\n",
        "k1 = original_image.width / 512\n",
        "k2 = original_image.height / 512\n",
        "\n",
        "boxes = torch.mul(boxes, torch.tensor((k1, k2, k1, k2))).long()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "a9htK_Th2tHh"
      },
      "source": [
        "annotated_image = deepcopy(original_image)\n",
        "draw = ImageDraw.Draw(annotated_image)\n",
        "for box in boxes:\n",
        "    draw.rectangle(box.tolist(), outline=\"black\")\n",
        "plt.figure(figsize=(15, 10))\n",
        "plt.imshow(annotated_image)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "-qKSMR_M6SVA"
      },
      "source": [
        "boxes = boxes[probs > 0.2]\n",
        "classes = classes[probs > 0.2]\n",
        "probs = probs[probs > 0.2]\n",
        "\n",
        "annotated_image = deepcopy(original_image)\n",
        "draw = ImageDraw.Draw(annotated_image)\n",
        "for box in boxes:\n",
        "    draw.rectangle(box.tolist(), outline=\"black\")\n",
        "plt.figure(figsize=(15, 10))\n",
        "plt.imshow(annotated_image)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4DvqYJDq6guQ"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}