{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9HZHH8tnBiSU"
   },
   "outputs": [],
   "source": [
    "! pip install -Uq catalyst==20.12 gym==0.17.3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Seminar. RL, DDPG.\n",
    "\n",
    "\n",
    "Hi! It's a second part of the seminar. Here we are going to introduce another way to train bot how to play games. A new algorithm will help bot to work in enviroments with continuos actinon spaces. However, the algorithm have no small changes in bot-enviroment communication process. That's why a lot of code for DQN part are reused. \n",
    "\n",
    "Let's code!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "a6vyaDMhBiSU"
   },
   "outputs": [],
   "source": [
    "from collections import deque, namedtuple\n",
    "import random\n",
    "import numpy as np\n",
    "import gym\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JevE61KWnNGH"
   },
   "outputs": [],
   "source": [
    "from catalyst.utils import set_global_seed, get_device\n",
    "\n",
    "\n",
    "set_global_seed(42)\n",
    "device = get_device()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Uta8S6jKBiSU"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import typing as tp\n",
    "from collections import deque, namedtuple\n",
    "\n",
    "Transition = namedtuple(\n",
    "    'Transition', \n",
    "    field_names=[\n",
    "        'state', \n",
    "        'action', \n",
    "        'reward',\n",
    "        'done', \n",
    "        'next_state'\n",
    "    ]\n",
    ")\n",
    "\n",
    "class ReplayBuffer:\n",
    "    def __init__(self, capacity: int):\n",
    "        self.buffer = deque(maxlen=capacity)\n",
    "    \n",
    "    def append(self, transition: Transition):\n",
    "        self.buffer.append(transition)\n",
    "    \n",
    "    def sample(self, size: int) -> tp.Sequence[np.array]:\n",
    "        indices = np.random.choice(\n",
    "            len(self.buffer), \n",
    "            size, \n",
    "            replace=size > len(self.buffer)\n",
    "        )\n",
    "        states, actions, rewards, dones, next_states = \\\n",
    "            zip(*[self.buffer[idx] for idx in indices])\n",
    "        states, actions, rewards, dones, next_states = (\n",
    "            np.array(states, dtype=np.float32), \n",
    "            np.array(actions, dtype=np.int64), \n",
    "            np.array(rewards, dtype=np.float32),\n",
    "            np.array(dones, dtype=np.bool), \n",
    "            np.array(next_states, dtype=np.float32)\n",
    "        )\n",
    "        return states, actions, rewards, dones, next_states\n",
    "    \n",
    "    def __len__(self) -> int:\n",
    "        return len(self.buffer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9u81h1z5BiSU"
   },
   "outputs": [],
   "source": [
    "from torch.utils.data.dataset import IterableDataset\n",
    "\n",
    "# as far as RL does not have some predefined dataset, \n",
    "# we need to specify epoch lenght by ourselfs\n",
    "class ReplayDataset(IterableDataset):\n",
    "\n",
    "    def __init__(self, buffer: ReplayBuffer, epoch_size: int = int(1e3)):\n",
    "        self.buffer = buffer\n",
    "        self.epoch_size = epoch_size\n",
    "\n",
    "    def __iter__(self) -> tp.Iterator[tp.Sequence[np.array]]:\n",
    "        states, actions, rewards, dones, next_states = \\\n",
    "            self.buffer.sample(self.epoch_size)\n",
    "        for i in range(len(dones)):\n",
    "            yield states[i], actions[i], rewards[i], dones[i], next_states[i]\n",
    "    \n",
    "    def __len__(self) -> int:\n",
    "        return self.epoch_size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first difference is action normalization. Some enviroments have action space bounds, and model's actino have to lie in the bounds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NormalizedActions(gym.ActionWrapper):\n",
    "\n",
    "    def action(self, action: float) -> float:\n",
    "        low_bound   = self.action_space.low\n",
    "        upper_bound = self.action_space.high\n",
    "        \n",
    "        action = low_bound + (action + 1.0) * 0.5 * (upper_bound - low_bound)\n",
    "        action = np.clip(action, low_bound, upper_bound)\n",
    "        \n",
    "        return action\n",
    "\n",
    "    def _reverse_action(self, action: float) -> float:\n",
    "        low_bound   = self.action_space.low\n",
    "        upper_bound = self.action_space.high\n",
    "        \n",
    "        action = 2 * (action - low_bound) / (upper_bound - low_bound) - 1\n",
    "        action = np.clip(action, low_bound, upper_bound)\n",
    "        \n",
    "        return action"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next difference is randomness. We can't just sample an action from action space. But we can add noise to generated action."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "iUckVz-yBiSU"
   },
   "outputs": [],
   "source": [
    "def get_action(\n",
    "        network: nn.Module,\n",
    "        state: np.array,\n",
    "        sigma: tp.Optional[float] = None\n",
    "    ) -> np.array:\n",
    "    state = torch.tensor(state, dtype=torch.float32).to(device).unsqueeze(0)\n",
    "    action = network(state).detach().cpu().numpy()[0]\n",
    "    if sigma is not None:\n",
    "        action = np.random.normal(action, sigma)\n",
    "    return action\n",
    "\n",
    "\n",
    "def generate_session(\n",
    "        env,\n",
    "        network: nn.Module,\n",
    "        sigma: tp.Optional[float] = None,\n",
    "        replay_buffer: tp.Optional[ReplayBuffer] = None,\n",
    "    ) -> tp.Tuple[float, int]:\n",
    "    total_reward = 0\n",
    "    state = env.reset()\n",
    "\n",
    "    for t in range(env.spec.max_episode_steps):\n",
    "        action = get_action(env, network, state=state, sigma=sigma)\n",
    "        next_state, reward, done, _ = env.step(action)\n",
    "\n",
    "        if replay_buffer is not None:\n",
    "            transition = Transition(\n",
    "                state, action, reward, done, next_state)\n",
    "            replay_buffer.append(transition)\n",
    "\n",
    "        total_reward += reward\n",
    "        state = next_state\n",
    "        if done:\n",
    "            break\n",
    "\n",
    "    return total_reward, t\n",
    "\n",
    "\n",
    "def generate_sessions(\n",
    "        env,\n",
    "        network: nn.Module,\n",
    "        sigma: tp.Optional[float] = None,\n",
    "        replay_buffer: tp.Optional[ReplayBuffer] = None,\n",
    "        num_sessions: int = 100,\n",
    "    ) -> tp.Tuple[float, int]:\n",
    "    sessions_reward, sessions_steps = 0, 0\n",
    "    for i_episone in range(num_sessions):\n",
    "        r, t = generate_session(\n",
    "            env=env, \n",
    "            network=network,\n",
    "            sigma=sigma,\n",
    "            replay_buffer=replay_buffer,\n",
    "        )\n",
    "        sessions_reward += r\n",
    "        sessions_steps += t\n",
    "    return sessions_reward, sessions_steps\n",
    "\n",
    "\n",
    "def soft_update(target: nn.Module, source: nn.Module, tau: float):\n",
    "    \"\"\"Updates the target data with smoothing by ``tau``\"\"\"\n",
    "    for target_param, param in zip(target.parameters(), source.parameters()):\n",
    "        target_param.data.copy_(\n",
    "            target_param.data * (1.0 - tau) + param.data * tau\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from catalyst import dl\n",
    "\n",
    "\n",
    "class GameCallback(dl.Callback):\n",
    "    def __init__(\n",
    "        self, \n",
    "        *,\n",
    "        env, \n",
    "        replay_buffer: ReplayBuffer,\n",
    "        session_period: int,\n",
    "        sigma: float,\n",
    "        actor_key: str,\n",
    "    ):\n",
    "        super().__init__(order=0)\n",
    "        self.env = env\n",
    "        self.replay_buffer = replay_buffer\n",
    "        self.session_period = session_period\n",
    "        self.sigma = sigma\n",
    "        self.actor_key = actor_key\n",
    "        \n",
    "    def on_stage_start(self, runner: dl.IRunner):\n",
    "        self.actor = runner.model[self.actor_key]\n",
    "        \n",
    "        self.actor.eval()\n",
    "        generate_sessions(\n",
    "            env=self.env, \n",
    "            network=self.actor,\n",
    "            sigma=self.sigma,\n",
    "            replay_buffer=self.replay_buffer,\n",
    "            num_sessions=1000,\n",
    "        )\n",
    "        self.actor.train()\n",
    "    \n",
    "    def on_epoch_start(self, runner: dl.IRunner):\n",
    "        self.session_counter = 0\n",
    "        self.session_steps = 0\n",
    "        \n",
    "    def on_batch_end(self, runner: dl.IRunner):\n",
    "        if runner.global_batch_step % self.session_period == 0:\n",
    "            self.actor.eval()\n",
    "\n",
    "            session_reward, session_steps  = generate_session(\n",
    "                env=self.env, \n",
    "                network=self.actor,\n",
    "                sigma=self.sigma,\n",
    "                replay_buffer=self.replay_buffer,\n",
    "            )\n",
    "\n",
    "            self.session_counter += 1\n",
    "            self.session_steps += session_steps\n",
    "\n",
    "            runner.batch_metrics.update({\"s_reward\": session_reward})\n",
    "            runner.batch_metrics.update({\"s_steps\": session_steps})\n",
    "\n",
    "            self.actor.train()\n",
    "            \n",
    "    def on_epoch_end(self, runner: dl.IRunner):\n",
    "        num_sessions = 100\n",
    "        \n",
    "        self.actor.eval()\n",
    "        valid_rewards, valid_steps = generate_sessions(\n",
    "            env=self.env, \n",
    "            network=self.actor,\n",
    "            num_sessions=num_sessions\n",
    "        )\n",
    "        self.actor.train()\n",
    "        \n",
    "        valid_rewards /= num_sessions\n",
    "        runner.epoch_metrics[\"train_num_samples\"] = self.session_steps\n",
    "        runner.epoch_metrics[\"train_updates_per_sample\"] = \\\n",
    "            runner.loader_sample_step / self.session_steps\n",
    "        runner.epoch_metrics[\"train_v_reward\"] = valid_rewards"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And the main difference is that we have two networks! Look at the algorithm:\n",
    "\n",
    "![DDPG algorithm](https://miro.medium.com/max/1084/1*BVST6rlxL2csw3vxpeBS8Q.png)\n",
    "\n",
    "One network is used to generate action (Policy Network). Another judge network's action and predict current reward. Because we have two networks, we can train our model to act in a continues space. Let's code this algorithm in Runner train step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_S0oeLknBiSU"
   },
   "outputs": [],
   "source": [
    "class CustomRunner(dl.Runner):\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        *,\n",
    "        gamma: float,\n",
    "        tau: flaot,\n",
    "        tau_period: int = 1,\n",
    "        **kwargs,\n",
    "    ):\n",
    "        super().__init__(**kwargs)\n",
    "        self.gamma = gamma\n",
    "        self.tau = tau\n",
    "        self.tau_period = tau_period\n",
    "\n",
    "    def on_stage_start(self, runner: dl.IRunner):\n",
    "        super().on_stage_start(runner)\n",
    "        soft_update(self.model[\"target_actor\"], self.model[\"actor\"], 1.0)\n",
    "        soft_update(self.model[\"target_critic\"], self.model[\"critic\"], 1.0)\n",
    "\n",
    "    def _handle_batch(self, batch: tp.Sequence[torch.Tensor]):\n",
    "        # model train/valid step\n",
    "        states, actions, rewards, dones, next_states = batch\n",
    "        actor, target_actor = self.model[\"actor\"], self.model[\"target_actor\"]\n",
    "        critic, target_critic = self.model[\"critic\"], self.model[\"target_critic\"]\n",
    "        actor_optimizer, critic_optimizer = self.optimizer[\"actor\"], self.optimizer[\"critic\"]\n",
    "\n",
    "        # get actions for the current state\n",
    "        pred_actions = actor(states)\n",
    "        # get q-values for the actions in current states\n",
    "        pred_critic_states = torch.cat([states, pred_actions], 1)\n",
    "        # use q-values to train the actor model\n",
    "        policy_loss = (-critic(pred_critic_states)).mean()\n",
    "\n",
    "        with torch.no_grad():\n",
    "            # get possible actions for the next states\n",
    "            next_state_actions = target_actor(next_states)\n",
    "            # get possible q-values for the next actions\n",
    "            next_critic_states = torch.cat([next_states, next_state_actions], 1)\n",
    "            next_state_values = target_critic(next_critic_states).detach().squeeze()\n",
    "            next_state_values[dones] = 0.0\n",
    "\n",
    "        # compute Bellman's equation value\n",
    "        target_state_values = next_state_values * self.gamma + rewards\n",
    "        # compute predicted values\n",
    "        critic_states = torch.cat([states, actions], 1)\n",
    "        state_values = critic(critic_states).squeeze()\n",
    "\n",
    "        # train the critic model\n",
    "        value_loss = self.criterion(\n",
    "            state_values,\n",
    "            target_state_values.detach()\n",
    "        )\n",
    "\n",
    "        self.batch_metrics.update({\n",
    "            \"critic_loss\": value_loss,\n",
    "            \"actor_loss\": policy_loss\n",
    "        })\n",
    "\n",
    "        if self.is_train_loader:\n",
    "            actor.zero_grad()\n",
    "            actor_optimizer.zero_grad()\n",
    "            policy_loss.backward()\n",
    "            actor_optimizer.step()\n",
    "\n",
    "            critic.zero_grad()\n",
    "            critic_optimizer.zero_grad()\n",
    "            value_loss.backward()\n",
    "            critic_optimizer.step()\n",
    "\n",
    "            if self.global_batch_step % self.tau_period == 0:\n",
    "                soft_update(target_actor, actor, self.tau)\n",
    "                soft_update(target_critic, critic, self.tau)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prepare networks generator and train models!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from catalyst import utils\n",
    "\n",
    "\n",
    "def get_network_actor(env):\n",
    "    inner_fn = utils.get_optimal_inner_init(nn.ReLU)\n",
    "    outer_fn = utils.outer_init\n",
    "    \n",
    "    network = torch.nn.Sequential(\n",
    "        nn.Linear(env.observation_space.shape[0], 400),\n",
    "        nn.ReLU(),\n",
    "        nn.Linear(400, 300),\n",
    "        nn.ReLU(),\n",
    "    )\n",
    "    head = torch.nn.Sequential(\n",
    "        nn.Linear(300, 1),\n",
    "        nn.Tanh()\n",
    "    )\n",
    "    \n",
    "    network.apply(inner_fn)\n",
    "    head.apply(outer_fn)\n",
    "    \n",
    "    return torch.nn.Sequential(network, head)\n",
    "\n",
    "def get_network_critic(env):\n",
    "    inner_fn = utils.get_optimal_inner_init(nn.LeakyReLU)\n",
    "    outer_fn = utils.outer_init\n",
    "    \n",
    "    network = torch.nn.Sequential(\n",
    "        nn.Linear(env.observation_space.shape[0] + 1, 400),\n",
    "        nn.LeakyReLU(0.01),\n",
    "        nn.Linear(400, 300),\n",
    "        nn.LeakyReLU(0.01),\n",
    "    )\n",
    "    head = nn.Linear(300, 1)\n",
    "    \n",
    "    network.apply(inner_fn)\n",
    "    head.apply(outer_fn)\n",
    "    \n",
    "    return torch.nn.Sequential(network, head)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "SaeB5fi2BiSU",
    "outputId": "27c01282-9f39-4444-db26-040c7f717a09",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# data\n",
    "batch_size = 64\n",
    "epoch_size = int(1e3) * batch_size\n",
    "buffer_size = int(1e5)\n",
    "# runner settings, ~training\n",
    "gamma = 0.99\n",
    "tau = 0.01\n",
    "tau_period = 1\n",
    "# callback, ~exploration\n",
    "session_period = 1\n",
    "sigma = 0.3\n",
    "# optimization\n",
    "lr_actor = 1e-4\n",
    "lr_critic = 1e-3\n",
    "\n",
    "# You can change game\n",
    "# env_name = \"LunarLanderContinuous-v2\"\n",
    "env_name = \"Pendulum-v0\"\n",
    "env = NormalizedActions(gym.make(env_name))\n",
    "replay_buffer = ReplayBuffer(buffer_size)\n",
    "\n",
    "actor, target_actor = get_network_actor(env), get_network_actor(env)\n",
    "critic, target_critic = get_network_critic(env), get_network_critic(env)\n",
    "utils.set_requires_grad(target_actor, requires_grad=False)\n",
    "utils.set_requires_grad(target_critic, requires_grad=False)\n",
    "\n",
    "models = {\n",
    "    \"actor\": actor,\n",
    "    \"critic\": critic,\n",
    "    \"target_actor\": target_actor,\n",
    "    \"target_critic\": target_critic,\n",
    "}\n",
    "\n",
    "criterion = torch.nn.MSELoss()\n",
    "optimizer = {\n",
    "    \"actor\": torch.optim.Adam(actor.parameters(), lr_actor),\n",
    "    \"critic\": torch.optim.Adam(critic.parameters(), lr=lr_critic),\n",
    "}\n",
    "\n",
    "loaders = {\n",
    "    \"train\": DataLoader(\n",
    "        ReplayDataset(replay_buffer, epoch_size=epoch_size), \n",
    "        batch_size=batch_size,\n",
    "    ),\n",
    "}\n",
    "\n",
    "\n",
    "runner = CustomRunner(\n",
    "    gamma=gamma, \n",
    "    tau=tau,\n",
    "    tau_period=tau_period,\n",
    "    device=device,\n",
    ")\n",
    "\n",
    "runner.train(\n",
    "    model=models,\n",
    "    criterion=criterion,\n",
    "    optimizer=optimizer,\n",
    "    loaders=loaders,\n",
    "    logdir=\"./logs_ddpg\",\n",
    "    num_epochs=10,\n",
    "    verbose=True,\n",
    "    main_metric=\"v_reward\",\n",
    "    minimize_metric=False,\n",
    "    load_best_on_end=True,\n",
    "    callbacks=[\n",
    "        GameCallback(\n",
    "            env=env, \n",
    "            replay_buffer=replay_buffer, \n",
    "            session_period=session_period,\n",
    "            sigma=sigma,\n",
    "            actor_key=\"actor\",\n",
    "        )\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And we can watch how our model plays in the games!\n",
    "\n",
    "\\* to run cells below, you should update your python environment. Instruction depends on your system specification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym.wrappers\n",
    "\n",
    "\n",
    "env = gym.wrappers.Monitor(\n",
    "    gym.make(env_name),\n",
    "    directory=\"videos_ddpg\", \n",
    "    force=True)\n",
    "generate_sessions(\n",
    "    env=env, \n",
    "    network=runner.model[\"actor\"],\n",
    "    num_sessions=100\n",
    ")\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# show video\n",
    "from IPython.display import HTML\n",
    "import os\n",
    "\n",
    "video_names = list(\n",
    "    filter(lambda s: s.endswith(\".mp4\"), os.listdir(\"./videos_ddpg/\")))\n",
    "\n",
    "HTML(\"\"\"\n",
    "<video width=\"640\" height=\"480\" controls>\n",
    "  <source src=\"{}\" type=\"video/mp4\">\n",
    "</video>\n",
    "\"\"\".format(\"./videos/\"+video_names[-1]))  # this may or may not be _last_ video. Try other indices"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Catalyst.RL.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}