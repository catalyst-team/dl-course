{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q catalyst==20.12 nltk torchtext==0.8.0 captum"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Seminar\n",
    "\n",
    "Hi! Today we starts NLP section in our course. Starting with embeddings and Recurrent Neural Networks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from catalyst.utils import set_global_seed, get_device\n",
    "\n",
    "\n",
    "set_global_seed(42)\n",
    "device = get_device()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text preprocessing\n",
    "\n",
    "Text preprocessing is the most important part of NLP. In comparison, an image is usually reshaped and normalized in a preprocessing pipeline. But a text is different. A text consists of words(or tokens), that has a different probability to be written. Words are arrays of characters, and different arrays can be related to one word(E.g. \"it\" and \"It\" or \"Имя\" and \"Имени\" is one word, but different word form.). That's why texts should be normalized and tokenized."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "example = \"Hello! My name is <unk> and i'm <unk>.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lower = example.lower()\n",
    "print(lower)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import WordPunctTokenizer\n",
    "\n",
    "\n",
    "tokenizer = WordPunctTokenizer()\n",
    "tokens = tokenizer.tokenize(lower)\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another part of the preprocessing is filtration. Every token has to be informative. Punctuation hasn't much information, and it should be deleted. Pronouns, prepositions, articles (and other small words) should be deleted too. Usually, they will not help to solve tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from string import punctuation\n",
    "\n",
    "\n",
    "filtered = [\n",
    "    token for token in tokens\n",
    "    if ((len(token) >= 3) and (token not in punctuation))\n",
    "]\n",
    "print(filtered)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Last part of preprocessing is adding special tokens. They means begining(`SOS`) or ending(`EOS`) of text/sentences, words out of vocabulary(`UNK`), padding for batching(`PAD`). A nueral networks can have other special tokens. For BERT some tokens should be masked. These tokens are swapped with `MASK` token."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SOS = \"<SOS>\" # or <SOT>/<BOT>/<BOS>\n",
    "EOS = \"<EOS>\" # so on...\n",
    "PAD = \"<PAD>\"\n",
    "UNK = \"<UNK>\"\n",
    "\n",
    "# Sometimes\n",
    "MASK = \"<MASK>\" # Masked Langueage Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text Classification\n",
    "\n",
    "The most popular task in NLP is text classification. Before 2012, this task is solved by pair of Tf-Idf method and some classification model. But now we have embeddings vector, mapped from tokens to some big continious high dimensions real space. Read more about Embeddings: [NLP course for you](https://lena-voita.github.io/nlp_course/word_embeddings.html).\n",
    "\n",
    "For text classification we will use mean of embeddings for each text as a feature vector. Let's code this!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "from torchtext.vocab import GloVe\n",
    "from torchtext.datasets import IMDB\n",
    "\n",
    "\n",
    "embedding_glove = GloVe(name='glove.twitter.27B.25d', dim=300, unk_init=torch.Tensor.normal_)\n",
    "train_dataset, test_dataset = IMDB()\n",
    "vocab = train_dataset.get_vocab()\n",
    "vocab.load_vectors(embedding_glove)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each text has different length, and we will build a batch by adding padding tokens at the end of the text. Effective way to do it by bucketing. However, it's not so easy to implement. So we will create simple padding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from typing import Tuple\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "\n",
    "PAD_ID = vocab.stoi['<pad>']\n",
    "\n",
    "\n",
    "def collate_fn(\n",
    "    batch: Tuple[torch.Tensor, torch.Tensor]\n",
    ") -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "    texts = []\n",
    "    max_len = max(t.size(0) for _, t in batch)\n",
    "    labels = torch.zeros(len(batch))\n",
    "    for idx, (label, txt) in enumerate(batch):\n",
    "        new_txt = torch.zeros((1, max_len)) + PAD_ID\n",
    "        new_txt[0, : txt.size(0)] = txt\n",
    "        texts.append(new_txt)\n",
    "        labels[idx] = label\n",
    "    return torch.cat(texts).type(torch.LongTensor), labels\n",
    "\n",
    "\n",
    "batch_size = 256\n",
    "\n",
    "loaders = {\n",
    "    \"train\": DataLoader(\n",
    "        train_dataset,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=True,\n",
    "        drop_last=True,\n",
    "        collate_fn=collate_fn,\n",
    "    ),\n",
    "    \"valid\": DataLoader(\n",
    "        test_dataset,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=False,\n",
    "        drop_last=False,\n",
    "        collate_fn=collate_fn,\n",
    "    ),\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our algorithm is this:\n",
    "- Get embeddings for each word\n",
    "- Get mean vector for text\n",
    "- Classify text by mean vector\n",
    "\n",
    "Let's code this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "from catalyst.contrib.nn import Lambda\n",
    "\n",
    "\n",
    "class EmbeddingModel(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        vocab_size: int,\n",
    "        embedding_size: int = 300,\n",
    "        hidden_size: int = 150,\n",
    "        dropout_p: float = 0.2,\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_size)\n",
    "        self.out = nn.Sequential(\n",
    "            Lambda(lambda x: x.reshape(x.size(0), embedding_size, x.size(1))),\n",
    "            nn.BatchNorm1d(embedding_size),\n",
    "            nn.Dropout2d(dropout_p),\n",
    "            Lambda(lambda x: x.mean(2)),\n",
    "            nn.Linear(embedding_size, hidden_size),\n",
    "            nn.BatchNorm1d(hidden_size),\n",
    "            nn.Dropout(dropout_p),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_size, 1),\n",
    "        )\n",
    "\n",
    "    def forward(self, input_ids: torch.Tensor) -> torch.Tensor:\n",
    "        embedded = self.embedding(input_ids)\n",
    "        return self.out(embedded).reshape(-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In next section, we will intepretate model's prediction. It works best with binary classificator with one output and we will train our model for this by changing criterion to `BCEWithLogitsLoss`.\n",
    "\n",
    "Create model, optimizer and criterion!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from catalyst.contrib.nn import RAdam\n",
    "\n",
    "\n",
    "model = EmbeddingModel(len(vocab), dropout_p=0.2)\n",
    "optimizer = RAdam(model.parameters(), lr=1e-3)\n",
    "criterion = nn.BCEWithLogitsLoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Embeddings are difficult to train. We will use pretrained one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.embedding.weight.data.copy_(vocab.vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from catalyst.dl import (\n",
    "    SupervisedRunner, MultiLabelAccuracyCallback)\n",
    "        \n",
    "runner = SupervisedRunner(device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "\n",
    "\n",
    "logdir = Path(\"emb_logs\") / datetime.now().strftime(\"%Y%m%d-%H%M%S\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`AccuracyCallback` doesnt' work well with binary classificator. So, we change it to `MultiLabelAccuracyCallback`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "runner.train(\n",
    "    model=model,\n",
    "    optimizer=optimizer,\n",
    "    criterion=criterion,\n",
    "    callbacks=[\n",
    "        MultiLabelAccuracyCallback(threshold=0.5),\n",
    "    ],\n",
    "    loaders=loaders,\n",
    "    verbose=True,\n",
    "    num_epochs=10,\n",
    "    logdir=logdir,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Interpretability\n",
    "\n",
    "Model's prediction interpretation is one of the ML-Engeenier task. To understand Neural Network prediction, we have great tool by PyTorch: [captum](https://github.com/pytorch/captum). It includes several algorithm, and we will use one (LayerIntegratedGradients, [arxiv](https://arxiv.org/pdf/1805.05492.pdf)) of them to understand which words influence on prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchtext.data.utils import get_tokenizer\n",
    "\n",
    "\n",
    "tokenize = get_tokenizer(\"basic_english\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List\n",
    "from captum.attr import LayerIntegratedGradients, TokenReferenceBase, visualization\n",
    "\n",
    "\n",
    "token_reference = TokenReferenceBase(reference_token_idx=PAD_ID)\n",
    "lig = LayerIntegratedGradients(model, model.embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# accumalate couple samples in this array for visualization purposes\n",
    "vis_data_records_ig = []\n",
    "\n",
    "def interpret_sentence(\n",
    "    model: nn.Module, sentence: str, min_len: int = 7, label: int = 0\n",
    "):\n",
    "    model.eval()\n",
    "    text = [tok for tok in tokenize(sentence)]\n",
    "    if len(text) < min_len:\n",
    "        text += ['<pad>'] * (min_len - len(text))\n",
    "    indexed = [vocab.stoi[t] for t in text]\n",
    "\n",
    "    model.zero_grad()\n",
    "\n",
    "    input_indices = torch.tensor(indexed, device=device)\n",
    "    input_indices = input_indices.unsqueeze(0)\n",
    "    \n",
    "    # input_indices dim: [sequence_length]\n",
    "    seq_length = min_len\n",
    "\n",
    "    # predict\n",
    "    pred = torch.sigmoid(model(input_indices)).item()\n",
    "    pred_label = \"pos\" if pred > 0.5 else \"neg\"\n",
    "\n",
    "    # generate reference indices for each sample\n",
    "    reference_indices = token_reference.generate_reference(\n",
    "        seq_length, device=device\n",
    "    ).unsqueeze(0)\n",
    "\n",
    "    # compute attributions and approximation delta using layer integrated gradients\n",
    "    attributions_ig, delta = lig.attribute(\n",
    "        input_indices,\n",
    "        reference_indices,\n",
    "        n_steps=5000,\n",
    "        return_convergence_delta=True,\n",
    "    )\n",
    "\n",
    "    print(f'pred: {pred_label}({pred:.2}), delta: {abs(delta)}')\n",
    "\n",
    "    add_attributions_to_visualizer(\n",
    "        attributions_ig,\n",
    "        text,\n",
    "        pred,\n",
    "        pred_label,\n",
    "        label,\n",
    "        delta,\n",
    "        vis_data_records_ig,\n",
    "    )\n",
    "\n",
    "def add_attributions_to_visualizer(\n",
    "    attributions: torch.Tensor,\n",
    "    text: str,\n",
    "    pred: int,\n",
    "    pred_ind: str,\n",
    "    label: int,\n",
    "    delta: float,\n",
    "    vis_data_records: List[visualization.VisualizationDataRecord],\n",
    "):\n",
    "    attributions = attributions.sum(dim=2).squeeze(0)\n",
    "    attributions = attributions / torch.norm(attributions)\n",
    "    attributions = attributions.cpu().detach().numpy()\n",
    "\n",
    "    # storing couple samples in an array for visualization purposes\n",
    "    vis_data_records.append(\n",
    "        visualization.VisualizationDataRecord(\n",
    "            attributions,\n",
    "            pred,\n",
    "            pred_ind,\n",
    "            label,\n",
    "            \"pos\" if label == 1 else \"neg\",\n",
    "            attributions.sum(),\n",
    "            text,\n",
    "            delta,\n",
    "        )\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have few sentence for testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "interpret_sentence(model, 'It was a fantastic performance!', label=1)\n",
    "interpret_sentence(model, 'Best film ever', label=1)\n",
    "interpret_sentence(model, 'It was a horrible movie', label=0)\n",
    "interpret_sentence(model, 'It is a disgusting movie!', label=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Visualize attributions based on Integrated Gradients')\n",
    "visualization.visualize_text(vis_data_records_ig)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part-of-Speech\n",
    "\n",
    "Move on from text classification to token classification. Tokens can include information like year, name, location and e.t.c. Or we try to analisy syntax of sentences by predcting part of speech for each token. Let's solve problem of part of speech prediction by a RNN neural network!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchtext.experimental.datasets import CoNLL2000Chunking\n",
    "\n",
    "train_dataset, test_dataset  = CoNLL2000Chunking()\n",
    "vocab, pos_vocab, _ = train_dataset.get_vocabs()\n",
    "vocab.load_vectors(embedding_glove)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PAD_ID = vocab.stoi['<pad>']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_fn(\n",
    "    batch: Tuple[torch.Tensor, torch.Tensor]\n",
    ") -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "    texts = []\n",
    "    token_types = []\n",
    "    max_len = max(t.size(0) for t, _, _ in batch)\n",
    "    for idx, (txt, tt, _) in enumerate(batch):\n",
    "        new_txt = torch.zeros((1, max_len)) + PAD_ID\n",
    "        new_tt = torch.zeros((1, max_len)) + PAD_ID\n",
    "        new_txt[0, : txt.size(0)] = txt\n",
    "        texts.append(new_txt)\n",
    "        new_tt[0, : tt.size(0)] = tt\n",
    "        token_types.append(new_tt)\n",
    "    return (\n",
    "        torch.cat(texts).type(torch.LongTensor),\n",
    "        torch.cat(token_types).type(torch.LongTensor),\n",
    "    )\n",
    "\n",
    "batch_size = 256\n",
    "\n",
    "loaders = {\n",
    "    \"train\": DataLoader(\n",
    "        train_dataset,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=True,\n",
    "        drop_last=True,\n",
    "        collate_fn=collate_fn,\n",
    "    ),\n",
    "    \"valid\": DataLoader(\n",
    "        test_dataset,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=False,\n",
    "        drop_last=False,\n",
    "        collate_fn=collate_fn,\n",
    "    ),\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our POS-model now consist of three components:\n",
    "- Embeddigns layer\n",
    "- (multi or single layer) RNN\n",
    "- Classifier for each token\n",
    "\n",
    "RNN has three main architectures: simple RNN, LSTM and GRU. Choose one of them to solve our task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class POSModel(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        vocab_size: int,\n",
    "        embedding_size: int = 300,\n",
    "        hidden_size: int = 150,\n",
    "        num_classes: int = 2,\n",
    "        dropout_p: float = 0.1,\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_size)\n",
    "        self.rnn = nn.LSTM(\n",
    "            embedding_size,\n",
    "            hidden_size,\n",
    "            num_layers=2,\n",
    "            dropout=dropout_p,\n",
    "            batch_first=True,\n",
    "        )\n",
    "        self.clf = nn.Linear(hidden_size, num_classes)\n",
    "\n",
    "    def forward(self, input_ids: torch.Tensor) -> torch.Tensor:\n",
    "        embedded = self.embedding(input_ids)\n",
    "        output, _ = self.rnn(embedded)\n",
    "        return self.clf(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create model, optimizer and criterion. We want predict POS for each token. But, some tokens, like `PAD`, hasn't POS property(we don't know their POS). That's why we will ignore them.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = POSModel(len(vocab), dropout_p=0.1, num_classes=len(pos_vocab))\n",
    "optimizer = RAdam(model.parameters(), lr=1e-2)\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=PAD_ID) # Ignore PAD token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.embedding.weight.data.copy_(vocab.vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Dict\n",
    "\n",
    "\n",
    "class POSRunner(SupervisedRunner):\n",
    "    def _handle_batch(self, batch: Dict[str, torch.Tensor]):\n",
    "        input_ids = batch[\"features\"]\n",
    "        pos_tags = batch[\"targets\"]\n",
    "        output = self.model(input_ids)\n",
    "        \n",
    "        self.input = {\"input_ids\": input_ids, \"targets\": pos_tags.reshape(-1)}\n",
    "        self.output = {\"logits\": output.reshape(-1, output.size(2))}\n",
    "        \n",
    "\n",
    "runner = POSRunner(device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logdir = Path(\"pos_logs\") / datetime.now().strftime(\"%Y%m%d-%H%M%S\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from catalyst.dl import AccuracyCallback\n",
    "\n",
    "\n",
    "runner.train(\n",
    "    model=model,\n",
    "    optimizer=optimizer,\n",
    "    criterion=criterion,\n",
    "    callbacks=[\n",
    "        AccuracyCallback(),\n",
    "    ],\n",
    "    loaders=loaders,\n",
    "    verbose=True,\n",
    "    num_epochs=10,\n",
    "    logdir=logdir,\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}