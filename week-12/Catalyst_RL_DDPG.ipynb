{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9HZHH8tnBiSU"
   },
   "outputs": [],
   "source": [
    "# ! pip install -Uq catalyst==20.11 gym==0.17.3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Seminar. RL, DDPG.\n",
    "\n",
    "\n",
    "Hi! It's a second part of the seminar. Here we are going to introduce another way to train bot how to play games. A new algorithm will help bot to work in enviroments with continuos actinon spaces. However, the algorithm have no small changes in bot-enviroment communication process. That's why a lot of code for DQN part are reused. \n",
    "\n",
    "Let's code!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "a6vyaDMhBiSU"
   },
   "outputs": [],
   "source": [
    "from collections import deque, namedtuple\n",
    "import random\n",
    "import numpy as np\n",
    "import gym\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from catalyst import dl, utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JevE61KWnNGH"
   },
   "outputs": [],
   "source": [
    "device = utils.get_device()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Uta8S6jKBiSU"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from collections import deque, namedtuple\n",
    "\n",
    "Transition = namedtuple(\n",
    "    'Transition', \n",
    "    field_names=[\n",
    "        'state', \n",
    "        'action', \n",
    "        'reward',\n",
    "        'done', \n",
    "        'next_state'\n",
    "    ]\n",
    ")\n",
    "\n",
    "class ReplayBuffer:\n",
    "    def __init__(self, capacity: int):\n",
    "        self.buffer = deque(maxlen=capacity)\n",
    "    \n",
    "    def append(self, transition: Transition):\n",
    "        self.buffer.append(transition)\n",
    "    \n",
    "    def sample(self, size: int):\n",
    "        indices = np.random.choice(\n",
    "            len(self.buffer), \n",
    "            size, \n",
    "            replace=size > len(self.buffer)\n",
    "        )\n",
    "        states, actions, rewards, dones, next_states = \\\n",
    "            zip(*[self.buffer[idx] for idx in indices])\n",
    "        states, actions, rewards, dones, next_states = (\n",
    "            np.array(states, dtype=np.float32), \n",
    "            np.array(actions, dtype=np.int64), \n",
    "            np.array(rewards, dtype=np.float32),\n",
    "            np.array(dones, dtype=np.bool), \n",
    "            np.array(next_states, dtype=np.float32)\n",
    "        )\n",
    "        return states, actions, rewards, dones, next_states\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.buffer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9u81h1z5BiSU"
   },
   "outputs": [],
   "source": [
    "from torch.utils.data.dataset import IterableDataset\n",
    "\n",
    "# as far as RL does not have some predefined dataset, \n",
    "# we need to specify epoch lenght by ourselfs\n",
    "class ReplayDataset(IterableDataset):\n",
    "\n",
    "    def __init__(self, buffer: ReplayBuffer, epoch_size: int = int(1e3)):\n",
    "        self.buffer = buffer\n",
    "        self.epoch_size = epoch_size\n",
    "\n",
    "    def __iter__(self):\n",
    "        states, actions, rewards, dones, next_states = \\\n",
    "            self.buffer.sample(self.epoch_size)\n",
    "        for i in range(len(dones)):\n",
    "            yield states[i], actions[i], rewards[i], dones[i], next_states[i]\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.epoch_size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first difference is action normalization. Some enviroments have action space bounds, and model's actino have to lie in the bounds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NormalizedActions(gym.ActionWrapper):\n",
    "\n",
    "    def action(self, action):\n",
    "        low_bound   = self.action_space.low\n",
    "        upper_bound = self.action_space.high\n",
    "        \n",
    "        action = low_bound + (action + 1.0) * 0.5 * (upper_bound - low_bound)\n",
    "        action = np.clip(action, low_bound, upper_bound)\n",
    "        \n",
    "        return action\n",
    "\n",
    "    def _reverse_action(self, action):\n",
    "        low_bound   = self.action_space.low\n",
    "        upper_bound = self.action_space.high\n",
    "        \n",
    "        action = 2 * (action - low_bound) / (upper_bound - low_bound) - 1\n",
    "        action = np.clip(action, low_bound, upper_bound)\n",
    "        \n",
    "        return actions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next difference is randomness. We can't just sample an action from action space. But we can add noise to generated action."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "iUckVz-yBiSU"
   },
   "outputs": [],
   "source": [
    "def get_action(env, network, state, sigma=None):\n",
    "    state = torch.tensor(state, dtype=torch.float32).to(device).unsqueeze(0)\n",
    "    action = network(state).detach().cpu().numpy()[0]\n",
    "    if sigma is not None:\n",
    "        action = np.random.normal(action, sigma)\n",
    "    return action\n",
    "\n",
    "\n",
    "def generate_session(\n",
    "    env, \n",
    "    network, \n",
    "    sigma=None,\n",
    "    replay_buffer=None,\n",
    "):\n",
    "    total_reward = 0\n",
    "    state = env.reset()\n",
    "\n",
    "    for t in range(env.spec.max_episode_steps):\n",
    "        action = get_action(env, network, state=state, sigma=sigma)\n",
    "        next_state, reward, done, _ = env.step(action)\n",
    "\n",
    "        if replay_buffer is not None:\n",
    "            transition = Transition(\n",
    "                state, action, reward, done, next_state)\n",
    "            replay_buffer.append(transition)\n",
    "\n",
    "        total_reward += reward\n",
    "        state = next_state\n",
    "        if done:\n",
    "            break\n",
    "\n",
    "    return total_reward, t\n",
    "\n",
    "def generate_sessions(\n",
    "    env, \n",
    "    network, \n",
    "    sigma=None,\n",
    "    replay_buffer=None,\n",
    "    num_sessions=100,\n",
    "):\n",
    "    sessions_reward, sessions_steps = 0, 0\n",
    "    for i_episone in range(num_sessions):\n",
    "        r, t = generate_session(\n",
    "            env=env, \n",
    "            network=network,\n",
    "            sigma=sigma,\n",
    "            replay_buffer=replay_buffer,\n",
    "        )\n",
    "        sessions_reward += r\n",
    "        sessions_steps += t\n",
    "    return sessions_reward, sessions_steps\n",
    "\n",
    "def soft_update(target, source, tau):\n",
    "    \"\"\"Updates the target data with smoothing by ``tau``\"\"\"\n",
    "    for target_param, param in zip(target.parameters(), source.parameters()):\n",
    "        target_param.data.copy_(\n",
    "            target_param.data * (1.0 - tau) + param.data * tau\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GameCallback(dl.Callback):\n",
    "    def __init__(\n",
    "        self, \n",
    "        *,\n",
    "        env, \n",
    "        replay_buffer, \n",
    "        session_period, \n",
    "        sigma,\n",
    "#         sigma_k,\n",
    "        actor_key,\n",
    "    ):\n",
    "        super().__init__(order=0)\n",
    "        self.env = env\n",
    "        self.replay_buffer = replay_buffer\n",
    "        self.session_period = session_period\n",
    "        self.sigma = sigma\n",
    "#         self.sigma_k = sigma_k\n",
    "        self.actor_key = actor_key\n",
    "        \n",
    "    def on_stage_start(self, runner: dl.IRunner):\n",
    "        self.actor = runner.model[self.actor_key]\n",
    "        \n",
    "        self.actor.eval()\n",
    "        generate_sessions(\n",
    "            env=self.env, \n",
    "            network=self.actor,\n",
    "            sigma=self.sigma,\n",
    "            replay_buffer=self.replay_buffer,\n",
    "            num_sessions=1000,\n",
    "        )\n",
    "        self.actor.train()\n",
    "    \n",
    "    def on_epoch_start(self, runner: dl.IRunner):\n",
    "        self.session_counter = 0\n",
    "        self.session_steps = 0\n",
    "        \n",
    "    def on_batch_end(self, runner: dl.IRunner):\n",
    "        if runner.global_batch_step % self.session_period == 0:\n",
    "            self.actor.eval()\n",
    "\n",
    "            session_reward, session_steps  = generate_session(\n",
    "                env=self.env, \n",
    "                network=self.actor,\n",
    "                sigma=self.sigma,\n",
    "                replay_buffer=self.replay_buffer,\n",
    "            )\n",
    "\n",
    "            self.session_counter += 1\n",
    "            self.session_steps += session_steps\n",
    "\n",
    "            runner.batch_metrics.update({\"s_reward\": session_reward})\n",
    "            runner.batch_metrics.update({\"s_steps\": session_steps})\n",
    "\n",
    "            self.actor.train()\n",
    "            \n",
    "    def on_epoch_end(self, runner: dl.IRunner):\n",
    "        num_sessions = 100\n",
    "        \n",
    "        self.actor.eval()\n",
    "        valid_rewards, valid_steps = generate_sessions(\n",
    "            env=self.env, \n",
    "            network=self.actor,\n",
    "            num_sessions=num_sessions\n",
    "        )\n",
    "        self.actor.train()\n",
    "        \n",
    "        valid_rewards /= num_sessions\n",
    "        runner.epoch_metrics[\"train_num_samples\"] = self.session_steps\n",
    "        runner.epoch_metrics[\"train_updates_per_sample\"] = \\\n",
    "            runner.loader_sample_step / self.session_steps\n",
    "        runner.epoch_metrics[\"train_v_reward\"] = valid_rewards"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And the main difference is that we have two networks! Look at the algorithm:\n",
    "\n",
    "![DDPG algorithm](https://miro.medium.com/max/1084/1*BVST6rlxL2csw3vxpeBS8Q.png)\n",
    "\n",
    "One network is used to generate action (Policy Network). Another judge network's action and predict current reward. Because we have two networks, we can train our model to act in a continues space. Let's code this algorithm in Runner train step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_S0oeLknBiSU"
   },
   "outputs": [],
   "source": [
    "class CustomRunner(dl.Runner):\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        *,\n",
    "        gamma,\n",
    "        tau,\n",
    "        tau_period=1,\n",
    "        **kwargs,\n",
    "    ):\n",
    "        super().__init__(**kwargs)\n",
    "        self.gamma = gamma\n",
    "        self.tau = tau\n",
    "        self.tau_period = tau_period\n",
    "\n",
    "    def on_stage_start(self, runner: dl.IRunner):\n",
    "        super().on_stage_start(runner)\n",
    "        soft_update(self.model[\"target_actor\"], self.model[\"actor\"], 1.0)\n",
    "        soft_update(self.model[\"target_critic\"], self.model[\"critic\"], 1.0)\n",
    "\n",
    "    def _handle_batch(self, batch):\n",
    "        # model train/valid step\n",
    "        states, actions, rewards, dones, next_states = batch\n",
    "        actor, target_actor = self.model[\"actor\"], self.model[\"target_actor\"]\n",
    "        critic, target_critic = self.model[\"critic\"], self.model[\"target_critic\"]\n",
    "        actor_optimizer, critic_optimizer = self.optimizer[\"actor\"], self.optimizer[\"critic\"]\n",
    "\n",
    "        # get actions for the current state\n",
    "        pred_actions = actor(states)\n",
    "        # get q-values for the actions in current states\n",
    "        pred_critic_states = torch.cat([states, pred_actions], 1)\n",
    "        # use q-values to train the actor model\n",
    "        policy_loss = (-critic(pred_critic_states)).mean()\n",
    "\n",
    "        with torch.no_grad():\n",
    "            # get possible actions for the next states\n",
    "            next_state_actions = target_actor(next_states)\n",
    "            # get possible q-values for the next actions\n",
    "            next_critic_states = torch.cat([next_states, next_state_actions], 1)\n",
    "            next_state_values = target_critic(next_critic_states).detach().squeeze()\n",
    "            next_state_values[dones] = 0.0\n",
    "\n",
    "        # compute Bellman's equation value\n",
    "        target_state_values = next_state_values * self.gamma + rewards\n",
    "        # compute predicted values\n",
    "        critic_states = torch.cat([states, actions], 1)\n",
    "        state_values = critic(critic_states).squeeze()\n",
    "\n",
    "        # train the critic model\n",
    "        value_loss = self.criterion(\n",
    "            state_values,\n",
    "            target_state_values.detach()\n",
    "        )\n",
    "\n",
    "        self.batch_metrics.update({\n",
    "            \"critic_loss\": value_loss,\n",
    "            \"actor_loss\": policy_loss\n",
    "        })\n",
    "\n",
    "        if self.is_train_loader:\n",
    "            actor.zero_grad()\n",
    "            actor_optimizer.zero_grad()\n",
    "            policy_loss.backward()\n",
    "            actor_optimizer.step()\n",
    "\n",
    "            critic.zero_grad()\n",
    "            critic_optimizer.zero_grad()\n",
    "            value_loss.backward()\n",
    "            critic_optimizer.step()\n",
    "\n",
    "            if self.global_batch_step % self.tau_period == 0:\n",
    "                soft_update(target_actor, actor, self.tau)\n",
    "                soft_update(target_critic, critic, self.tau)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prepare networks generator and train models!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_network_actor(env):\n",
    "    inner_fn = utils.get_optimal_inner_init(nn.ReLU)\n",
    "    outer_fn = utils.outer_init\n",
    "    \n",
    "    network = torch.nn.Sequential(\n",
    "        nn.Linear(env.observation_space.shape[0], 400),\n",
    "        nn.ReLU(),\n",
    "        nn.Linear(400, 300),\n",
    "        nn.ReLU(),\n",
    "    )\n",
    "    head = torch.nn.Sequential(\n",
    "        nn.Linear(300, 1),\n",
    "        nn.Tanh()\n",
    "    )\n",
    "    \n",
    "    network.apply(inner_fn)\n",
    "    head.apply(outer_fn)\n",
    "    \n",
    "    return torch.nn.Sequential(network, head)\n",
    "\n",
    "def get_network_critic(env):\n",
    "    inner_fn = utils.get_optimal_inner_init(nn.LeakyReLU)\n",
    "    outer_fn = utils.outer_init\n",
    "    \n",
    "    network = torch.nn.Sequential(\n",
    "        nn.Linear(env.observation_space.shape[0] + 1, 400),\n",
    "        nn.LeakyReLU(0.01),\n",
    "        nn.Linear(400, 300),\n",
    "        nn.LeakyReLU(0.01),\n",
    "    )\n",
    "    head = nn.Linear(300, 1)\n",
    "    \n",
    "    network.apply(inner_fn)\n",
    "    head.apply(outer_fn)\n",
    "    \n",
    "    return torch.nn.Sequential(network, head)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "SaeB5fi2BiSU",
    "outputId": "27c01282-9f39-4444-db26-040c7f717a09",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# data\n",
    "batch_size = 64\n",
    "epoch_size = int(1e3) * batch_size\n",
    "buffer_size = int(1e5)\n",
    "# runner settings, ~training\n",
    "gamma = 0.99\n",
    "tau = 0.01\n",
    "tau_period = 1\n",
    "# callback, ~exploration\n",
    "session_period = 1\n",
    "sigma = 0.3\n",
    "# optimization\n",
    "lr_actor = 1e-4\n",
    "lr_critic = 1e-3\n",
    "\n",
    "# env_name = \"LunarLanderContinuous-v2\"\n",
    "env_name = \"Pendulum-v0\"\n",
    "env = NormalizedActions(gym.make(env_name))\n",
    "replay_buffer = ReplayBuffer(buffer_size)\n",
    "\n",
    "actor, target_actor = get_network_actor(env), get_network_actor(env)\n",
    "critic, target_critic = get_network_critic(env), get_network_critic(env)\n",
    "utils.set_requires_grad(target_actor, requires_grad=False)\n",
    "utils.set_requires_grad(target_critic, requires_grad=False)\n",
    "\n",
    "models = {\n",
    "    \"actor\": actor,\n",
    "    \"critic\": critic,\n",
    "    \"target_actor\": target_actor,\n",
    "    \"target_critic\": target_critic,\n",
    "}\n",
    "\n",
    "criterion = torch.nn.MSELoss()\n",
    "optimizer = {\n",
    "    \"actor\": torch.optim.Adam(actor.parameters(), lr_actor),\n",
    "    \"critic\": torch.optim.Adam(critic.parameters(), lr=lr_critic),\n",
    "}\n",
    "\n",
    "loaders = {\n",
    "    \"train\": DataLoader(\n",
    "        ReplayDataset(replay_buffer, epoch_size=epoch_size), \n",
    "        batch_size=batch_size,\n",
    "    ),\n",
    "}\n",
    "\n",
    "\n",
    "runner = CustomRunner(\n",
    "    gamma=gamma, \n",
    "    tau=tau,\n",
    "    tau_period=tau_period,\n",
    "    device=device,\n",
    ")\n",
    "\n",
    "runner.train(\n",
    "    model=models,\n",
    "    criterion=criterion,\n",
    "    optimizer=optimizer,\n",
    "    loaders=loaders,\n",
    "    logdir=\"./logs_ddpg\",\n",
    "    num_epochs=10,\n",
    "    verbose=True,\n",
    "    main_metric=\"v_reward\",\n",
    "    minimize_metric=False,\n",
    "    load_best_on_end=True,\n",
    "    callbacks=[\n",
    "        GameCallback(\n",
    "            env=env, \n",
    "            replay_buffer=replay_buffer, \n",
    "            session_period=session_period,\n",
    "            sigma=sigma,\n",
    "            actor_key=\"actor\",\n",
    "        )\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And we can watch how our model plays in the games!\n",
    "\n",
    "\\* to run cells below, you should update your python environment. Instruction depends on your system specification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym.wrappers\n",
    "env = gym.wrappers.Monitor(\n",
    "    gym.make(env_name),\n",
    "    directory=\"videos_ddpg\", \n",
    "    force=True)\n",
    "generate_sessions(\n",
    "    env=env, \n",
    "    network=runner.model[\"actor\"],\n",
    "    num_sessions=100\n",
    ")\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# show video\n",
    "from IPython.display import HTML\n",
    "import os\n",
    "\n",
    "video_names = list(\n",
    "    filter(lambda s: s.endswith(\".mp4\"), os.listdir(\"./videos_ddpg/\")))\n",
    "\n",
    "HTML(\"\"\"\n",
    "<video width=\"640\" height=\"480\" controls>\n",
    "  <source src=\"{}\" type=\"video/mp4\">\n",
    "</video>\n",
    "\"\"\".format(\"./videos/\"+video_names[-1]))  # this may or may not be _last_ video. Try other indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Catalyst.RL.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
