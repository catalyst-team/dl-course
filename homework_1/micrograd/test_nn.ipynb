{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Task 1\n",
    "\n",
    "Here we will see how our nn and autograd works.\n",
    "\n",
    "Also we will review some Catalyst's abstractions, implement callbacks and datasets.\n",
    "\n",
    "Unfortunately, python is slow, and implementing dynamic computational graph in pure python for product-ready solution is not a good idea. But this task will help you to understand what's happening when you call `backward` method for variable or tensor. Also it will help you in learning Catalyst framework and will teach how you to write your code in more Catalyst-like way."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.datasets import make_moons\n",
    "from sklearn.model_selection import train_test_split\n",
    "from nn import Linear, ReLU, CrossEntropyLoss, Module\n",
    "from optim import SGD\n",
    "from engine import Value\n",
    "from matplotlib import pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "sns.set(style=\"whitegrid\", font_scale=1.4)\n",
    "%pylab inline"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Defining toy dataset\n",
    "\n",
    "To be more human-readable and easy to understand, we want to store every data in key-value format.\n",
    "\n",
    "So, the dataset should yield dict, moreover we will store train/valid datasets in a dict."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "class Dataset:\n",
    "    def __init__(self, X, y):\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "\n",
    "    def __getitem__(self, item):\n",
    "        return {\"features\": ..., \"targets\": ...}\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.y)\n",
    "\n",
    "\n",
    "\n",
    "X, y = make_moons(200, noise=0.2)\n",
    "X_train, X_val, y_train, y_val = # Split data to Train and Valid\n",
    "datasets = {\"train\": Dataset(X_train, y_train), \"valid\": Dataset(X_val, y_val)}"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Take a look on a data."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 8))\n",
    "plt.title(\"Data\", fontsize=14)\n",
    "colors = list(map(lambda x: \"green\" if x ==0 else \"orange\", y))\n",
    "plt.scatter(X[:, 0], X[:, 1], c=colors)\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Model\n",
    "\n",
    "Let's define our model in PyTorch-style. But don't forget to implement `parameters()` method."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "class SimpleModel(Module):\n",
    "    def __init__(self):\n",
    "        # Create your own network!\n",
    "\n",
    "    def forward(self, inp):\n",
    "        return ...\n",
    "\n",
    "    def parameters(self):\n",
    "        return ... "
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## For loop\n",
    "\n",
    "Let's start with simple train/test loop."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "criterion = CrossEntropyLoss()\n",
    "model = SimpleModel()\n",
    "optimizer = SGD(model.parameters(), lr=0.1)\n",
    "\n",
    "num_epochs = 100\n",
    "batch_size = 4\n",
    "log_period = 5\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    current_batch = []\n",
    "    metrics = {}\n",
    "    for k, dataset in datasets.items():\n",
    "        loader_metrics = {}\n",
    "        epoch_loss = 0\n",
    "        epoch_accuracy = 0\n",
    "        for idx, data in enumerate(dataset):\n",
    "            last = idx == (len(dataset)-1)\n",
    "            current_batch.append(data)\n",
    "            if last or len(current_batch) == batch_size:\n",
    "                current_batch=[]\n",
    "                for data in dataset:\n",
    "                    #Train your model!"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "\n",
    "## General training loop - Catalyst intro - RunnerÂ¶\n",
    "\n",
    "Code above can be reused for almost all machine learning task. Let's take a look on experiment structure\n",
    "\n",
    "```\n",
    "for stage in stage:\n",
    "    for epoch in epochs:\n",
    "        for loader on loaders:\n",
    "            for batch in loader:\n",
    "                # do something\n",
    "```\n",
    "\n",
    "### Runner\n",
    "\n",
    "In most cases we only need to adapt our batch handling method. And here comes the Runner.\n",
    "\n",
    "Runner is the main part of your experiment. It runs train loop, calls callbacks (we will discusds them later) and keeps track on your model. And the only thing you need to change is _handle_batch method.\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.notebook import tqdm\n",
    "\n",
    "\n",
    "class Runner:\n",
    "    def __init__(\n",
    "        self,\n",
    "        model,\n",
    "        criterion,\n",
    "        optimizer,\n",
    "        datasets,\n",
    "        batch_size\n",
    "    ):\n",
    "        self.model = model\n",
    "        self.criterion = criterion\n",
    "        self.optimizer = optimizer\n",
    "        self.datasets = datasets\n",
    "\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "        self.input = {}\n",
    "        self.output = {}\n",
    "        self.batch_metrics = {}\n",
    "        self.loader_metrcis = {}\n",
    "        self.epoch_metrics = {}\n",
    "\n",
    "    def _handle_batch(self, batch, train=True):\n",
    "        \"\"\"\n",
    "        Stores the main logic of data aggregating.\n",
    "        \"\"\"\n",
    "        loss = 0\n",
    "        correct = 0\n",
    "        for data in batch:\n",
    "            # Calculate predictions, loss and metric\n",
    "        \n",
    "        loss = loss / len(batch)\n",
    "        accuracy = correct / len(batch)\n",
    "        \n",
    "        if train:\n",
    "            # Optimize model's parameters\n",
    "\n",
    "        self.batch_metrics = {\"loss\": loss.item(), \"accuracy\": accuracy}\n",
    "\n",
    "\n",
    "    def train(self, num_epochs: int = 100, verbose=False):\n",
    "        for epoch in range(num_epochs):\n",
    "            self.epoch_metrics = {}\n",
    "            self.epoch = epoch\n",
    "            for dataset_name, dataset in self.datasets.items():\n",
    "                self.dataset_name = dataset_name\n",
    "                self.loader_metrics = {}\n",
    "                current_batch = []\n",
    "\n",
    "                if verbose:\n",
    "                    iter_ = tqdm(enumerate(dataset), total=len(dataset))\n",
    "                else:\n",
    "                    iter_ = enumerate(dataset)\n",
    "\n",
    "                for idx, data in iter_:\n",
    "                    last = idx == (len(dataset)-1)\n",
    "                    current_batch.append(data)\n",
    "\n",
    "                    if last or len(current_batch) == self.batch_size:\n",
    "                        # Handle batch\n",
    "                        current_batch = []\n",
    "\n",
    "                        for k, v in self.batch_metrics.items():\n",
    "                            if k not in self.loader_metrics:\n",
    "                                self.loader_metrics[k] = []\n",
    "                            self.loader_metrics[k].append(v)\n",
    "                \n",
    "                for metric, value in self.loader_metrics.items():\n",
    "                    value = np.mean(self.loader_metrics[metric])\n",
    "                    self.loader_metrics[metric] = value\n",
    "                    print(f\"epoch {epoch}: {dataset_name} {metric} - {value}\")"
   ]
  },
  {
   "source": [
    "### Run training"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = CrossEntropyLoss()\n",
    "model = SimpleModel()\n",
    "optimizer = SGD(model.parameters(), lr=0.1)\n",
    "runner = Runner(\n",
    "    model=model,\n",
    "    criterion=criterion,\n",
    "    optimizer=optimizer,\n",
    "    datasets=datasets,\n",
    "    batch_size=3,\n",
    ")\n",
    "runner.train(10)"
   ]
  },
  {
   "source": [
    "### Visualization"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "h = 0.25\n",
    "x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1\n",
    "y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1\n",
    "xx, yy = np.meshgrid(np.arange(x_min, x_max, h),\n",
    "                     np.arange(y_min, y_max, h))\n",
    "Xmesh = np.c_[xx.ravel(), yy.ravel()]\n",
    "inputs = [list(map(Value, xrow)) for xrow in Xmesh]\n",
    "scores = list(map(model, inputs))\n",
    "Z = np.array([s[1].exp().data/(s[0].exp()+s[1].exp()).data for s in scores])\n",
    "Z = Z.reshape(xx.shape)\n",
    "\n",
    "fig = plt.figure(figsize=(10, 8))\n",
    "plt.title(\"Decision boundary\", fontsize=14)\n",
    "plt.contourf(xx, yy, Z, cmap=plt.cm.Spectral, alpha=0.6)\n",
    "plt.scatter(X[:, 0], X[:, 1], c=y, s=40, cmap=plt.cm.Spectral)\n",
    "plt.xlim(xx.min(), xx.max())\n",
    "plt.ylim(yy.min(), yy.max())\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "\n",
    "## General training loop - Catalyst intro - Callbacks\n",
    "\n",
    "But could we make it even more general? Should we rewrite loss.backward or compute accuracy again and again? Do we really need it?\n",
    "\n",
    "I don't think so, I think, we could introduce another general abstaction for typical train-loop logic. Let's introduce Callbacks!\n",
    "\n",
    "### Callbacks\n",
    "\n",
    "In catalyst callbacks have significant impact in everything you do.\n",
    "Let's try to implement some of them.\n",
    "\n",
    "There are a list of moments, where callbacks can be integrated. We will need only three of them.\n",
    "```\n",
    "on_stage_start\n",
    "    on_epoch_start\n",
    "        on_loader_start\n",
    "            on_batch_start\n",
    "------->    on_batch_end\n",
    "----->  on_loader_end\n",
    "--> on_epoch_end\n",
    "on_stage_end\n",
    "```"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "class Callback:\n",
    "    def on_stage_start(self):\n",
    "        pass\n",
    "\n",
    "    def on_stage_end(self):\n",
    "        pass\n",
    "\n",
    "    def on_epoch_start(self):\n",
    "        pass\n",
    "\n",
    "    def on_epoch_end(self, runner):\n",
    "        pass\n",
    "\n",
    "    def on_loader_start(self):\n",
    "        pass\n",
    "\n",
    "    def on_loader_end(self, runner):\n",
    "        pass\n",
    "\n",
    "    def on_batch_start(self):\n",
    "        pass\n",
    "\n",
    "    def on_batch_end(self, runner):\n",
    "        pass\n",
    "\n",
    "\n",
    "class LossCallback(Callback):\n",
    "    \"\"\"\n",
    "    Aggregating loss value.\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        self.cum_loss = 0\n",
    "        self.num_batches = 0\n",
    "\n",
    "    def on_batch_end(self, runner):\n",
    "        \"\"\"\n",
    "        On batch end action.\n",
    "\n",
    "        Accumulates loss and num batches.\n",
    "\n",
    "        Args:\n",
    "            output: dict with loss and other model's outputs.\n",
    "        \"\"\"\n",
    "        self.cum_loss += runner.output[\"loss\"]\n",
    "        self.num_batches += 1\n",
    "\n",
    "    def on_loader_end(self, runner):\n",
    "        \"\"\"\n",
    "        On loader end action.\n",
    "\n",
    "        Args:\n",
    "            epoch_metrics: dict with epoch metrics\n",
    "\n",
    "        Returns:\n",
    "            loss over the loader.\n",
    "        \"\"\"\n",
    "        runner.epoch_metrics[\"loss\"] = self.cum_loss / self.num_batches\n",
    "        self.cum_loss = 0\n",
    "        self.num_batches = 0\n",
    "\n",
    "\n",
    "class AccuracyCallback(Callback):\n",
    "    \"\"\"\n",
    "    Aggregating accuracy value.\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        self.correct = 0\n",
    "\n",
    "    def on_batch_end(self, runner):\n",
    "        \"\"\"\n",
    "        On batch end action\n",
    "\n",
    "        Accumulates number of correct predictions.\n",
    "\n",
    "        Args:\n",
    "            output: dict with number of the correct predictions\n",
    "        \"\"\"\n",
    "        self.correct +=  # Calculate Accuracy\n",
    "\n",
    "    def on_loader_end(self, runner):\n",
    "        \"\"\"\n",
    "        On loader end action.\n",
    "\n",
    "        Args:\n",
    "            epoch_metrics: dict with epoch metrics\n",
    "\n",
    "        Returns:\n",
    "            accuracy value over the loader.\n",
    "        \"\"\"\n",
    "        runner.epoch_metrics[\"accuracy\"] = # Sum up metrics\n",
    "        self.correct=0\n",
    "\n",
    "\n",
    "class LoggerCallback(Callback):\n",
    "    \"\"\"\n",
    "    Log metrics to output.\n",
    "    \"\"\"\n",
    "    def __init__(self, log_period):\n",
    "        self.log_period = log_period\n",
    "\n",
    "    def on_epoch_end(self, runner):\n",
    "        \"\"\"\n",
    "        On epoch end action.\n",
    "\n",
    "        Prints all epoch metrics if log_period is suitable.\n",
    "\n",
    "        Args:\n",
    "            epoch_metrics: dict with epoch metrics\n",
    "            epoch: current epoch\n",
    "        \"\"\"\n",
    "        if runner.epoch % self.log_period == 0:\n",
    "            log_string = f\"Epoch: {runner.epoch}\\n\"\n",
    "            for metric, value in runner.epoch_metrics.items():\n",
    "                log_string += # Logging all metrics\n",
    "            print(log_string)\n",
    "\n",
    "\n",
    "class OptimizerCallback(Callback):\n",
    "    def on_batch_start(self, runner):\n",
    "        if runner.dataset_name == \"train\":\n",
    "            # Reset gradients\n",
    "\n",
    "    def on_batch_end(self, runner):\n",
    "        loss = 0\n",
    "        for data, outp in zip(runner.input, runner.output):\n",
    "            current_loss = runner.criterion(outp, data[\"targets\"])\n",
    "            loss += current_loss\n",
    "        \n",
    "        loss = loss / len(runner.input)\n",
    "        \n",
    "        if runner.dataset_name == \"train\":\n",
    "            # Optimize model's parameters"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from tqdm.notebook import tqdm\n",
    "\n",
    "\n",
    "class Runner:\n",
    "    def __init__(\n",
    "        self,\n",
    "        model,\n",
    "        criterion,\n",
    "        optimizer,\n",
    "        datasets,\n",
    "        batch_size,\n",
    "        callbacks,\n",
    "    ):\n",
    "        self.model = model\n",
    "        self.criterion = criterion\n",
    "        self.optimizer = optimizer\n",
    "        self.datasets = datasets\n",
    "\n",
    "        self.batch_size = batch_size\n",
    "        self.callbacks = callbacks\n",
    "        \n",
    "        self.input = {}\n",
    "        self.output = {}\n",
    "        self.batch_metrics = {}\n",
    "        self.loader_metrcis = {}\n",
    "        self.epoch_metrics = {}\n",
    "\n",
    "    def _handle_batch(self, batch):\n",
    "        \"\"\"\n",
    "        Stores the main logic of data aggregating.\n",
    "        \"\"\"\n",
    "        output = []\n",
    "        for data in batch:\n",
    "            # Calculate predictions\n",
    "        self.input = batch\n",
    "        self.output = output\n",
    "\n",
    "\n",
    "    def train(self, num_epochs: int = 100, verbose=False):\n",
    "        for epoch in range(num_epochs):\n",
    "            self.epoch_metrics = {}\n",
    "            self.epoch = epoch\n",
    "            for dataset_name, dataset in self.datasets.items():\n",
    "                self.dataset_name = dataset_name\n",
    "                self.loader_metrics = {}\n",
    "                current_batch = []\n",
    "\n",
    "                if verbose:\n",
    "                    iter_ = tqdm(enumerate(dataset), total=len(dataset))\n",
    "                else:\n",
    "                    iter_ = enumerate(dataset)\n",
    "\n",
    "                for idx, data in iter_:\n",
    "                    last = idx == (len(dataset)-1)\n",
    "                    current_batch.append(data)\n",
    "\n",
    "                    if last or len(current_batch) == self.batch_size:\n",
    "                        for clb in self.callbacks.values():\n",
    "                            clb.on_batch_start(self)\n",
    "\n",
    "                        # Handle batch\n",
    "                        current_batch = []\n",
    "\n",
    "                        for clb in self.callbacks.values():\n",
    "                            clb.on_batch_end(self)\n",
    "\n",
    "                for clb in self.callbacks.values():\n",
    "                    clb.on_loader_end(self)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Run training"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "criterion = CrossEntropyLoss()\n",
    "model = SimpleModel()\n",
    "optimizer = SGD(model.parameters(), lr=0.1)\n",
    "runner = Runner(\n",
    "    model=model,\n",
    "    criterion=criterion,\n",
    "    optimizer=optimizer,\n",
    "    datasets=datasets,\n",
    "    batch_size=3,\n",
    "    callbacks={\n",
    "        \"loss\": LossCallback(),\n",
    "        \"accuracy\": AccuracyCallback(),\n",
    "        \"logger\": LoggerCallback(log_period=5),\n",
    "        \"optimizer\": OptimizerCallback(),\n",
    "    }\n",
    ")\n",
    "runner.train(50)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Visualization"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "h = 0.25\n",
    "x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1\n",
    "y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1\n",
    "xx, yy = np.meshgrid(np.arange(x_min, x_max, h),\n",
    "                     np.arange(y_min, y_max, h))\n",
    "Xmesh = np.c_[xx.ravel(), yy.ravel()]\n",
    "inputs = [list(map(Value, xrow)) for xrow in Xmesh]\n",
    "scores = list(map(model, inputs))\n",
    "Z = np.array([s[1].exp().data/(s[0].exp()+s[1].exp()).data for s in scores])\n",
    "Z = Z.reshape(xx.shape)\n",
    "\n",
    "fig = plt.figure(figsize=(10, 8))\n",
    "plt.title(\"Decision boundary\", fontsize=14)\n",
    "plt.contourf(xx, yy, Z, cmap=plt.cm.Spectral, alpha=0.6)\n",
    "plt.scatter(X[:, 0], X[:, 1], c=y, s=40, cmap=plt.cm.Spectral)\n",
    "plt.xlim(xx.min(), xx.max())\n",
    "plt.ylim(yy.min(), yy.max())\n",
    "plt.show()\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## MNIST\n",
    "\n",
    "Try to train model on MNIST task!"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import mnist\n",
    "X_train, y_train, X_val, y_val, X_test, y_test = mnist.load_dataset()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "class MNISTDataset:\n",
    "    def __init__(self, x, y):\n",
    "        self.x = x\n",
    "        self.y = y\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return {\"features\": ..., \"targets\": ...}\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.y)\n",
    "\n",
    "# Python is slow, that's why we use only small group of object\n",
    "datasets = {\n",
    "    \"train\": MNISTDataset(X_train[:2000], y_train[:2000]),\n",
    "    \"valid\": MNISTDataset(X_val[:200], y_val[:200])\n",
    "} "
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "class MnistModel(Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        inp_shape=28*28,\n",
    "        out_shape=10,\n",
    "        hidden_shapes=[10, 10]\n",
    "    ):\n",
    "        # Create your model!\n",
    "\n",
    "    def forward(self, inp):\n",
    "        return ...\n",
    "\n",
    "    def parameters(self):\n",
    "        parameters = [] # Don't forget about parameters!\n",
    "        return parameters"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "criterion = CrossEntropyLoss()\n",
    "model = MnistModel()\n",
    "optimizer = SGD(model.parameters(), lr=0.1)\n",
    "runner = Runner(\n",
    "    model=model,\n",
    "    criterion=criterion,\n",
    "    optimizer=optimizer,\n",
    "    datasets=datasets,\n",
    "    batch_size=16,\n",
    "    callbacks={\n",
    "        \"loss\": LossCallback(),\n",
    "        \"accuracy\": AccuracyCallback(),\n",
    "        \"logger\": LoggerCallback(log_period=1),\n",
    "        \"optimizer\": OptimizerCallback(),\n",
    "    }\n",
    ")\n",
    "runner.train(5, verbose=True)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}