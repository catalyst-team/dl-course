{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "seminar_done",
      "private_outputs": true,
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5WVASxK_7oqI"
      },
      "source": [
        "# 5 Seminar\n",
        "\n",
        "Hi! Today we are going to learn Metric Learning. We'll start with rank metrics, then move on to the Triple Loss objective and try to use another approach to get useful vector representations of objects."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Nb5_yl-aVDxT"
      },
      "source": [
        "!pip install -U catalyst albumentations"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zn1ibXQbVHhN"
      },
      "source": [
        "from catalyst.utils import set_global_seed\n",
        "\n",
        "\n",
        "set_global_seed(42)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NcERM9BxVH1F"
      },
      "source": [
        "# Metric Learning"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BxWXtT02VJSV"
      },
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from PIL import Image"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rDbN91BXVJpX"
      },
      "source": [
        "## Metrics\n",
        "\n",
        "Metric Learning objection is a finding such a vector presenations of objects that similar object should be near and different one far away. To understand model perfomance rank metrics are used. \n",
        "\n",
        "Let's generate some ranks sequences."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QBoYfAn0it_i"
      },
      "source": [
        "target_rank = np.arange(10)[:, np.newaxis].repeat(5, axis=1).T\n",
        "predicted_rank = target_rank.copy()\n",
        "\n",
        "for i in range(5):\n",
        "    np.random.shuffle(target_rank[i])\n",
        "for i in range(5):\n",
        "    np.random.shuffle(predicted_rank[i])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zz0otQYBkIZ1"
      },
      "source": [
        "print(f\"Target rank of first class: {target_rank[0]}\")\n",
        "print(f\"Predicted rank of first class: {predicted_rank[0]}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VJugRC6JisE3"
      },
      "source": [
        "### Recall\n",
        "\n",
        "First at all, we have Recall. It's showed how much relative objects have been predicted in ratio of all objects in the data. It can use only first object or first five objects in prediction.\n",
        "\n",
        "$$\n",
        "\\text{Recall}_k = \\frac{\\text{# predicted labels that are relevant}}{\\text{# all labels}}\n",
        "$$"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rO2mLeR1ir0p"
      },
      "source": [
        "recall_at_1 = (target_rank[:,0]==predicted_rank[:,0])/target_rank.shape[0]\n",
        "print(f\"Recall@1: {recall_at_1}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rFH3EyQuk5AI"
      },
      "source": [
        "recall_at_5 = [len(np.intersect1d(target_rank[i, :5], predicted_rank[i, :5]))/target_rank.shape[1] for i in range(5)]\n",
        "print(f\"Recall@5: {recall_at_5}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vUci4AeDivWe"
      },
      "source": [
        "### mAP\n",
        "\n",
        "Precision is usually used to present a model perfomance. \n",
        "\n",
        "$$\n",
        "\\text{Precision}_k = \\frac{\\text{# predicted labels that are relevant}}{\\text{# all predicted labels}}\n",
        "$$\n",
        "\n",
        "However, for more accurate perfomnace estimation precison is weighted by recall:\n",
        "\n",
        "$$\n",
        "\\text{AP}_K = \\sum_{k = 1}^K (\\text{Recall}_k - \\text{Recall}_{k - 1}) \\text{Precision}_k\n",
        "$$\n",
        "\n",
        "To undestand entire model perfomance $\\text{AP}$ can be averaged across all classes. This metrics is called $\\text{mAP}$.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_hF997jVVKvK"
      },
      "source": [
        "recall_at = lambda k:  [len(np.intersect1d(target_rank[i, :k], predicted_rank[i, :k]))/target_rank.shape[1] for i in range(5)]\n",
        "precision_at = lambda k:  [len(np.intersect1d(target_rank[i, :k], predicted_rank[i, :k]))/k for i in range(5)]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WOeFJ-Y2GMFC"
      },
      "source": [
        "recall_at_10 = np.array([recall_at(k) for k in range(1, 11)])\n",
        "print(recall_at_10[:, 0])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YdtFjC_bGu6H"
      },
      "source": [
        "precision_at_10 = np.array([precision_at(k) for k in range(1, 11)])\n",
        "print(precision_at_10[:,0])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u65dpbY1P3yK"
      },
      "source": [
        "AP_0_at_10 = sum((recall_at_10[1:, 0]-recall_at_10[:-1, 0])*precision_at_10[1:, 0])\n",
        "print(AP_0_at_10)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IlWMLcdHnfyJ"
      },
      "source": [
        "AP_at_10 = np.sum((recall_at_10[1:]-recall_at_10[:-1])*precision_at_10[1:], 0)\n",
        "print(AP_at_10)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "McH6zybUnhL7"
      },
      "source": [
        "mAP_at_10 = np.mean(AP_at_10)\n",
        "print(mAP_at_10)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r33dVg28k5dk"
      },
      "source": [
        "### Cumulative Matching Characteristic\n",
        "Cumulative Matching Characteristics (CMC) curves are the most popular evaluation metrics for person re-identification methods. Consider a simple single-gallery-shot setting, where each gallery identity has only one instance. For each query the top-k accuracy is\n",
        "\n",
        "$$\n",
        "\\text{Acc}_k = \\cases{1, \\text{ if top-k ranked gallery samples contain the query identity} \\\\ 0,\\text{ otherwise}}\n",
        "$$\n",
        "\n",
        "\n",
        "And $\\text{CMC}$ is the average $\\text{Acc}_k$ across queries."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KlvSMRSlk7Pw"
      },
      "source": [
        "acc_k = lambda k: [int(target_rank[i, 0] in predicted_rank[i, :k]) for i in range(5)]\n",
        "print(acc_k(6))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qKj1F3iOnjcT"
      },
      "source": [
        "cmc = np.mean(acc_k(6))\n",
        "print(cmc)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P3Ag8n9LVNXY"
      },
      "source": [
        "## Omniglot\n",
        "\n",
        "Introduce new dataset: [Omniglot](https://github.com/brendenlake/omniglot). It contains 1623 different handwritten characters from 50 different alphabets. They have been written by 20 different people and collect via Amazon's Mechanical Turk. Some of the characters are presented on the picture:\n",
        "\n",
        "![photo](https://raw.githubusercontent.com/brendenlake/omniglot/master/omniglot_grid.jpg)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SeSQX6O2VOJz"
      },
      "source": [
        "# Download dataset\n",
        "!git clone https://github.com/brendenlake/omniglot\n",
        "!unzip omniglot/python/images_background\n",
        "!unzip omniglot/python/images_evaluation"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pdFbYLP2ShFZ"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_ek5rO5bIJtb"
      },
      "source": [
        "Look at the file structure. Each folder is called after alphabetic system and contains character's folder with drawn examples.\n",
        "\n",
        "Let's show the `a` character from the Latin alphabet."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PpXIVViPqWTv"
      },
      "source": [
        "%matplotlib inline\n",
        "from pathlib import Path\n",
        "\n",
        "\n",
        "latin_a = Path('images_background/') / 'Latin' / 'character01'\n",
        "paths = list(latin_a.glob(\"*.png\"))\n",
        "\n",
        "_, axs = plt.subplots(3, 5, figsize=(15, 10))\n",
        "\n",
        "for i in range(15):\n",
        "    image = Image.open(paths[i])\n",
        "    axs[i % 3][i % 5].imshow(image)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pnzArRZjIpAk"
      },
      "source": [
        "We use `images_background` as train part of dataset and `images_evaluation` as valid part."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SLKviaYGBCkI"
      },
      "source": [
        "data = {\n",
        "    \"train\": {\n",
        "        \"labels\": [],\n",
        "        \"image_paths\": [],\n",
        "        \"cls2label\": {},\n",
        "        \"root\": Path(\"images_background\"),\n",
        "    },\n",
        "    \"valid\": {\n",
        "        \"labels\": [],\n",
        "        \"image_paths\": [],\n",
        "        \"cls2label\": {},\n",
        "        \"root\": Path(\"images_evaluation\"),\n",
        "    },\n",
        "}\n",
        "\n",
        "for part in data:\n",
        "    cur_part = data[part]\n",
        "    for alphabet_path in cur_part[\"root\"].iterdir():\n",
        "        for ch in alphabet_path.iterdir():\n",
        "            cur_part[\"labels\"].extend(f\"{alphabet_path.name}_{ch.name}\" for _ in ch.glob(\"*.png\"))\n",
        "            cur_part[\"image_paths\"].extend(ch.glob(\"*.png\"))\n",
        "\n",
        "    cur_part[\"labels\"] = np.array(cur_part[\"labels\"])\n",
        "    cur_part[\"image_paths\"] = np.array(cur_part[\"image_paths\"])\n",
        "    cur_part[\"cls2label\"] = {c: num for num, c in enumerate(np.unique(cur_part[\"labels\"]))}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6RyJZHIgVO0p"
      },
      "source": [
        "## Metric Learning Pipeline\n",
        "\n",
        "Look at the Metric Learning pipeline in training stage\n",
        "\n",
        "![training stage](https://miro.medium.com/max/1400/1*_kwjkuV7MtJCCYriwwo3uA.png)\n",
        "\n",
        "and in validation stage \n",
        "\n",
        "![validating stage](https://miro.medium.com/max/1400/1*w3NVYqXA_e-EwWrvnvvrDw.png)\n",
        "\n",
        "\n",
        "(from [Metric Learning with Catalyst](https://medium.com/pytorch/metric-learning-with-catalyst-8c8337dfab1a) by [@AlexeySh](https://github.com/AlekseySh))\n",
        "\n",
        "To train model for Netric Learning objective, we need:\n",
        "\n",
        "- Metric Learning Dataset for training\n",
        "- Query/Gallery Dataset for validation\n",
        "- In-batch sampler of triplets\n",
        "- Triplet Loss criterion\n",
        "\n",
        "Implement each part step by step.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-hO8W9rWAOJe"
      },
      "source": [
        "### Data Loaders"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vbNLSRhki7HI"
      },
      "source": [
        "Metric Learning Dataset is similar with usual dataset. One difference is the `get_labels` method. It should return all labels in dataset. That used for class balanced batch sampler."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jjPyLMj0bucU"
      },
      "source": [
        "import torch\n",
        "from catalyst.contrib.utils.image import imread\n",
        "from catalyst.data.dataset.metric_learning import (\n",
        "    MetricLearningTrainDataset,\n",
        "    QueryGalleryDataset,\n",
        ")\n",
        "from torch.utils.data import Dataset\n",
        "\n",
        "\n",
        "class OmniGlotMLDataset(MetricLearningTrainDataset):\n",
        "    def __init__(self, images, targets, transform=None):\n",
        "        self.images = images\n",
        "        self.targets = targets\n",
        "        self.transform = transform\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        img, target = self.images[index], int(self.targets[index])\n",
        "        img = imread(img)\n",
        "\n",
        "        if self.transform is not None:\n",
        "            img = self.transform(image=img)[\"image\"]\n",
        "\n",
        "        return img, target\n",
        "\n",
        "    def get_labels(self):\n",
        "        return self.targets.tolist()\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.images.shape[0]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x9UymmY4jnSz"
      },
      "source": [
        "For validation, we need to create an Query-Gallery Dataset. Some of the object must be marked as query."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gi_-V_dpqWNw"
      },
      "source": [
        "class OmniGlotQGDataset(QueryGalleryDataset):\n",
        "    def __init__(\n",
        "        self, images, targets, transform=None, gallery_fraq=0.2,\n",
        "    ):\n",
        "        self._omniglot = OmniGlotMLDataset(\n",
        "            images=images, targets=targets, transform=transform\n",
        "        )\n",
        "\n",
        "        self._gallery_size = int(gallery_fraq * len(self._omniglot))\n",
        "        self._query_size = len(self._omniglot) - self._gallery_size\n",
        "\n",
        "        self._is_query = torch.zeros(len(self._omniglot)).type(torch.bool)\n",
        "        self._is_query[: self._query_size] = True\n",
        "\n",
        "    def __getitem__(self, idx: int):\n",
        "        image, label = self._omniglot[idx]\n",
        "        return {\n",
        "            \"features\": image,\n",
        "            \"targets\": label,\n",
        "            \"is_query\": self._is_query[idx],\n",
        "        }\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self._omniglot)\n",
        "\n",
        "    @property\n",
        "    def gallery_size(self):\n",
        "        return self._gallery_size\n",
        "\n",
        "    @property\n",
        "    def query_size(self):\n",
        "        return self._query_size"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wfMjYZUtJkxc"
      },
      "source": [
        "We need `BalanceBatchSampler`. This sampler behaviour guarantees that we can always form the triplets inside the batch and overcome classes’ imbalance\n",
        "\n",
        "These code is copied from the `Catalyst` codebase."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qP03eZHE_XaK"
      },
      "source": [
        "from torch.utils.data import DataLoader, Sampler\n",
        "\n",
        "\n",
        "class BalanceBatchSampler(Sampler):\n",
        "    \"\"\"\n",
        "    This kind of sampler can be used for both metric learning and\n",
        "    classification task.\n",
        "    Sampler with the given strategy for the C unique classes dataset:\n",
        "    - Selection P of C classes for the 1st batch\n",
        "    - Selection K instances for each class for the 1st batch\n",
        "    - Selection P of C - P remaining classes for 2nd batch\n",
        "    - Selection K instances for each class for the 2nd batch\n",
        "    - ...\n",
        "    The epoch ends when there are no classes left.\n",
        "    So, the batch sise is P * K except the last one.\n",
        "    Thus, in each epoch, all the classes will be selected once, but this\n",
        "    does not mean that all the instances will be selected during the epoch.\n",
        "    One of the purposes of this sampler is to be used for\n",
        "    forming triplets and pos/neg pairs inside the batch.\n",
        "    To guarante existance of these pairs in the batch,\n",
        "    P and K should be > 1. (1)\n",
        "    Behavior in corner cases:\n",
        "    - If a class does not contain K instances,\n",
        "    a choice will be made with repetition.\n",
        "    - If C % P == 1 then one of the classes should be dropped\n",
        "    otherwise statement (1) will not be met.\n",
        "    This type of sampling can be found in the classical paper of Person Re-Id,\n",
        "    where P equals 32 and K equals 4:\n",
        "    `In Defense of the Triplet Loss for Person Re-Identification`_.\n",
        "    .. _In Defense of the Triplet Loss for Person Re-Identification:\n",
        "        https://arxiv.org/abs/1703.07737\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, labels, p, k):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            labels: list of classes labeles for each elem in the dataset\n",
        "            p: number of classes in a batch, should be > 1\n",
        "            k: number of instances of each class in a batch, should be > 1\n",
        "        \"\"\"\n",
        "        super().__init__(self)\n",
        "        classes = set(labels)\n",
        "\n",
        "        self._labels = labels\n",
        "        self._p = p\n",
        "        self._k = k\n",
        "\n",
        "        self._batch_size = self._p * self._k\n",
        "        self._classes = classes\n",
        "\n",
        "        # to satisfy statement (1)\n",
        "        num_classes = len(self._classes)\n",
        "        if num_classes % self._p == 1:\n",
        "            self._num_epoch_classes = num_classes - 1\n",
        "        else:\n",
        "            self._num_epoch_classes = num_classes\n",
        "\n",
        "    @property\n",
        "    def batch_size(self):\n",
        "        \"\"\"\n",
        "        Returns:\n",
        "            this value should be used in DataLoader as batch size\n",
        "        \"\"\"\n",
        "        return self._batch_size\n",
        "\n",
        "    @property\n",
        "    def batches_in_epoch(self):\n",
        "        \"\"\"\n",
        "        Returns:\n",
        "            number of batches in an epoch\n",
        "        \"\"\"\n",
        "        return int(np.ceil(self._num_epoch_classes / self._p))\n",
        "\n",
        "    def __len__(self):\n",
        "        \"\"\"\n",
        "        Returns:\n",
        "            number of samples in an epoch\n",
        "        \"\"\"\n",
        "        return self._num_epoch_classes * self._k\n",
        "\n",
        "    def __iter__(self):\n",
        "        \"\"\"\n",
        "        Returns:\n",
        "            indeces for sampling dataset elems during an epoch\n",
        "        \"\"\"\n",
        "        inds = []\n",
        "\n",
        "        for cls_id in sample(self._classes, self._num_epoch_classes):\n",
        "            all_cls_inds = find_value_ids(self._labels, cls_id)\n",
        "\n",
        "            # we've checked in __init__ that this value must be > 1\n",
        "            num_samples_exists = len(all_cls_inds)\n",
        "\n",
        "            if num_samples_exists < self._k:\n",
        "                selected_inds = sample(\n",
        "                    all_cls_inds, k=num_samples_exists\n",
        "                ) + choices(all_cls_inds, k=self._k - num_samples_exists)\n",
        "            else:\n",
        "                selected_inds = sample(all_cls_inds, k=self._k)\n",
        "\n",
        "            inds.extend(selected_inds)\n",
        "\n",
        "        return iter(inds)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pWGIOvpKKXGF"
      },
      "source": [
        "We won't use augmentation in our pipeline. But the reason we'll use `albumentations` is that it has convinient tools for the image preprocessing. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T0Hm6nievNak"
      },
      "source": [
        "import albumentations as albu\n",
        "from albumentations.pytorch import ToTensorV2 as ToTensor\n",
        "\n",
        "\n",
        "IMAGE_SIZE = 64\n",
        "\n",
        "transform = albu.Compose([\n",
        "    albu.LongestMaxSize(IMAGE_SIZE),\n",
        "    albu.PadIfNeeded(IMAGE_SIZE, IMAGE_SIZE, border_mode=0),\n",
        "    albu.Normalize(),\n",
        "    ToTensor()\n",
        "])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SH1oDudDK1Pc"
      },
      "source": [
        "Process train/valid data, create datasets and loaders."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ND43OdI9h7Q5"
      },
      "source": [
        "train_images = data[\"train\"][\"image_paths\"]\n",
        "train_labels = data[\"train\"][\"labels\"]\n",
        "\n",
        "indexes = np.arange(train_images.shape[0])\n",
        "np.random.shuffle(indexes)\n",
        "train_images = train_images[indexes]\n",
        "train_targets = np.array([data[\"train\"][\"cls2label\"][c] for c in train_labels])[indexes]\n",
        "\n",
        "dataset_train = OmniGlotMLDataset(images=train_images, targets=train_targets, transform=transform)\n",
        "sampler = BalanceBatchSampler(labels=dataset_train.get_labels(), p=8, k=16)\n",
        "train_loader = DataLoader(dataset=dataset_train, sampler=sampler, batch_size=sampler.batch_size)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lTryxzI1Atkc"
      },
      "source": [
        "valid_images = data[\"valid\"][\"image_paths\"]\n",
        "valid_labels = data[\"valid\"][\"labels\"]\n",
        "valid_targets = np.array([data[\"valid\"][\"cls2label\"][c] for c in valid_labels])\n",
        "dataset_valid = OmniGlotQGDataset(\n",
        "    images=valid_images,\n",
        "    targets=valid_targets,\n",
        "    transform=transform,\n",
        "    gallery_fraq=0.4,\n",
        ")\n",
        "valid_loader = DataLoader(dataset=dataset_valid, batch_size=256)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T8pjY2glAQnT"
      },
      "source": [
        "### Training\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zIrqr5r7LAm_"
      },
      "source": [
        "We need the triplet generator for metric learning pipeline. The most popular one is `AllTriplets` and `HardTrpilet`. They are implemented in the catalyst. However, let's look their through code deeply."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jfrefovqAjBk"
      },
      "source": [
        "from itertools import combinations, product\n",
        "from random import sample\n",
        "from sys import maxsize\n",
        "\n",
        "from catalyst.data import InBatchTripletsSampler\n",
        "from catalyst.utils.misc import find_value_ids\n",
        "from catalyst.utils import convert_labels2list\n",
        "from torch.nn.functional import normalize\n",
        "\n",
        "\n",
        "class AllTripletsSampler(InBatchTripletsSampler):\n",
        "    \"\"\"\n",
        "    This sampler selects all the possible triplets for the given labels\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, max_output_triplets=maxsize):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            max_output_triplets: with the strategy of choosing all\n",
        "                the triplets, their number in the batch can be very large,\n",
        "                because of it we can sample only random part of them,\n",
        "                determined by this parameter.\n",
        "        \"\"\"\n",
        "        self._max_out_triplets = max_output_triplets\n",
        "\n",
        "    def _sample(self, *_, labels):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            labels: labels of the samples in the batch\n",
        "            *_: note, that we ignore features argument\n",
        "        Returns: indeces of triplets\n",
        "        \"\"\"\n",
        "        num_labels = len(labels)\n",
        "\n",
        "        triplets = []\n",
        "        for label in set(labels):\n",
        "            ids_pos_cur = [i for i in range(num_labels) if labels[i] == label]\n",
        "            ids_neg_cur = [i for i in range(num_labels) if labels[i] != label]\n",
        "\n",
        "            pos_pairs = list(combinations(ids_pos_cur, r=2))\n",
        "\n",
        "            tri = [(a, p, n) for (a, p), n in product(pos_pairs, ids_neg_cur)]\n",
        "            triplets.extend(tri)\n",
        "\n",
        "        triplets = sample(triplets, min(len(triplets), self._max_out_triplets))\n",
        "        ids_anchor, ids_pos, ids_neg = zip(*triplets)\n",
        "\n",
        "        return list(ids_anchor), list(ids_pos), list(ids_neg)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pmlFkb9HLndy"
      },
      "source": [
        "Torch has the TripletLoss criterion. But it doesn't work with in-batch sampler. We must join these two enteties in new one. It has to sample triplets by the given rule and calculate triplet loss with the margin."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7q0UsHNtAkJs"
      },
      "source": [
        "import torch.nn as nn\n",
        "\n",
        "\n",
        "class TripletMarginLossWithSampler(nn.Module):\n",
        "    \"\"\"\n",
        "    This class combines in-batch sampling of triplets and\n",
        "    default TripletMargingLoss from PyTorch.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, margin, sampler_inbatch):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            margin: margin value\n",
        "            sampler_inbatch: sampler for forming triplets inside the batch\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "        self._sampler_inbatch = sampler_inbatch\n",
        "        self._triplet_margin_loss = TripletMarginLoss(margin=margin)\n",
        "\n",
        "    def forward(self, features, labels):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            features: features with the shape of [batch_size, features_dim]\n",
        "            labels: labels of samples having batch_size elements\n",
        "        Returns: loss value\n",
        "        \"\"\"\n",
        "        labels_list = convert_labels2list(labels)\n",
        "\n",
        "        (\n",
        "            features_anchor,\n",
        "            features_positive,\n",
        "            features_negative,\n",
        "        ) = self._sampler_inbatch.sample(features=features, labels=labels_list)\n",
        "\n",
        "        loss = self._triplet_margin_loss(\n",
        "            anchor=features_anchor,\n",
        "            positive=features_positive,\n",
        "            negative=features_negative,\n",
        "        )\n",
        "        return loss"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FbHDxzRvMWNr"
      },
      "source": [
        "Create simple model with pretrained MobileNet backbone. It's able to increase training time. Good model must be trained for long time (several hundreads epochs). For educational purposes, we adjust train time to one epoch."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ULcfnZSWVUJI"
      },
      "source": [
        "from catalyst import dl, utils\n",
        "from catalyst.contrib import models\n",
        "from catalyst.contrib.nn import TripletMarginLossWithSampler, RAdam\n",
        "\n",
        "from torchvision.models import mobilenet_v2\n",
        "\n",
        "base_model = mobilenet_v2(pretrained=True)\n",
        "model = nn.Sequential(\n",
        "    base_model.features, nn.AdaptiveMaxPool2d((1, 1)), nn.Flatten()\n",
        ")\n",
        "optimizer = RAdam(model.parameters(), lr=0.01)\n",
        "\n",
        "# 3. criterion with triplets sampling\n",
        "sampler_inbatch = AllTripletsSampler()\n",
        "criterion = TripletMarginLossWithSampler(\n",
        "    margin=0.4, sampler_inbatch=sampler_inbatch\n",
        ")\n",
        "\n",
        "# 4. training with catalyst Runner\n",
        "callbacks = [\n",
        "    dl.CriterionCallback(\"embeddings\", \"targets\", \"loss\"),\n",
        "    dl.ControlFlowCallback(\n",
        "        dl.CMCScoreCallback(\"embeddings\", \"targets\", \"is_query\", topk_args=[5]),\n",
        "        loaders=\"valid\",\n",
        "    ),\n",
        "    dl.PeriodicLoaderCallback(\"valid\", \"cmc05\", valid=1, minimize=False),\n",
        "]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DxUcNn0MB0HS"
      },
      "source": [
        "runner = dl.SupervisedRunner(input_key=\"features\", output_key=\"embeddings\")\n",
        "runner.train(\n",
        "    model=model,\n",
        "    criterion=criterion,\n",
        "    optimizer=optimizer,\n",
        "    callbacks=callbacks,\n",
        "    loaders={\"train\": train_loader, \"valid\": valid_loader},\n",
        "    verbose=True,\n",
        "    valid_metric = 'cmc05',\n",
        "    minimize_valid_metric=False,\n",
        "    valid_loader=\"valid\",\n",
        "    num_epochs=1\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8YXQ16mjM7se"
      },
      "source": [
        "Let's implement `HardTriplet` sampler and compare it with the `AllTriplet` one."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dEl-BRyk4PXT"
      },
      "source": [
        "class HardTripletsSampler(InBatchTripletsSampler):\n",
        "    \"\"\"\n",
        "    This sampler selects hardest triplets based on distances between features:\n",
        "    the hardest positive sample has the maximal distance to the anchor sample,\n",
        "    the hardest negative sample has the minimal distance to the anchor sample.\n",
        "    Note that a typical triplet loss chart is as follows:\n",
        "    1. Falling: loss decreases to a value equal to the margin.\n",
        "    2. Long plato: the loss oscillates near the margin.\n",
        "    3. Falling: loss decreases to zero.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, norm_required=False):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            norm_required: set True if features normalisation is needed\n",
        "        \"\"\"\n",
        "        self._norm_required = norm_required\n",
        "\n",
        "    def _sample(self, features, labels):\n",
        "        \"\"\"\n",
        "        This method samples the hardest triplets inside the batch.\n",
        "        Args:\n",
        "            features: has the shape of [batch_size, feature_size]\n",
        "            labels: labels of the samples in the batch\n",
        "        Returns:\n",
        "            the batch of the triplets in the order below:\n",
        "            (anchor, positive, negative)\n",
        "        \"\"\"\n",
        "        assert features.shape[0] == len(labels)\n",
        "\n",
        "        if self._norm_required:\n",
        "            features = normalize(samples=features.detach())\n",
        "\n",
        "        dist_mat = torch.cdist(x1=features, x2=features, p=2)\n",
        "\n",
        "        ids_anchor, ids_pos, ids_neg = self._sample_from_distmat(\n",
        "            distmat=dist_mat, labels=labels\n",
        "        )\n",
        "\n",
        "        return ids_anchor, ids_pos, ids_neg\n",
        "\n",
        "    @staticmethod\n",
        "    def _sample_from_distmat(distmat, labels):\n",
        "        \"\"\"\n",
        "        This method samples the hardest triplets based on the given\n",
        "        distances matrix. It chooses each sample in the batch as an\n",
        "        anchor and then finds the harderst positive and negative pair.\n",
        "        Args:\n",
        "            distmat: matrix of distances between the features\n",
        "            labels: labels of the samples in the batch\n",
        "        Returns:\n",
        "            the batch of triplets in the order below:\n",
        "            (anchor, positive, negative)\n",
        "        \"\"\"\n",
        "        ids_all = set(range(len(labels)))\n",
        "\n",
        "        ids_anchor, ids_pos, ids_neg = [], [], []\n",
        "\n",
        "        for i_anch, label in enumerate(labels):\n",
        "            ids_label = set(find_value_ids(it=labels, value=label))\n",
        "\n",
        "            ids_pos_cur = np.array(list(ids_label - {i_anch}), int)\n",
        "            ids_neg_cur = np.array(list(ids_all - ids_label), int)\n",
        "\n",
        "            i_pos = ids_pos_cur[distmat[i_anch, ids_pos_cur].argmax()]\n",
        "            i_neg = ids_neg_cur[distmat[i_anch, ids_neg_cur].argmin()]\n",
        "\n",
        "            ids_anchor.append(i_anch)\n",
        "            ids_pos.append(i_pos)\n",
        "            ids_neg.append(i_neg)\n",
        "\n",
        "        return ids_anchor, ids_pos, ids_neg"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "giXpKuUPNIwn"
      },
      "source": [
        "Check that all parameters are the same."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HtvlKnoVlA0t"
      },
      "source": [
        "base_model = mobilenet_v2(pretrained=True)\n",
        "model = nn.Sequential(\n",
        "    base_model.features, nn.AdaptiveMaxPool2d((1, 1)), nn.Flatten()\n",
        ")\n",
        "optimizer = RAdam(model.parameters(), lr=0.01)\n",
        "\n",
        "# 3. criterion with triplets sampling\n",
        "sampler_inbatch = HardTripletsSampler()\n",
        "criterion = TripletMarginLossWithSampler(\n",
        "    margin=0.4, sampler_inbatch=sampler_inbatch\n",
        ")\n",
        "\n",
        "# 4. training with catalyst Runner\n",
        "callbacks = [\n",
        "    dl.CriterionCallback(\"embeddings\", \"targets\", \"loss\"),\n",
        "    dl.ControlFlowCallback(\n",
        "        dl.CMCScoreCallback(\"embeddings\", \"targets\", \"is_query\", topk_args=[5]),\n",
        "        loaders=\"valid\",\n",
        "    ),\n",
        "    dl.PeriodicLoaderCallback(\"valid\", \"cmc05\", valid=3)]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aTkZMdM43r5n"
      },
      "source": [
        "runner = dl.SupervisedRunner(input_key=\"features\", output_key=\"embeddings\")\n",
        "runner.train(\n",
        "    model=model,\n",
        "    criterion=criterion,\n",
        "    optimizer=optimizer,\n",
        "    callbacks=callbacks,\n",
        "    loaders={\"train\": train_loader, \"valid\": valid_loader},\n",
        "    verbose=True,\n",
        "    valid_metric = 'cmc05',\n",
        "    minimize_valid_metric=False,\n",
        "    valid_loader=\"valid\",\n",
        "    num_epochs=3\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1xFowdyrtWVT"
      },
      "source": [
        "## *Face for Class Identification\n",
        "\n",
        "![](https://asset-pdf.scinapse.io/prod/2786817236/figures/figure-2.jpg)\n",
        "\n",
        "Another way to train vector presentation is using *Face methods: SphereFace, CosFace and ArcFace. It used simple `CrossEntropyLoss` with additional techniques to separate classes in a hyperbolic sphere. We'll implement `CosFace`. It's easy to understand and to write."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MhbkXn3W4x4O"
      },
      "source": [
        "import math\n",
        "\n",
        "import torch.nn.functional as F\n",
        "\n",
        "\n",
        "class CosFace(nn.Module):\n",
        "    def __init__(\n",
        "        self, in_features, out_features, s: float = 10, m: float = 0.25,\n",
        "    ):\n",
        "        super(CosFace, self).__init__()\n",
        "        self.in_features = in_features\n",
        "        self.out_features = out_features\n",
        "        self.s = s\n",
        "        self.m = m\n",
        "\n",
        "        self.weight = nn.Parameter(\n",
        "            torch.FloatTensor(out_features, in_features)\n",
        "        )\n",
        "        nn.init.xavier_uniform_(self.weight)\n",
        "\n",
        "    def forward(self, input, target):\n",
        "        cosine = normalize(input) @ normalize(weight).T\n",
        "        phi = cosine - self.m\n",
        "        one_hot = torch.zeros(cosine.size()).to(input.device)\n",
        "        one_hot.scatter_(1, target.view(-1, 1).long(), 1)\n",
        "        logits = (one_hot * phi) + ((1.0 - one_hot) * cosine)\n",
        "        logits *= self.s\n",
        "\n",
        "        return logits"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OnvZmu80k-4c"
      },
      "source": [
        "Notice that we don't need special sampler for this pipeline. It can be added as class balancer, but the pipeline can be simplified."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CjwqZtaTQftA"
      },
      "source": [
        "dataset_train = OmniGlotMLDataset(images=train_images, targets=train_targets, transform=transform)\n",
        "train_loader = DataLoader(dataset=dataset_train, batch_size=sampler.batch_size)\n",
        "\n",
        "dataset_valid = OmniGlotQGDataset(\n",
        "    images=valid_images,\n",
        "    targets=valid_targets,\n",
        "    transform=transform,\n",
        "    gallery_fraq=0.2,\n",
        ")\n",
        "valid_loader = DataLoader(dataset=dataset_valid, batch_size=256)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "foiAVGfTlbjk"
      },
      "source": [
        "We need the special model class and runner. A model should process features and targets in training procedure. And in validation period the model returns only extracted feature vector."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jEYnqQH34So0"
      },
      "source": [
        "class OmniGlotModel(nn.Module):\n",
        "    def __init__(self, num_training_classes):\n",
        "        super().__init__()\n",
        "\n",
        "        base_model = mobilenet_v2(pretrained=True)\n",
        "        self.feature_extractor = nn.Sequential(\n",
        "            base_model.features, nn.AdaptiveMaxPool2d((1, 1)), nn.Flatten(),\n",
        "        )\n",
        "\n",
        "        self.face = CosFace(1280, num_training_classes)\n",
        "\n",
        "    def forward(self, features, targets=None):\n",
        "        features = self.feature_extractor(features)\n",
        "        if self.training and (targets is not None):\n",
        "            output = self.face(features, targets)\n",
        "        else:\n",
        "            output = normalize(features) * self.face.s\n",
        "        return output"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O811x7r5N0Zy"
      },
      "source": [
        "class FaceRunner(dl.SupervisedRunner):\n",
        "    def predict_batch(self, batch):\n",
        "        return {\n",
        "            \"features\": self.model(batch[\"features\"].to(self.device)),\n",
        "            \"targets\": batch[\"targets\"],\n",
        "        }\n",
        "\n",
        "    def _handle_batch(self, batch):\n",
        "        if self.model.training:\n",
        "            y_pred = self.model(batch[\"features\"], batch[\"targets\"])\n",
        "        else:\n",
        "            y_pred = self.model(batch[\"features\"])\n",
        "        self.output = {\"logits\": y_pred}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RMZM_wTUl_SR"
      },
      "source": [
        "Let's train and compare results with TripletLoss."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K6PDztgUtgpu"
      },
      "source": [
        "model = OmniGlotModel(np.unique(train_labels).shape[0])\n",
        "optimizer = RAdam(model.parameters(), lr=0.1)\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "callbacks = [\n",
        "    dl.CriterionCallback('logits', 'targets', 'loss'),\n",
        "    dl.ControlFlowCallback(\n",
        "        dl.CMCScoreCallback('logits', 'targets', 'is_query', topk_args=[5]),\n",
        "        loaders = 'valid'\n",
        "    ),\n",
        "    dl.PeriodicLoaderCallback('valid', 'cmc05', valid=5)\n",
        "]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jqfpF45m60f0"
      },
      "source": [
        "runner = FaceRunner()\n",
        "runner.train(\n",
        "    model=model,\n",
        "    criterion=criterion,\n",
        "    optimizer=optimizer,\n",
        "    callbacks=callbacks,\n",
        "    loaders={\"train\": train_loader, \"valid\": valid_loader},\n",
        "    verbose=True,\n",
        "    valid_metric = 'cmc05',\n",
        "    minimize_valid_metric=False,\n",
        "    valid_loader=\"valid\",\n",
        "    num_epochs=5\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c_pJ-_UfhOOJ"
      },
      "source": [
        "## Visualization\n",
        "\n",
        "Look at the final results! You need to project feature vectors to 2D space and plot them."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1gmfLB0Lj4qa"
      },
      "source": [
        "!pip install umap-learn"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JASzLt3KhtJ6"
      },
      "source": [
        "import seaborn as sns\n",
        "\n",
        "from umap import UMAP"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EKICQrGkiJLB"
      },
      "source": [
        "results = {\"features\": [], \"targets\": []}\n",
        "for prediction in runner.predict_loader(loader=valid_loader):\n",
        "    results[\"features\"].extend(\n",
        "        feature for feature in prediction[\"features\"].cpu().numpy()\n",
        "    )\n",
        "    results[\"targets\"].extend(\n",
        "        feature for feature in prediction[\"targets\"].cpu().numpy()\n",
        "    )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k6oVu5NjiLsw"
      },
      "source": [
        "proj = UMAP(n_components=2)\n",
        "new_features = proj.fit_transform(np.array(results[\"features\"])[np.isin(results[\"targets\"],np.arange(10))])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MZwTP55WkjK_"
      },
      "source": [
        "plot = {\n",
        "    \"x\": new_features[:, 0],\n",
        "    \"y\": new_features[:, 1],\n",
        "    \"color\": np.array(results[\"targets\"])[np.isin(results[\"targets\"],np.arange(10))]\n",
        "}\n",
        "\n",
        "sns.set()\n",
        "fig, ax = plt.subplots(figsize=(15, 10))\n",
        "sns.scatterplot(x=\"x\", y=\"y\", hue=\"color\", data=plot, ax=ax)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZfI28pZxU603"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}