{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "segmentation_baseline.ipynb",
      "provenance": [],
      "private_outputs": true,
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jcxpU7lw-k89",
        "pycharm": {
          "name": "#%% md\n"
        }
      },
      "source": [
        "# Segmentation task\n",
        "\n",
        "Hi! It's a segmentation task baseline notebook.\n",
        "It include a data reader, baseline model and submission generator.\n",
        "\n",
        "You should use GPU to train your model, so we recommend using [Kaggle Notebooks](https://www.kaggle.com/docs/notebooks).\n",
        "To get maximum score of the task, your model should have IoU greater than `0.8`.\n",
        "\n",
        "You can use everything, that suits into the rules in `README.md`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "MPpX6TsaEFwW"
      },
      "source": [
        "# !pip install -U catalyst albumentations"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ia-jKNpv-uiS"
      },
      "source": [
        "# For Colab user: download dataset and upload zip files.\n",
        "# If you use Kaggle Notebooks, you already have the dataset in a hard drive.\n",
        "\n",
        "# !gdown https://drive.google.com/uc?id=19fBCItau0MP1ABKlBNkpj1pMxOzIZLML&export=download\n",
        "# !gdown https://drive.google.com/uc?id=1X7TLVCvi2a57SyjAdExPppzakxfenRD0&export=download\n",
        "# !unzip train.zip -d train\n",
        "# !unzip test.zip -d test"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XoX11UWh-uIJ"
      },
      "source": [
        "from datetime import datetime\n",
        "import numpy as np\n",
        "from pathlib import Path\n",
        "import pandas as pd\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils import data\n",
        "\n",
        "import catalyst\n",
        "from catalyst import dl\n",
        "from catalyst.utils import metrics, imread, set_global_seed"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SaqjtWTwlXAM"
      },
      "source": [
        "set_global_seed(42)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FO6rmbPvknSQ",
        "pycharm": {
          "name": "#%% md\n"
        }
      },
      "source": [
        "## Dataset\n",
        "\n",
        "Load train data. Don't forget to add test data. Use test data, to compare methods/models/etc."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "iVn6EFYFEFxB"
      },
      "source": [
        "train_image_path = Path(\"train\") / \"images\"\n",
        "train_mask_path = Path(\"train\") / \"masks\"\n",
        "ALL_IMAGES = sorted(train_image_path.glob(\"*.png\"))\n",
        "ALL_MASKS = sorted(train_mask_path.glob(\"*.png\"))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n-po_PN9-xXc"
      },
      "source": [
        "from torch.utils.data import Dataset\n",
        "\n",
        "\n",
        "class SegmentationDataset(Dataset):\n",
        "    def __init__(self, images=None, masks=None, transforms=None) -> None:\n",
        "        self.images = images\n",
        "        self.masks = masks\n",
        "        self.transforms = transforms\n",
        "\n",
        "    def __len__(self) -> int:\n",
        "        return len(self.images)\n",
        "\n",
        "    def __getitem__(self, idx: int) -> dict:\n",
        "        image_path = self.images[idx]\n",
        "        image = imread(image_path)\n",
        "\n",
        "        result = {\"image\": image}\n",
        "\n",
        "        if self.masks is not None:\n",
        "            result[\"mask\"] = imread(self.masks[idx]).mean(2) // 255\n",
        "\n",
        "        if self.transforms is not None:\n",
        "            result = self.transforms(**result)\n",
        "            if result.get(\"mask\", None) is not None:\n",
        "                result[\"mask\"] = result[\"mask\"].unsqueeze(0)\n",
        "\n",
        "        result[\"filename\"] = image_path.name\n",
        "        result[\"image size\"] = image.shape[:2]\n",
        "\n",
        "        return result"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i4Ra4BVlkEXD",
        "pycharm": {
          "name": "#%% md\n"
        }
      },
      "source": [
        "## Augmentations\n",
        "\n",
        "To train an accurate model for a segmentation task, you need a lot of data.\n",
        "Use data augmentations to simulate a bigger dataset."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "QUAVP6dUEFxN"
      },
      "source": [
        "import albumentations as albu\n",
        "from albumentations.pytorch import ToTensorV2 as ToTensor\n",
        "import cv2\n",
        "\n",
        "IMAGE_SIZE = 256\n",
        "train_transform = albu.Compose([\n",
        "    albu.HorizontalFlip(p=0.5),\n",
        "    albu.Resize(IMAGE_SIZE, IMAGE_SIZE),\n",
        "    albu.RandomResizedCrop(IMAGE_SIZE, IMAGE_SIZE, p=0.3),\n",
        "    albu.Normalize(),\n",
        "    ToTensor()\n",
        "])\n",
        "\n",
        "valid_transform = albu.Compose([\n",
        "    albu.Resize(IMAGE_SIZE, IMAGE_SIZE),\n",
        "    albu.Normalize(),\n",
        "    ToTensor()\n",
        "])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kxI3m4fPkEU3"
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "batch_size = 16\n",
        "num_workers = 4\n",
        "\n",
        "indices = np.arange(len(ALL_IMAGES))\n",
        "\n",
        "train_indices, valid_indices = train_test_split(\n",
        "    indices, test_size=0.2, random_state=42, shuffle=True\n",
        ")\n",
        "\n",
        "np_images = np.array(ALL_IMAGES)\n",
        "np_masks = np.array(ALL_MASKS)\n",
        "\n",
        "train_dataset = SegmentationDataset(\n",
        "    images = np_images[train_indices].tolist(),\n",
        "    masks = np_masks[train_indices].tolist(),\n",
        "    transforms = train_transform\n",
        ")\n",
        "\n",
        "valid_dataset = SegmentationDataset(\n",
        "    images = np_images[valid_indices].tolist(),\n",
        "    masks = np_masks[valid_indices].tolist(),\n",
        "    transforms = valid_transform\n",
        ")\n",
        "\n",
        "loaders = {\n",
        "    \"train\": DataLoader(\n",
        "        train_dataset,\n",
        "        batch_size=batch_size,\n",
        "        shuffle=True,\n",
        "        num_workers=num_workers,\n",
        "        drop_last=True,\n",
        "    ),\n",
        "    \"valid\": DataLoader(\n",
        "        valid_dataset,\n",
        "        batch_size=batch_size,\n",
        "        shuffle=False,\n",
        "        num_workers=num_workers,\n",
        "        drop_last=True,\n",
        "    )\n",
        "}\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "id": "f-i6VRW4EFxV"
      },
      "source": [
        "Our current baseline model is `U-Net`.\n",
        "You can do anything with it: add pretrained backbone, make model wider or deeper or change a model architecture.\n",
        "You can use `torchvision` module to create a backbone, but not a whole model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iyw0JH3R-z6m"
      },
      "source": [
        "class Baseline(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        \n",
        "        self.down_1 = self.make_down_layer_(3, 64)\n",
        "        self.down_2 = self.make_down_layer_(64, 128)\n",
        "        self.down_3 = self.make_down_layer_(128, 256)\n",
        "        self.down_4 = self.make_down_layer_(256, 512)\n",
        "\n",
        "        self.up_1 = self.make_up_layer_(512, 256)\n",
        "        self.up_2 = self.make_up_layer_(256, 128)\n",
        "        self.up_3 = self.make_up_layer_(128, 64)\n",
        "        self.up_4 = nn.Sequential(\n",
        "            nn.ConvTranspose2d(64, 1, kernel_size=3, padding=1, stride=2, output_padding=1)\n",
        "        )\n",
        "\n",
        "    def make_down_layer_(self, in_channels, out_channels):\n",
        "        return nn.Sequential(\n",
        "            nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1),\n",
        "            nn.BatchNorm2d(out_channels),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(out_channels, out_channels, kernel_size=3, padding=1),\n",
        "            nn.BatchNorm2d(out_channels),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n",
        "        )\n",
        "\n",
        "    def make_up_layer_(self, in_channels, out_channels):\n",
        "        return nn.ModuleList(\n",
        "            [\n",
        "                nn.ConvTranspose2d(\n",
        "                    in_channels,\n",
        "                    out_channels,\n",
        "                    kernel_size=3,\n",
        "                    stride=2,\n",
        "                    padding=1,\n",
        "                    output_padding=1,\n",
        "                ),\n",
        "                nn.BatchNorm2d(2 * out_channels),\n",
        "                nn.ReLU(),\n",
        "                nn.ConvTranspose2d(\n",
        "                    2 * out_channels,\n",
        "                    out_channels,\n",
        "                    kernel_size=3,\n",
        "                    stride=1,\n",
        "                    padding=1,\n",
        "                ),\n",
        "                nn.BatchNorm2d(out_channels),\n",
        "                nn.ReLU(),\n",
        "            ]\n",
        "        )\n",
        "\n",
        "    def forward(self, image):\n",
        "        x_1 = self.down_1(image)\n",
        "        x_2 = self.down_2(x_1)\n",
        "        x_3 = self.down_3(x_2)\n",
        "        x_4 = self.down_4(x_3)\n",
        "\n",
        "        u_1 = self.up_1[0](x_4)\n",
        "        u_1 = torch.cat([x_3, u_1], axis=1)\n",
        "        for m in self.up_1[1:]:\n",
        "            u_1 = m(u_1)\n",
        "        \n",
        "        u_2 = self.up_2[0](u_1)\n",
        "        u_2 = torch.cat([x_2, u_2], axis=1)\n",
        "        for m in self.up_2[1:]:\n",
        "            u_2 = m(u_2)\n",
        "\n",
        "        u_3 = self.up_3[0](u_2)\n",
        "        u_3 = torch.cat([x_1, u_3], axis=1)\n",
        "        for m in self.up_3[1:]:\n",
        "            u_3 = m(u_3)\n",
        "\n",
        "        return self.up_4(u_3)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uRKiYSE0-zxs"
      },
      "source": [
        "from catalyst.contrib.nn import DiceLoss, IoULoss\n",
        "from catalyst.dl.runner import SupervisedRunner\n",
        "from torch.nn.functional import interpolate\n",
        "\n",
        "\n",
        "class SegmentationRunner(SupervisedRunner):\n",
        "    def predict_batch(self, batch):\n",
        "        prediction = {\"filename\": batch[\"filename\"]}\n",
        "        masks = self.model(batch[self.input_key].to(self.device))\n",
        "        image_size = list(zip(*batch[\"image size\"]))\n",
        "        prediction[\"mask\"] = [\n",
        "            interpolate(mask.unsqueeze(0), image_size).squeeze(0)\n",
        "            for mask, image_size in zip(masks, image_size)\n",
        "        ]\n",
        "        return prediction\n",
        "\n",
        "# we have multiple criterions\n",
        "model = Baseline()\n",
        "criterion = {\n",
        "    \"dice\": DiceLoss(),\n",
        "    \"iou\": IoULoss(),\n",
        "    \"bce\": nn.BCEWithLogitsLoss()\n",
        "}\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3, weight_decay=0.0003)\n",
        "\n",
        "runner = SegmentationRunner(input_key=\"image\", input_target_key=\"mask\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L2bR8k7y-48I"
      },
      "source": [
        "callbacks = [\n",
        "    dl.CriterionCallback(\n",
        "        input_key=\"mask\", prefix=\"loss_dice\", criterion_key=\"dice\"\n",
        "    ),\n",
        "    dl.CriterionCallback(\n",
        "        input_key=\"mask\", prefix=\"loss_iou\", criterion_key=\"iou\"\n",
        "    ),\n",
        "    dl.CriterionCallback(\n",
        "        input_key=\"mask\", prefix=\"loss_bce\", criterion_key=\"bce\"\n",
        "    ),\n",
        "    dl.MetricAggregationCallback(\n",
        "        prefix=\"loss\",\n",
        "        mode=\"weighted_sum\",\n",
        "        metrics={\"loss_dice\": 1.0, \"loss_iou\": 1.0, \"loss_bce\": 0.8},\n",
        "    ),\n",
        "    dl.DiceCallback(input_key=\"mask\"),\n",
        "    dl.IouCallback(input_key=\"mask\"),\n",
        "]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YCKPbrUxm6tY"
      },
      "source": [
        "runner.train(\n",
        "    model=model,\n",
        "    criterion=criterion,\n",
        "    optimizer=optimizer,\n",
        "    loaders=loaders,\n",
        "    callbacks=callbacks,\n",
        "    logdir=Path(\"logs\") / datetime.now().strftime(\"%Y%m%d-%H%M%S\"),\n",
        "    num_epochs=50,\n",
        "    main_metric=\"iou\", # kaggle competition metric\n",
        "    minimize_metric=False,\n",
        "    verbose=True,\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "pycharm": {
          "name": "#%% md\n"
        },
        "id": "37ume7Q1EFxq"
      },
      "source": [
        "## Submission\n",
        "\n",
        "To generate submission, you'll have to write masks for images.\n",
        "Usually, in `Kaggle` segmentation competitions masks are encoded in the run length format.\n",
        "For more information, check `Evaluation` page in `Overview`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8KHg4nnF-_F9"
      },
      "source": [
        "def rle_encoding(x):\n",
        "    \"\"\"\n",
        "    x: numpy array of shape (height, width), 1 - mask, 0 - background\n",
        "    Returns run length as list\n",
        "    \"\"\"\n",
        "    dots = np.where(x.T.flatten() == 1)[\n",
        "        0\n",
        "    ]  # .T sets Fortran order down-then-right\n",
        "    run_lengths = []\n",
        "    prev = -2\n",
        "    for b in dots:\n",
        "        if b > prev + 1:\n",
        "            run_lengths.extend((b + 1, 0))\n",
        "        run_lengths[-1] += 1\n",
        "        prev = b\n",
        "    return \" \".join([str(i) for i in run_lengths])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "id": "uwVgpY2cEFxv"
      },
      "source": [
        "This code below will generate a submission.\n",
        "It reads images from `test` folder and gathers prediction from the trained model.\n",
        "Check your submission before uploading it into `Kaggle`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y9_GuIdy3VaY"
      },
      "source": [
        "from PIL import Image\n",
        "import pandas as pd\n",
        "\n",
        "submission = {\"ImageId\": [], \"EncodedPixels\": []}\n",
        "threshold = 0.5\n",
        "\n",
        "test_image_path = Path(\"test\") / \"images\"\n",
        "TEST_IMAGES = sorted(train_image_path.glob(\"*.png\"))\n",
        "test_dataset = SegmentationDataset(\n",
        "    images=TEST_IMAGES,\n",
        "    transforms=valid_transform\n",
        ")\n",
        "test_loader = DataLoader(\n",
        "    test_dataset,\n",
        "    batch_size=batch_size,\n",
        "    shuffle=False,\n",
        "    num_workers=num_workers,\n",
        ")\n",
        "\n",
        "for prediction in runner.predict_loader(loader=test_loader):\n",
        "    submission[\"ImageId\"].extend(s[:-4] for s in prediction[\"filename\"])\n",
        "    submission[\"EncodedPixels\"].extend(\n",
        "        rle_encoding(mask.cpu().numpy().squeeze(0) > threshold) for mask in prediction[\"mask\"]\n",
        "    )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LolUhGHo3VYj"
      },
      "source": [
        "pd.DataFrame(submission).to_csv(\"submission.csv\", index=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "AoVZgqrNZ1be"
      },
      "source": [
        "pd.DataFrame(submission)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TASLJXauMpr8"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}