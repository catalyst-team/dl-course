{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Task 1\n",
    "\n",
    "Here we will see how our nn and autograd works.\n",
    "\n",
    "Also we will review some Catalyst's abstractions, implement callbacks and datasets.\n",
    "\n",
    "Unfortunately, python is slow, and implementing dynamic computational graph in pure python for product-ready solution is not a good idea. But this task will help you to understand what's happening when you call `backward` method for variable or tensor. Also it will help you in learning Catalyst framework and will teach how you to write your code in more Catalyst-like way."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.datasets import make_moons\n",
    "from sklearn.model_selection import train_test_split\n",
    "from .nn import Linear, ReLU, CrossEntropyLoss, Module\n",
    "from .optim import SGD\n",
    "from .engine import Value\n",
    "from matplotlib import pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "sns.set(style=\"whitegrid\", font_scale=1.4)\n",
    "%pylab inline"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Defining toy dataset\n",
    "\n",
    "To be more human-readable and easy to understand, we want to store every data in key-value format.\n",
    "\n",
    "So, the dataset should yield dict, moreover we will store train/valid datasets in a dict."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "class Dataset:\n",
    "    def __init__(self, X, y):\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "\n",
    "    def __getitem__(self, item):\n",
    "        return {\"features\": ..., \"targets\": ...}\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.y)\n",
    "\n",
    "\n",
    "\n",
    "X, y = make_moons(200, noise=0.2)\n",
    "X_train, X_val, y_train, y_val = ...\n",
    "datasets = {\"train\": Dataset(X_train, y_train), \"valid\": Dataset(X_val, y_val)}"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Take a look on a data."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 8))\n",
    "plt.title(\"Data\", fontsize=14)\n",
    "colors = list(map(lambda x: \"green\" if x ==0 else \"orange\", y))\n",
    "plt.scatter(X[:, 0], X[:, 1], c=colors)\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Model\n",
    "\n",
    "Let's define our model in PyTorch-style. But don't forget to implement `parameters()` method."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "class SimpleModel(Module):\n",
    "    def __init__(self):\n",
    "        # Write your model\n",
    "\n",
    "    def forward(self, inp):\n",
    "        return ...\n",
    "\n",
    "    def parameters(self):\n",
    "        parameters = [] # Don't forget about parameters!\n",
    "        return parameters"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## For loop\n",
    "\n",
    "Let's start with simple train/test loop."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "criterion = CrossEntropyLoss()\n",
    "model = SimpleModel()\n",
    "optimizer = SGD(model.parameters(), lr=0.1)\n",
    "\n",
    "num_epochs = 100\n",
    "batch_size = 4\n",
    "log_period = 5\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    current_batch = []\n",
    "    metrics = {}\n",
    "    for k, dataset in datasets.items():\n",
    "        epoch_loss = 0\n",
    "        epoch_accuracy = 0\n",
    "        current_batch=[]\n",
    "            for data in dataset:\n",
    "                # Train your model!"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Catalyst\n",
    "\n",
    "Code above can be reused for almost all machine learning task. Let's take a look on experiment structure\n",
    "\n",
    "```\n",
    "for stage in stage:\n",
    "    for epoch in epochs:\n",
    "        for loader on loaders:\n",
    "            for batch in loader:\n",
    "                # do something\n",
    "```\n",
    "\n",
    "### Callbacks\n",
    "\n",
    "In catalyst callbacks have significant impact in everything you do.\n",
    "Let's try to implement some of them.\n",
    "\n",
    "There are a list of moments, where callbacks can be integrated. We will need only three of them.\n",
    "```\n",
    "on_stage_start\n",
    "    on_epoch_start\n",
    "        on_loader_start\n",
    "            on_batch_start\n",
    "------->    on_batch_end\n",
    "----->  on_loader_end\n",
    "--> on_epoch_end\n",
    "on_stage_end\n",
    "```"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "class Callback:\n",
    "    def on_stage_start(self):\n",
    "        pass\n",
    "\n",
    "    def on_stage_end(self):\n",
    "        pass\n",
    "\n",
    "    def on_epoch_start(self):\n",
    "        pass\n",
    "\n",
    "    def on_epoch_end(self, *args, **kwargs):\n",
    "        pass\n",
    "\n",
    "    def on_loader_start(self):\n",
    "        pass\n",
    "\n",
    "    def on_loader_end(self, *args, **kwargs):\n",
    "        pass\n",
    "\n",
    "    def on_batch_start(self):\n",
    "        pass\n",
    "\n",
    "    def on_batch_end(self, *args, **kwargs):\n",
    "        pass\n",
    "\n",
    "class LossCallback(Callback):\n",
    "    \"\"\"\n",
    "    Aggregating loss value.\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        self.cum_loss = 0\n",
    "        self.num_batches = 0\n",
    "\n",
    "    def on_batch_end(self, output, train=True):\n",
    "        \"\"\"\n",
    "        On batch end action.\n",
    "\n",
    "        Accumulates loss and num batches.\n",
    "\n",
    "        Args:\n",
    "            output: dict with loss and other model's outputs.\n",
    "        \"\"\"\n",
    "        self.cum_loss += output[\"loss\"]\n",
    "        self.num_batches += 1\n",
    "\n",
    "    def on_loader_end(self, epoch_metrics):\n",
    "        \"\"\"\n",
    "        On loader end action.\n",
    "\n",
    "        Args:\n",
    "            epoch_metrics: dict with epoch metrics\n",
    "\n",
    "        Returns:\n",
    "            loss over the loader.\n",
    "        \"\"\"\n",
    "        epoch_metrics[\"loss\"] = self.cum_loss / self.num_batches\n",
    "        self.cum_loss = 0\n",
    "        self.num_batches = 0\n",
    "        return epoch_metrics\n",
    "\n",
    "\n",
    "class AccuracyCallback(Callback):\n",
    "    \"\"\"\n",
    "    Aggregating accuracy value.\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        self.correct = 0\n",
    "\n",
    "    def on_batch_end(self, output):\n",
    "        \"\"\"\n",
    "        On batch end action\n",
    "\n",
    "        Accumulates number of correct predictions.\n",
    "\n",
    "        Args:\n",
    "            output: dict with number of the correct predictions\n",
    "        \"\"\"\n",
    "        self.correct += ... # calculate accuracy\n",
    "\n",
    "    def on_loader_end(self, epoch_metrics, num_its):\n",
    "        \"\"\"\n",
    "        On loader end action.\n",
    "\n",
    "        Args:\n",
    "            epoch_metrics: dict with epoch metrics\n",
    "\n",
    "        Returns:\n",
    "            accuracy value over the loader.\n",
    "        \"\"\"\n",
    "        epoch_metrics[\"accuracy\"] = ... # sum up metrics\n",
    "        return epoch_metrics\n",
    "\n",
    "\n",
    "class LoggerCallback(Callback):\n",
    "    \"\"\"\n",
    "    Log metrics to output.\n",
    "    \"\"\"\n",
    "    def __init__(self, log_period):\n",
    "        self.log_period = log_period\n",
    "\n",
    "    def on_epoch_end(self, epoch_metrics, epoch):\n",
    "        \"\"\"\n",
    "        On epoch end action.\n",
    "\n",
    "        Prints all epoch metrics if log_period is suitable.\n",
    "\n",
    "        Args:\n",
    "            epoch_metrics: dict with epoch metrics\n",
    "            epoch: current epoch\n",
    "        \"\"\"\n",
    "        if epoch % self.log_period == 0:\n",
    "            log_string = f\"Epoch: {epoch}\\n\"\n",
    "            for metric, value in epoch_metrics.items():\n",
    "                log_string += ... # Save metrics in log\n",
    "            print(log_string)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Runner\n",
    "\n",
    "Runner is the main part of your experiment. It runs train loop, call callbacks and stores your model.\n",
    "\n",
    "In most cases we only need to adapt `_handle_batch` method."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from tqdm.notebook import tqdm\n",
    "\n",
    "\n",
    "class Runner:\n",
    "    def __init__(\n",
    "        self,\n",
    "        model,\n",
    "        criterion,\n",
    "        optimizer,\n",
    "        datasets,\n",
    "        batch_size,\n",
    "        callbacks,\n",
    "    ):\n",
    "        self.model = model\n",
    "        self.criterion = criterion\n",
    "        self.optimizer = optimizer\n",
    "        self.datasets = datasets\n",
    "        self.batch_size = batch_size\n",
    "        self.callbacks = callbacks\n",
    "\n",
    "    def _handle_batch(self, batch, train=True):\n",
    "        \"\"\"\n",
    "        Stores the main logic of data aggregating.\n",
    "        \"\"\"\n",
    "        loss = 0\n",
    "        correct = 0\n",
    "        for data in batch:\n",
    "            # Pass forward data through model\n",
    "        if train:\n",
    "            # Train your model\n",
    "        self.output = {\"loss\": loss.item(), \"correct\": correct}\n",
    "\n",
    "\n",
    "    def train(self, num_epochs: int = 100, verbose=False):\n",
    "        for epoch in range(num_epochs):\n",
    "            current_batch = []\n",
    "            epoch_metrics = {}\n",
    "            for k, dataset in self.datasets.items():\n",
    "                loader_metrics = {}\n",
    "                if verbose:\n",
    "                    iter_ = tqdm(enumerate(dataset), total=len(dataset))\n",
    "\n",
    "                else:\n",
    "                    iter_ = enumerate(dataset)\n",
    "                for idx, data in iter_:\n",
    "                    last = idx == (len(dataset)-1)\n",
    "                    current_batch.append(data)\n",
    "\n",
    "                    if last or len(current_batch) == self.batch_size:\n",
    "                        # Handle batch and load loss/metrics\n",
    "\n",
    "                loader_metrics = \\\n",
    "                    self.callbacks[\"loss\"].on_loader_end(loader_metrics)\n",
    "                loader_metrics = \\\n",
    "                    self.callbacks[\"accuracy\"].on_loader_end(\n",
    "                        loader_metrics, len(dataset)\n",
    "                    )\n",
    "\n",
    "                for metric, value in loader_metrics.items():\n",
    "                    epoch_metrics[f\"{k} {metric}\"] = value\n",
    "\n",
    "            self.callbacks[\"logger\"].on_epoch_end(epoch_metrics, epoch)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Run training"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "criterion = CrossEntropyLoss()\n",
    "model = SimpleModel()\n",
    "optimizer = SGD(model.parameters(), lr=0.1)\n",
    "runner = Runner(\n",
    "    model=model,\n",
    "    criterion=criterion,\n",
    "    optimizer=optimizer,\n",
    "    datasets=datasets,\n",
    "    batch_size=3,\n",
    "    callbacks={\n",
    "        \"loss\": LossCallback(),\n",
    "        \"accuracy\": AccuracyCallback(),\n",
    "        \"logger\": LoggerCallback(log_period=5)\n",
    "    }\n",
    ")\n",
    "runner.train(50)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Plot the results"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "h = 0.25\n",
    "x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1\n",
    "y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1\n",
    "xx, yy = np.meshgrid(np.arange(x_min, x_max, h),\n",
    "                     np.arange(y_min, y_max, h))\n",
    "Xmesh = np.c_[xx.ravel(), yy.ravel()]\n",
    "inputs = [list(map(Value, xrow)) for xrow in Xmesh]\n",
    "scores = list(map(model, inputs))\n",
    "Z = np.array([s[1].exp().data/(s[0].exp()+s[1].exp()).data for s in scores])\n",
    "Z = Z.reshape(xx.shape)\n",
    "\n",
    "fig = plt.figure(figsize=(10, 8))\n",
    "plt.title(\"Decision boundary\", fontsize=14)\n",
    "plt.contourf(xx, yy, Z, cmap=plt.cm.Spectral, alpha=0.6)\n",
    "plt.scatter(X[:, 0], X[:, 1], c=y, s=40, cmap=plt.cm.Spectral)\n",
    "plt.xlim(xx.min(), xx.max())\n",
    "plt.ylim(yy.min(), yy.max())\n",
    "plt.show()\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## MNIST"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import mnist\n",
    "X_train, y_train, X_val, y_val, X_test, y_test = mnist.load_dataset()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "class MNISTDataset:\n",
    "    def __init__(self, x, y):\n",
    "        self.x = x\n",
    "        self.y = y\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return {\"features\": self.x[idx], \"targets\": self.y[idx]}\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.y)\n",
    "\n",
    "datasets = {\n",
    "    \"train\": MNISTDataset(X_train[:2000], y_train[:2000]),\n",
    "    \"valid\": MNISTDataset(X_val[:200], y_val[:200])\n",
    "}"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "class MnistModel(Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        inp_shape=28*28,\n",
    "        out_shape=10,\n",
    "        hidden_shapes=[10, 10]\n",
    "    ):\n",
    "        # Create your model!\n",
    "\n",
    "    def forward(self, inp):\n",
    "        return ...\n",
    "\n",
    "    def parameters(self):\n",
    "        parameters = [] # Don't forget about parameters!\n",
    "        return parameters"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "criterion = CrossEntropyLoss()\n",
    "model = MnistModel()\n",
    "optimizer = SGD(model.parameters(), lr=0.1)\n",
    "runner = Runner(\n",
    "    model=model,\n",
    "    criterion=criterion,\n",
    "    optimizer=optimizer,\n",
    "    datasets=datasets,\n",
    "    batch_size=16,\n",
    "    callbacks={\n",
    "        \"loss\": LossCallback(),\n",
    "        \"accuracy\": AccuracyCallback(),\n",
    "        \"logger\": LoggerCallback(log_period=1)\n",
    "    }\n",
    ")\n",
    "runner.train(5, verbose=True)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}