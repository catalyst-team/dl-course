{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.9"
    },
    "colab": {
      "name": "vae",
      "provenance": [],
      "collapsed_sections": []
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vh0SFdAZ9AnN"
      },
      "source": [
        "# Variational AutoEncoders\n",
        "\n",
        "Hi! Today we are going to learn about variationals autoencoders. We'll code them to encode handwritten numbers and restore them from the compact vector representation."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RzTS8DltDK4g"
      },
      "source": [
        "!pip install catalyst"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mHeumeR_9AnN"
      },
      "source": [
        "from catalyst.utils import set_global_seed, get_device"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3QwNhl8l9AnN"
      },
      "source": [
        "set_global_seed(42)\n",
        "device = get_device()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fgutGR6Q9AnN"
      },
      "source": [
        "We'll work with `MNIST` dataset. Download it, show examples of the writting and prepare the dataset to be loaded into models."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3VtDGOnY9AnO"
      },
      "source": [
        "from catalyst.contrib.datasets import mnist\n",
        "\n",
        "\n",
        "train = mnist.MNIST('.', train=True, download=True)\n",
        "valid = mnist.MNIST('.', train=False, download=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dQ8S7z-S9AnO"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "_, axs = plt.subplots(4, 4, figsize=(10, 10))\n",
        "\n",
        "for i in range(16):\n",
        "    axs[i // 4][i % 4].imshow(train[100 * i + i][0])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PQ-c7Lcl9AnP"
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5UQN0cvZ9AnP"
      },
      "source": [
        "from catalyst.utils import get_loader\n",
        "\n",
        "\n",
        "batch_size = 256\n",
        "num_workers = 4\n",
        "\n",
        "def transform(x):\n",
        "    image = torch.FloatTensor(x['image'])\n",
        "    image = torch.where(image > 127, torch.ones(image.shape), torch.zeros(image.shape))\n",
        "    return {'image': image, 'targets': x['targets']}\n",
        "\n",
        "\n",
        "train_data_loader = get_loader(\n",
        "    train,\n",
        "    open_fn=lambda x : {'image': x[0].reshape(1, 28, 28), 'targets': x[1]},\n",
        "    dict_transform=transform,\n",
        "    batch_size=batch_size,\n",
        "    num_workers=num_workers,\n",
        "    shuffle=True,\n",
        "    sampler=None,\n",
        "    drop_last=True,\n",
        ")\n",
        "\n",
        "valid_data_loader = get_loader(\n",
        "    valid,\n",
        "    open_fn=lambda x : {'image': x[0].reshape(1, 28, 28), 'targets': x[1]},\n",
        "    dict_transform=transform,\n",
        "    batch_size=batch_size,\n",
        "    num_workers=num_workers,\n",
        "    shuffle=False,\n",
        "    sampler=None,\n",
        "    drop_last=False,\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jyPcXgTU9AnP"
      },
      "source": [
        "A variational autoencoder consists of two parts: encoder and decoder. The encoder shrinks objects into some vector. The decoder generates an proximate an 'image' of object. In our case, objects are images. We will use CNNs for encoding images and UpScale Convolution operations for decoding."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UfNxTbEt9AnQ"
      },
      "source": [
        "class Encoder(nn.Module):\n",
        "    def __init__(self, latent_size=2):\n",
        "        super().__init__()\n",
        "        self.feature_extractor = nn.Sequential(\n",
        "            nn.Conv2d(1, 4, kernel_size=3, stride = 2, padding=1),\n",
        "            nn.BatchNorm2d(4),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(4, 16, kernel_size=3, stride = 2, padding=1),\n",
        "            nn.BatchNorm2d(16),\n",
        "            nn.ReLU(),\n",
        "            nn.Flatten()\n",
        "        )\n",
        "        self.latent_space = nn.Linear(16*7*7, 2 * latent_size)\n",
        "        \n",
        "        self.latent_size = latent_size\n",
        "        \n",
        "    def forward(self, images):\n",
        "        features = self.feature_extractor(images)\n",
        "        latent = self.latent_space(features)\n",
        "        return latent[:, :self.latent_size], latent[:, self.latent_size:]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8MzkShvW9AnQ"
      },
      "source": [
        "from catalyst.contrib.nn.modules import Lambda\n",
        "\n",
        "\n",
        "class Decoder(nn.Module):\n",
        "    def __init__(self, image_size=(28, 28), latent_size=2):\n",
        "        super().__init__()\n",
        "        \n",
        "        self.image_size = image_size\n",
        "        self.latent_size = latent_size\n",
        "        \n",
        "        self.map_generator = nn.Sequential(\n",
        "            nn.Linear(latent_size, 16 * 49),\n",
        "            Lambda(lambda x: x.view(x.size(0), 16, 7, 7)),\n",
        "        )\n",
        "        self.deconv = nn.Sequential(\n",
        "            self.make_up_layer_(16, 8), # 7 -> 14\n",
        "            self.make_up_layer_(8, 4), # 14 -> 28\n",
        "        )\n",
        "            \n",
        "        self.output = nn.Sequential(\n",
        "            nn.Conv2d(4, 1, 3, padding=1),\n",
        "        )\n",
        "            \n",
        "    def forward(self, points):\n",
        "        feature_map = self.map_generator(points)\n",
        "        feature_map = self.deconv(feature_map)\n",
        "        return self.output(feature_map)\n",
        "            \n",
        "    def make_up_layer_(self, in_channels, out_channels):\n",
        "        return nn.Sequential(nn.ConvTranspose2d(\n",
        "                    in_channels,\n",
        "                    out_channels,\n",
        "                    kernel_size=3,\n",
        "                    stride=2,\n",
        "                    padding=1,\n",
        "                    output_padding=1,\n",
        "            ),\n",
        "            nn.BatchNorm2d(out_channels),\n",
        "            nn.LeakyReLU(0.2),\n",
        "            )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "laVYz1-09AnQ"
      },
      "source": [
        "Joint the encoder and decoder to create VAE! We have discussed in the lecture about it, and we knew how to train VAE. We need sample points in latent space, pass them forward through the decoder and compare a decoder result with original object. Also we should sample points from some normal distribution, which parameters approach to $(0, I)$."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "onYIJt1w9AnR"
      },
      "source": [
        "LOG_SCALE_MAX = 2\n",
        "LOG_SCALE_MIN = -10\n",
        "\n",
        "def normal_sample(loc, log_scale):\n",
        "    scale = torch.exp(0.5 * log_scale)\n",
        "    return loc + scale * torch.randn_like(scale)\n",
        "\n",
        "\n",
        "class VAE(nn.Module):\n",
        "    def __init__(self, image_size=(28, 28), latent_size=2):\n",
        "        super().__init__()\n",
        "\n",
        "        self.encoder = Encoder(latent_size)\n",
        "        self.decoder = Decoder(image_size, latent_size)\n",
        "        \n",
        "    def forward(self, images):\n",
        "        loc, log_scale = self.encoder(images)\n",
        "        log_scale = torch.clamp(log_scale, LOG_SCALE_MIN, LOG_SCALE_MAX)\n",
        "\n",
        "        z_ = normal_sample(loc, log_scale) if self.training else loc\n",
        "        x_ = self.decoder(z_)\n",
        "\n",
        "        return {\n",
        "            'decoder_result': x_,\n",
        "            'loc': loc,\n",
        "            'log_scale': log_scale\n",
        "        }"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G4M6aBDX9AnR"
      },
      "source": [
        "class KLVAELoss(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "    \n",
        "    def forward(self, loc, log_scale):\n",
        "        return (-0.5 * torch.sum(1 + log_scale - loc.pow(2) - log_scale.exp(), dim=1)).mean()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zNveyqmL9AnR"
      },
      "source": [
        "We need to modify `BinaryCrossEntropyLoss` function, because it doesn't work properly with images."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9bXQTK6d9AnS"
      },
      "source": [
        "To monitor decoded images, we have to write a new callback function. It will log image into the tensorboard."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9QTRsWka9AnS"
      },
      "source": [
        "from catalyst.core import Callback, CallbackOrder\n",
        "\n",
        "\n",
        "class LogFigureCallback(Callback):\n",
        "    def __init__(self):\n",
        "        super().__init__(CallbackOrder.External)\n",
        "\n",
        "    def on_epoch_end(self, runner):\n",
        "        if runner.is_valid_loader:\n",
        "            logger = runner.loggers['_tensorboard']\n",
        "            logger = logger.loggers[runner.loader_key]\n",
        "            logger.add_images(f'image/epoch', torch.sigmoid(runner.batch['decoder_result']))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KgJeE0lQ9AnS"
      },
      "source": [
        "Create model, criterion, optimizer. Train model!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OH9INo049AnS"
      },
      "source": [
        "from catalyst.contrib.nn.optimizers import RAdam\n",
        "\n",
        "\n",
        "model = VAE()\n",
        "criterion = {\n",
        "    'ae': nn.BCEWithLogitsLoss(),\n",
        "    'kl': KLVAELoss()\n",
        "}\n",
        "optimizer = RAdam(model.parameters(), lr=1e-2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Jh9bwXGk9AnS"
      },
      "source": [
        "from catalyst import dl\n",
        "\n",
        "\n",
        "callbacks = [\n",
        "    dl.CriterionCallback(\n",
        "        input_key='decoder_result', target_key='image', metric_key='loss_ae', criterion_key='ae',\n",
        "    ),\n",
        "    dl.CriterionCallback(\n",
        "        input_key='loc', target_key='log_scale', metric_key='loss_kl', criterion_key='kl'\n",
        "    ),\n",
        "    dl.MetricAggregationCallback(\n",
        "        metric_key='loss',\n",
        "        mode='weighted_sum',\n",
        "        metrics={'loss_ae': 1.0, 'loss_kl': 0.01},\n",
        "    ),\n",
        "    LogFigureCallback(),\n",
        "]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kHpXUSdt9AnT"
      },
      "source": [
        "class VAERunner(dl.SupervisedRunner):\n",
        "    def predict_batch(self, batch):\n",
        "        prediction = {'image':batch['image'], 'targets':batch['targets']}\n",
        "        prediction.update(self.model(batch['image'].to(runner.device)))\n",
        "        return prediction\n",
        "    \n",
        "    def handle_batch(self, batch):\n",
        "        self.batch.update(self.model(batch['image']))\n",
        "\n",
        "\n",
        "runner = VAERunner()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1luViqgM9AnT"
      },
      "source": [
        "from datetime import datetime\n",
        "from pathlib import Path\n",
        "\n",
        "\n",
        "logdir = Path('logs') / datetime.now().strftime('%Y%m%d-%H%M%S')\n",
        "\n",
        "%reload_ext tensorboard\n",
        "%tensorboard --logdir logs"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DRc5e1Tu9AnT"
      },
      "source": [
        "runner.train(\n",
        "    model=model,\n",
        "    optimizer=optimizer,\n",
        "    criterion=criterion,\n",
        "    loaders={'train': train_data_loader, 'valid': valid_data_loader},\n",
        "    callbacks=callbacks,\n",
        "    num_epochs=1,\n",
        "    verbose=True,\n",
        "    logdir=logdir,\n",
        "    valid_loader='valid',\n",
        "    valid_metric='loss',\n",
        "    load_best_on_end = True\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7jK0Wmjr9AnT"
      },
      "source": [
        "One of the main feature of VAE it's a generating new objects. We can do this by mixing latent representation of objects."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Vp7EFbHA9AnU"
      },
      "source": [
        "test_data = next(iter(valid_data_loader))\n",
        "test_data['targets']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FWIDyJBk9AnU"
      },
      "source": [
        "model.eval()\n",
        "locs, _ = model.encoder(test_data['image'].to(device))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j9-h845p9AnU"
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "\n",
        "def plot_transition(i, j):\n",
        "    _, ax = plt.subplots(1, 11, figsize=(15, 5))\n",
        "    \n",
        "    line = np.linspace(0, 1, 11)\n",
        "    for k in range(0, 11):\n",
        "        point = line[k] * locs[j] + (1 - line[k]) * locs[i]\n",
        "        decoded = torch.sigmoid(model.decoder(point.unsqueeze(0).to(device)).squeeze())\n",
        "        ax[k].imshow(decoded.squeeze().cpu().detach().numpy())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q6WkZ2WB9AnU"
      },
      "source": [
        "%matplotlib inline\n",
        "plot_transition(0, -3)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nlIKoaK29AnU"
      },
      "source": [
        "We can enhance generated images by many ways. And we choose to add classification task. The model will classify object based on the corresponding latent representation."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hHMBdJhF9AnU"
      },
      "source": [
        "\n",
        "class VAEClassify(nn.Module):\n",
        "    def __init__(self, num_classes=10, image_size=(28, 28), latent_size=10):\n",
        "        super().__init__()\n",
        "\n",
        "        self.encoder = Encoder(latent_size)\n",
        "        self.decoder = Decoder(image_size, latent_size)\n",
        "        self.clf = nn.Linear(latent_size, num_classes)\n",
        "        \n",
        "    def forward(self, images):\n",
        "        loc, log_scale = self.encoder(images)\n",
        "        log_scale = torch.clamp(log_scale, LOG_SCALE_MIN, LOG_SCALE_MAX)\n",
        "\n",
        "        z_ = normal_sample(loc, log_scale) if self.training else loc\n",
        "        x_ = self.decoder(z_)\n",
        "\n",
        "        logits = self.clf(z_)\n",
        "        return {\n",
        "            'logits': logits, \n",
        "            'decoder_result': x_,\n",
        "            'loc': loc,\n",
        "            'log_scale': log_scale\n",
        "        }"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Midu14w99AnV"
      },
      "source": [
        "from catalyst.contrib.nn.optimizers import RAdam\n",
        "\n",
        "\n",
        "model = VAEClassify()\n",
        "criterion = {\n",
        "    'ce': nn.CrossEntropyLoss(),\n",
        "    'ae': nn.BCEWithLogitsLoss(),\n",
        "    'kl': KLVAELoss()\n",
        "}\n",
        "optimizer = RAdam(model.parameters(), lr=1e-2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iESeOFbB9AnV"
      },
      "source": [
        "callbacks = [\n",
        "    dl.CriterionCallback(\n",
        "        input_key='decoder_result', target_key='image', metric_key='loss_ae', criterion_key='ae',\n",
        "    ),\n",
        "    dl.CriterionCallback(\n",
        "        input_key='loc', target_key='log_scale', metric_key='loss_kl', criterion_key='kl'\n",
        "    ),\n",
        "    dl.CriterionCallback(\n",
        "        input_key='logits', target_key='targets', metric_key='loss_ce', criterion_key='ce',\n",
        "    ),\n",
        "    dl.MetricAggregationCallback(\n",
        "        metric_key='loss',\n",
        "        mode='weighted_sum',\n",
        "        metrics={'loss_ae': 1.0, 'loss_kl': 0.01, 'loss_ce': 1.0},\n",
        "    ),\n",
        "    dl.AccuracyCallback(input_key='logits', target_key='targets'),\n",
        "    LogFigureCallback(),\n",
        "]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rK-VVkpR9AnV"
      },
      "source": [
        "runner = VAERunner()\n",
        "\n",
        "runner.train(\n",
        "    model=model,\n",
        "    optimizer=optimizer,\n",
        "    criterion=criterion,\n",
        "    loaders={'train': train_data_loader, 'valid': valid_data_loader},\n",
        "    callbacks=callbacks,\n",
        "    num_epochs=10,\n",
        "    verbose=True,\n",
        "    logdir=Path('logs') / datetime.now().strftime('%Y%m%d-%H%M%S'),\n",
        "    valid_loader='valid',\n",
        "    valid_metric='loss',\n",
        "    load_best_on_end = True\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VWpM1lIQ9AnV"
      },
      "source": [
        "Let's compare results with the usual VAE."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mc2WxlzH9AnV"
      },
      "source": [
        "model.eval()\n",
        "locs, _ = model.encoder(test_data['image'].to(device))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dmYxzlGC9AnW"
      },
      "source": [
        "plot_transition(0, -3)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GNEilPcV9AnW"
      },
      "source": [
        "Let's check how our model restore noised objects. The model aren't trained to restore, but it can do this very well."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XQ67W92j9AnW"
      },
      "source": [
        "_, ax = plt.subplots(2, 6, figsize=(10, 4))\n",
        "    \n",
        "for k in range(0, 12):\n",
        "    image = test_data['image'][k]\n",
        "    ax[k // 6][k % 6].imshow(image.squeeze().cpu().detach().numpy())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9sl2gZES9AnW"
      },
      "source": [
        "_, ax = plt.subplots(2, 6, figsize=(10, 4))\n",
        "    \n",
        "for k in range(0, 12):\n",
        "    image = test_data['image'][k]\n",
        "    noise = torch.rand(image.size())\n",
        "    ax[k // 6][k % 6].imshow((image + noise).squeeze().cpu().detach().numpy())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zb6U-ojB9AnW"
      },
      "source": [
        "_, ax = plt.subplots(2, 6, figsize=(10, 4))\n",
        "    \n",
        "for k in range(0, 12):\n",
        "    image = test_data['image'][k]\n",
        "    noise = torch.rand(image.size())*0.2\n",
        "    point, _ = model.encoder((image + noise).unsqueeze(0).to(device))\n",
        "    decoded = torch.sigmoid(model.decoder(point.unsqueeze(0).to(device)).squeeze())\n",
        "    ax[k // 6][k % 6].imshow(decoded.cpu().detach().numpy())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8sXAc1tn9AnW"
      },
      "source": [
        "In the end, let's look at the latent space. We choose 2D plain space, so it's easy to plot the points."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3rDIlgxT9AnX"
      },
      "source": [
        "predictions = {'image': [], 'loc': [], 'target': []}\n",
        "\n",
        "for pred in runner.predict_loader(loader=valid_data_loader):\n",
        "    predictions['image'].extend(o.reshape(28, 28) for o in pred['image'].numpy())\n",
        "    predictions['loc'].extend(i for i in pred['loc'].cpu().numpy())\n",
        "    predictions['target'].extend(i for i in pred['targets'].numpy())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S7sbwK1t9AnX"
      },
      "source": [
        "predictions['x'] = [o[0] for o in predictions['loc']]\n",
        "predictions['y'] = [o[1] for o in predictions['loc']]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4M_XC7cF9AnX"
      },
      "source": [
        "import seaborn as sns\n",
        "\n",
        "sns.set()\n",
        "\n",
        "_, ax = plt.subplots(1, 1, figsize=(10, 10))\n",
        "sns.scatterplot(x='x', y='y', hue='target', data=predictions, ax=ax, legend='full')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ceaBqGiwQ-xr"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}