{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "1 Seminar",
      "provenance": [],
      "private_outputs": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YHEN61c1OwMv",
        "colab_type": "text"
      },
      "source": [
        "# Seminar 1. Intro.\n",
        "Hi! Today we are going to recall our memories about numpy, sci-kit learn, matplotlib, make our own simple Neural Network and train it to solve some classification tasks."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gZR2Z-0tN0WG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!pip install mnist"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JPUehjglN5Vn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from IPython import display\n",
        "import numpy as np\n",
        "import random"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Mf_26P2DOeWg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "np.random.seed(42)\n",
        "random.seed(42)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7XZHmVxNOsmM",
        "colab_type": "text"
      },
      "source": [
        "## Numpy"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q2rOjOL1EiB3",
        "colab_type": "text"
      },
      "source": [
        "### Initialization"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "If5mxOaLOjBb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "a = [1. , 1.4 , 2.5]\n",
        "print(f\"Simple way: {np.array(a)}\")\n",
        "print(f\"Zeros:\\n {np.zeros((2,3))}\")\n",
        "print(f\"Range: {np.arange(10)}\")\n",
        "print(f\"Complicated range: {np.arange(4, 12, 2)}\")\n",
        "print(f\"Space: {np.linspace(1, 4, 6)}\")\n",
        "print(f\"Identity matrix:\\n {np.eye(4)}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ComMD10nEkxk",
        "colab_type": "text"
      },
      "source": [
        "### Random"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0PYocIZHEmiu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(f\"From 0 to 1: {np.random.rand()}\")\n",
        "print(f\"Vector from 0 to 1: {np.random.rand(5)}\")\n",
        "print(f\"Vector from 0 to 10: {np.random.randint(10, size=5)}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PDzRMMd_F3XW",
        "colab_type": "text"
      },
      "source": [
        "### Matrix Operation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "twvUXA9n_INY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "a = np.arange(10)\n",
        "b = np.linspace(-10, 10, 10)\n",
        "print(f\"a: {a}\\nshape:{a.shape}\")\n",
        "print(f\"b: {a}\\nshape:{b.shape}\")\n",
        "print(f\"a + b: {a + b},\\n\\t a * b: {a * b}\")\n",
        "print(f\"Dot product: {a.dot(b)}\")\n",
        "print(f\"Mean: {a.mean()}, STD: {a.std()}\")\n",
        "print(f\"Sum: {a.sum()}, Min: {a.min()}, Max: {a.max()}\")\n",
        "print(f\"Reshape:\\n{a.reshape(-1, 1)}\\nshape: {a.reshape(-1, 1).shape}\")\n",
        "c = a.reshape(-1, 1).repeat(5, axis=1)\n",
        "print(f\"Repeat:\\n{c}\")\n",
        "print(f\"Transpose:\\n{c.T}\\nshape: {c.T}\")\n",
        "print(f\"Unique items: {np.unique(c)}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q4LmocC6F7Po",
        "colab_type": "text"
      },
      "source": [
        "### Indexing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vDPlf50O_IK5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "a = np.arange(100).reshape(10, 10)\n",
        "print(f\"Array:\\n{a}\\nshape:{a.shape}\")\n",
        "print(f\"Get first column: {a[:, 0]}\")\n",
        "print(f\"Get last row: {a[-1, :]}\")\n",
        "print(f\"Add new awis:\\n{a[:, np.newaxis]}\\nshape: {a[:, np.newaxis].shape}\")\n",
        "print(f\"Specific indexing:\\n{a[4:6, 7:]}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6T3mE9ACPXSe",
        "colab_type": "text"
      },
      "source": [
        "## Scikit Learn\n",
        "[Docs](https://scikit-learn.org/stable/modules/classes.html)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jvmLjGOlPYvJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.datasets import make_blobs\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-mmGgreBPZYU",
        "colab_type": "text"
      },
      "source": [
        "## Matplotlib"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tzMTqUUZPbT9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "chDEutqxH7Tj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "plt.plot(np.arange(10), 6 * np.arange(10))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0UZ_HAzbH7SN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X, y = make_blobs((50, 50, 50))\n",
        "plt.scatter(X[:, 0], X[:, 1], c=y)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1qT5V355H7Po",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "plt.figure(figsize=(10, 10))\n",
        "plt.scatter(X[:, 0], X[:, 1], c=y)\n",
        "plt.title(\"Adversal data\")\n",
        "plt.grid()\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u5Y4ULaGH7NZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(X, y)\n",
        "\n",
        "plt.figure(figsize=(10, 10))\n",
        "plt.scatter(X_train[:, 0], X_train[:, 1], c=y_train, alpha=1.0)\n",
        "plt.scatter(X_test[:, 0], X_test[:, 1], c=y_test, alpha=.3)\n",
        "plt.grid()\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4YGPOoKnPkXh",
        "colab_type": "text"
      },
      "source": [
        "## Neural Network\n",
        "Based on [ml-mipt](https://github.com/girafe-ai/ml-mipt) course."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AoBqI7MUPmHB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from time import time, sleep\n",
        "\n",
        "\n",
        "class Module(object):\n",
        "    \"\"\"\n",
        "    Basically, you can think of a module as of a something (black box) \n",
        "    which can process `input` data and produce `ouput` data.\n",
        "    This is like applying a function which is called `forward`: \n",
        "        \n",
        "        output = module.forward(input)\n",
        "    \n",
        "    The module should be able to perform a backward pass: to differentiate the `forward` function. \n",
        "    More, it should be able to differentiate it if is a part of chain (chain rule).\n",
        "    The latter implies there is a gradient from previous step of a chain rule. \n",
        "    \n",
        "        gradInput = module.backward(input, gradOutput)\n",
        "    \"\"\"\n",
        "    def __init__ (self):\n",
        "        self.output = None\n",
        "        self.gradInput = None\n",
        "        self.training = True\n",
        "    \n",
        "    def forward(self, input):\n",
        "        \"\"\"\n",
        "        Takes an input object, and computes the corresponding output of the module.\n",
        "        \"\"\"\n",
        "        return self.updateOutput(input)\n",
        "\n",
        "    def backward(self, input, gradOutput):\n",
        "        \"\"\"\n",
        "        Performs a backpropagation step through the module, with respect to the given input.\n",
        "        \n",
        "        This includes \n",
        "         - computing a gradient w.r.t. `input` (is needed for further backprop),\n",
        "         - computing a gradient w.r.t. parameters (to update parameters while optimizing).\n",
        "        \"\"\"\n",
        "        self.updateGradInput(input, gradOutput)\n",
        "        self.accGradParameters(input, gradOutput)\n",
        "        return self.gradInput\n",
        "    \n",
        "\n",
        "    def updateOutput(self, input):\n",
        "        \"\"\"\n",
        "        Computes the output using the current parameter set of the class and input.\n",
        "        This function returns the result which is stored in the `output` field.\n",
        "        \n",
        "        Make sure to both store the data in `output` field and return it. \n",
        "        \"\"\"\n",
        "        \n",
        "        # The easiest case:\n",
        "            \n",
        "        # self.output = input \n",
        "        # return self.output\n",
        "        \n",
        "        pass\n",
        "\n",
        "    def updateGradInput(self, input, gradOutput):\n",
        "        \"\"\"\n",
        "        Computing the gradient of the module with respect to its own input. \n",
        "        This is returned in `gradInput`. Also, the `gradInput` state variable is updated accordingly.\n",
        "        \n",
        "        The shape of `gradInput` is always the same as the shape of `input`.\n",
        "        \n",
        "        Make sure to both store the gradients in `gradInput` field and return it.\n",
        "        \"\"\"\n",
        "        \n",
        "        # The easiest case:\n",
        "        \n",
        "        # self.gradInput = gradOutput \n",
        "        # return self.gradInput\n",
        "        \n",
        "        pass   \n",
        "    \n",
        "    def accGradParameters(self, input, gradOutput):\n",
        "        \"\"\"\n",
        "        Computing the gradient of the module with respect to its own parameters.\n",
        "        No need to override if module has no parameters (e.g. ReLU).\n",
        "        \"\"\"\n",
        "        pass\n",
        "    \n",
        "    def zeroGradParameters(self): \n",
        "        \"\"\"\n",
        "        Zeroes `gradParams` variable if the module has params.\n",
        "        \"\"\"\n",
        "        pass\n",
        "        \n",
        "    def getParameters(self):\n",
        "        \"\"\"\n",
        "        Returns a list with its parameters. \n",
        "        If the module does not have parameters return empty list. \n",
        "        \"\"\"\n",
        "        return []\n",
        "        \n",
        "    def getGradParameters(self):\n",
        "        \"\"\"\n",
        "        Returns a list with gradients with respect to its parameters. \n",
        "        If the module does not have parameters return empty list. \n",
        "        \"\"\"\n",
        "        return []\n",
        "    \n",
        "    def train(self):\n",
        "        \"\"\"\n",
        "        Sets training mode for the module.\n",
        "        Training and testing behaviour differs for Dropout, BatchNorm.\n",
        "        \"\"\"\n",
        "        self.training = True\n",
        "    \n",
        "    def evaluate(self):\n",
        "        \"\"\"\n",
        "        Sets evaluation mode for the module.\n",
        "        Training and testing behaviour differs for Dropout, BatchNorm.\n",
        "        \"\"\"\n",
        "        self.training = False\n",
        "    \n",
        "    def __repr__(self):\n",
        "        \"\"\"\n",
        "        Pretty printing. Should be overrided in every module if you want \n",
        "        to have readable description. \n",
        "        \"\"\"\n",
        "        return \"Module\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eE6e6flB6FNq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Sequential(Module):\n",
        "    \"\"\"\n",
        "         This class implements a container, which processes `input` data sequentially. \n",
        "         \n",
        "         `input` is processed by each module (layer) in self.modules consecutively.\n",
        "         The resulting array is called `output`. \n",
        "    \"\"\"\n",
        "    \n",
        "    def __init__ (self):\n",
        "        super(Sequential, self).__init__()\n",
        "        self.modules = []\n",
        "   \n",
        "    def add(self, module):\n",
        "        \"\"\"\n",
        "        Adds a module to the container.\n",
        "        \"\"\"\n",
        "        self.modules.append(module)\n",
        "\n",
        "    def updateOutput(self, input):\n",
        "        \"\"\"\n",
        "        Basic workflow of FORWARD PASS:\n",
        "        \n",
        "            y_0    = module[0].forward(input)\n",
        "            y_1    = module[1].forward(y_0)\n",
        "            ...\n",
        "            output = module[n-1].forward(y_{n-2})   \n",
        "            \n",
        "            \n",
        "        Just write a little loop. \n",
        "        \"\"\"\n",
        "\n",
        "        # Your code goes here. ################################################\n",
        "        self.output = input\n",
        "        \n",
        "        for module in self.modules:\n",
        "            self.output = module.forward(self.output)\n",
        "        \n",
        "        return self.output\n",
        "\n",
        "    def backward(self, input, gradOutput):\n",
        "        \"\"\"\n",
        "        Workflow of BACKWARD PASS:\n",
        "            \n",
        "            g_{n-1} = module[n-1].backward(y_{n-2}, gradOutput)\n",
        "            g_{n-2} = module[n-2].backward(y_{n-3}, g_{n-1})\n",
        "            ...\n",
        "            g_1 = module[1].backward(y_0, g_2)   \n",
        "            gradInput = module[0].backward(input, g_1)   \n",
        "             \n",
        "             \n",
        "        !!!\n",
        "                \n",
        "        To ech module you need to provide the input, module saw while forward pass, \n",
        "        it is used while computing gradients. \n",
        "        Make sure that the input for `i-th` layer the output of `module[i]` (just the same input as in forward pass) \n",
        "        and NOT `input` to this Sequential module. \n",
        "        \n",
        "        !!!\n",
        "        \n",
        "        \"\"\"\n",
        "        # Your code goes here. ################################################\n",
        "        \n",
        "        for i in range(len(self.modules)-1, 0, -1):\n",
        "            gradOutput = self.modules[i].backward(self.modules[i-1].output, gradOutput)\n",
        "        \n",
        "        self.gradInput = self.modules[0].backward(input, gradOutput)\n",
        "        \n",
        "        return self.gradInput\n",
        "      \n",
        "\n",
        "    def zeroGradParameters(self): \n",
        "        for module in self.modules:\n",
        "            module.zeroGradParameters()\n",
        "    \n",
        "    def getParameters(self):\n",
        "        \"\"\"\n",
        "        Should gather all parameters in a list.\n",
        "        \"\"\"\n",
        "        return [x.getParameters() for x in self.modules]\n",
        "    \n",
        "    def getGradParameters(self):\n",
        "        \"\"\"\n",
        "        Should gather all gradients w.r.t parameters in a list.\n",
        "        \"\"\"\n",
        "        return [x.getGradParameters() for x in self.modules]\n",
        "    \n",
        "    def __repr__(self):\n",
        "        string = \"\".join([str(x) + '\\n' for x in self.modules])\n",
        "        return string\n",
        "    \n",
        "    def __getitem__(self,x):\n",
        "        return self.modules.__getitem__(x)\n",
        "    \n",
        "    def train(self):\n",
        "        \"\"\"\n",
        "        Propagates training parameter through all modules\n",
        "        \"\"\"\n",
        "        self.training = True\n",
        "        for module in self.modules:\n",
        "            module.train()\n",
        "    \n",
        "    def evaluate(self):\n",
        "        \"\"\"\n",
        "        Propagates training parameter through all modules\n",
        "        \"\"\"\n",
        "        self.training = False\n",
        "        for module in self.modules:\n",
        "            module.evaluate()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "efNLSSFS6FFc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Criterion(object):\n",
        "    def __init__ (self):\n",
        "        self.output = None\n",
        "        self.gradInput = None\n",
        "        \n",
        "    def forward(self, input, target):\n",
        "        \"\"\"\n",
        "            Given an input and a target, compute the loss function \n",
        "            associated to the criterion and return the result.\n",
        "            \n",
        "            For consistency this function should not be overrided,\n",
        "            all the code goes in `updateOutput`.\n",
        "        \"\"\"\n",
        "        return self.updateOutput(input, target)\n",
        "\n",
        "    def backward(self, input, target):\n",
        "        \"\"\"\n",
        "            Given an input and a target, compute the gradients of the loss function\n",
        "            associated to the criterion and return the result. \n",
        "            For consistency this function should not be overrided,\n",
        "            all the code goes in `updateGradInput`.\n",
        "        \"\"\"\n",
        "        return self.updateGradInput(input, target)\n",
        "    \n",
        "    def updateOutput(self, input, target):\n",
        "        \"\"\"\n",
        "        Function to override.\n",
        "        \"\"\"\n",
        "        return self.output\n",
        "\n",
        "    def updateGradInput(self, input, target):\n",
        "        \"\"\"\n",
        "        Function to override.\n",
        "        \"\"\"\n",
        "        return self.gradInput   \n",
        "\n",
        "    def __repr__(self):\n",
        "        \"\"\"\n",
        "        Pretty printing. Should be overrided in every module if you want \n",
        "        to have readable description. \n",
        "        \"\"\"\n",
        "        return \"Criterion\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N5AH9tM962yD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Linear(Module):\n",
        "    \"\"\"\n",
        "    A module which applies a linear transformation \n",
        "    A common name is fully-connected layer, InnerProductLayer in caffe. \n",
        "    \n",
        "    The module should work with 2D _input of shape (n_samples, n_feature).\n",
        "    \"\"\"\n",
        "    def __init__(self, n_in, n_out):\n",
        "        super(Linear, self).__init__()\n",
        "       \n",
        "        # This is a nice initialization\n",
        "        stdv = 1./np.sqrt(n_in)\n",
        "        self.W = np.random.uniform(-stdv, stdv, size = (n_out, n_in))\n",
        "        self.b = np.random.uniform(-stdv, stdv, size = n_out)\n",
        "        \n",
        "        self.gradW = np.zeros_like(self.W)\n",
        "        self.gradb = np.zeros_like(self.b)\n",
        "        \n",
        "    def updateOutput(self, _input):\n",
        "        # Your code goes here. ################################################\n",
        "        self.output = np.dot(_input, self.W.T) + self.b\n",
        "        \n",
        "        return self.output\n",
        "    \n",
        "    def updateGradInput(self, _input, gradOutput):\n",
        "        # Your code goes here. ################################################\n",
        "        self.gradInput = np.dot(gradOutput, self.W)\n",
        "        \n",
        "        return self.gradInput\n",
        "    \n",
        "    def accGradParameters(self, _input, gradOutput):\n",
        "        # Your code goes here. ################################################\n",
        "        # self.gradW = ... ; self.gradb = ...\n",
        "        self.gradW += np.dot(gradOutput.T, _input)\n",
        "        self.gradb += np.sum(gradOutput, axis=0)\n",
        "\n",
        "        pass\n",
        "    \n",
        "    def zeroGradParameters(self):\n",
        "        self.gradW.fill(0)\n",
        "        self.gradb.fill(0)\n",
        "        \n",
        "    def getParameters(self):\n",
        "        return [self.W, self.b]\n",
        "    \n",
        "    def getGradParameters(self):\n",
        "        return [self.gradW, self.gradb]\n",
        "    \n",
        "    def __repr__(self):\n",
        "        s = self.W.shape\n",
        "        q = 'Linear %d -> %d' %(s[1],s[0])\n",
        "        return q"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ICQHX5t_6-o7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class LogSoftMax(Module):\n",
        "    def __init__(self):\n",
        "         super(LogSoftMax, self).__init__()\n",
        "    \n",
        "    def updateOutput(self, _input):\n",
        "        # start with normalization for numerical stability\n",
        "        self.output = np.subtract(_input, _input.max(axis=1, keepdims=True))\n",
        "        \n",
        "        # Your code goes here. ################################################\n",
        "        self.output = - _input - np.log(np.sum(np.exp(-_input), axis=1)).reshape(-1, 1)\n",
        "\n",
        "        return self.output\n",
        "    \n",
        "    def updateGradInput(self, _input, gradOutput):\n",
        "        # Your code goes here. ################################################\n",
        "        _input = np.subtract(_input, _input.max(axis=1, keepdims=True))\n",
        "        s = 1 / np.sum(np.exp(-_input), axis=1).reshape(-1, 1)\n",
        "        self.gradInput = - gradOutput + gradOutput * s * np.exp(- _input)\n",
        "\n",
        "        return self.gradInput\n",
        "    \n",
        "    def __repr__(self):\n",
        "        return \"LogSoftMax\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a2BolEaJ7B9v",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class ReLU(Module):\n",
        "    def __init__(self):\n",
        "         super(ReLU, self).__init__()\n",
        "    \n",
        "    def updateOutput(self, _input):\n",
        "        self.output = np.maximum(_input, 0)\n",
        "        return self.output\n",
        "    \n",
        "    def updateGradInput(self, _input, gradOutput):\n",
        "        self.gradInput = np.multiply(gradOutput , _input > 0)\n",
        "        return self.gradInput\n",
        "    \n",
        "    def __repr__(self):\n",
        "        return \"ReLU\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jveUm1Ta7Fa2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class ClassNLLCriterion(Criterion):\n",
        "    def __init__(self):\n",
        "        a = super(ClassNLLCriterion, self)\n",
        "        super(ClassNLLCriterion, self).__init__()\n",
        "        \n",
        "    def updateOutput(self, _input, target): \n",
        "        # Your code goes here. ################################################\n",
        "\n",
        "        self.output = - np.mean(_input[np.arange(target.shape[0]), target])\n",
        "        return self.output\n",
        "\n",
        "    def updateGradInput(self, _input, target):\n",
        "        # Your code goes here. ################################################\n",
        "\n",
        "        self.gradInput = np.zeros_like(_input)\n",
        "        self.gradInput[np.arange(target.shape[0]), target] = -1 / _input.shape[0]\n",
        "        return self.gradInput\n",
        "    \n",
        "    def __repr__(self):\n",
        "        return \"ClassNLLCriterion\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mbe8ik157T3D",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def simple_sgd(variables, gradients, config, state):  \n",
        "    # 'variables' and 'gradients' have complex structure, accumulated_grads will be stored in a simpler one\n",
        "    state.setdefault('accumulated_grads', {})\n",
        "    \n",
        "    var_index = 0 \n",
        "    for current_layer_vars, current_layer_grads in zip(variables, gradients): \n",
        "        for current_var, current_grad in zip(current_layer_vars, current_layer_grads):\n",
        "            \n",
        "            current_var -= config['learning_rate'] * current_grad\n",
        "            var_index += 1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9SiNkcvbPnNX",
        "colab_type": "text"
      },
      "source": [
        "### Simple classification task"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QHpD7FvlPqQZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "dataset = make_blobs((200, 200, 200))\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(dataset[0], dataset[1], test_size=0.3)\n",
        "\n",
        "plt.figure(figsize=(10, 10))\n",
        "plt.scatter(X_train[:, 0], X_train[:, 1], c=y_train, alpha=1.0)\n",
        "plt.scatter(X_test[:, 0], X_test[:, 1], c=y_test, alpha=.3)\n",
        "plt.grid()\n",
        "plt.show()\n",
        "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.4)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2i4WBvahK7Ov",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "net = Sequential()\n",
        "net.add(Linear(2, 3))\n",
        "net.add(LogSoftMax())\n",
        "\n",
        "criterion = ClassNLLCriterion()\n",
        "\n",
        "print(net)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "guAcoLa2K7Mo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Optimizer params\n",
        "optimizer_config = {'learning_rate' : 1e-2}\n",
        "optimizer_state = {}\n",
        "\n",
        "# Looping params\n",
        "n_epoch = 20\n",
        "batch_size = 128"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5xQAbg-ZK7KU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# batch generator\n",
        "def get_batches(dataset, batch_size):\n",
        "    X, Y = dataset\n",
        "    n_samples = X.shape[0]\n",
        "        \n",
        "    # Shuffle at the start of epoch\n",
        "    indices = np.arange(n_samples)\n",
        "    np.random.shuffle(indices)\n",
        "    \n",
        "    for start in range(0, n_samples, batch_size):\n",
        "        end = min(start + batch_size, n_samples)\n",
        "        \n",
        "        batch_idx = indices[start:end]\n",
        "    \n",
        "        yield X[batch_idx], Y[batch_idx]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WnVR-lt_Vp3x",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def train():\n",
        "    train_loss_history = []\n",
        "    val_loss_history = [0,]\n",
        "    val_acc_history = [0,]\n",
        "    steps = [0,]\n",
        "\n",
        "    for i in range(n_epoch):\n",
        "        steps.append(steps[-1])\n",
        "        for x_batch, y_batch in get_batches((X_train, y_train), batch_size):\n",
        "            \n",
        "            net.zeroGradParameters()\n",
        "            \n",
        "            # Forward\n",
        "            predictions = net.forward(x_batch)\n",
        "            loss = criterion.forward(predictions, y_batch)\n",
        "        \n",
        "            # Backward\n",
        "            dp = criterion.backward(predictions, y_batch)\n",
        "            net.backward(x_batch, dp)\n",
        "            \n",
        "            # Update weights\n",
        "            simple_sgd(net.getParameters(), \n",
        "                    net.getGradParameters(), \n",
        "                    optimizer_config,\n",
        "                    optimizer_state)      \n",
        "            \n",
        "            train_loss_history.append(loss)\n",
        "            steps[-1] += 1\n",
        "\n",
        "        sum_loss = 0\n",
        "        sum_acc = 0\n",
        "        count_val_steps = 0\n",
        "        for x_batch, y_batch in get_batches((X_val, y_val), batch_size):\n",
        "            predictions = net.forward(x_batch)\n",
        "            loss = criterion.forward(predictions, y_batch)\n",
        "            sum_loss += loss\n",
        "            sum_acc += accuracy_score(y_batch, np.argmax(predictions, axis=1))\n",
        "            count_val_steps += 1\n",
        "\n",
        "        val_loss_history.append(sum_loss / count_val_steps)\n",
        "        val_acc_history.append(sum_acc / count_val_steps)\n",
        "\n",
        "        # Visualize\n",
        "        display.clear_output(wait=True)\n",
        "        fig, ax = plt.subplots(1, 3, figsize=(15, 5))\n",
        "        # plt.figure(figsize=(8, 6))\n",
        "            \n",
        "        ax[0].set_title(\"Training loss\")\n",
        "        ax[0].set_xlabel(\"#iteration\")\n",
        "        ax[0].set_ylabel(\"loss\")\n",
        "        ax[0].plot(train_loss_history, 'b')\n",
        "        ax[0].grid()\n",
        "\n",
        "        ax[1].set_title(\"Validation loss\")\n",
        "        ax[1].set_xlabel(\"#iteration\")\n",
        "        ax[1].set_ylabel(\"loss\")\n",
        "        ax[1].plot(steps, val_loss_history, 'b')\n",
        "        ax[1].grid()\n",
        "\n",
        "        ax[2].set_title(\"Validation Accuracy\")\n",
        "        ax[2].set_xlabel(\"#iteration\")\n",
        "        ax[2].set_ylabel(\"accuracy\")\n",
        "        ax[2].plot(steps, val_acc_history, 'b')\n",
        "        ax[2].grid()\n",
        "\n",
        "        plt.show()\n",
        "        \n",
        "        print('Current loss: %f' % loss)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GVh-Kla5n2Va",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eunZzRfZnXyC",
        "colab_type": "text"
      },
      "source": [
        "Let's make network more complicated:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s8PR9W8UVpwK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "net = Sequential()\n",
        "net.add(Linear(2, 10))\n",
        "net.add(ReLU())\n",
        "net.add(Linear(10, 3))\n",
        "net.add(LogSoftMax())\n",
        "\n",
        "criterion = ClassNLLCriterion()\n",
        "\n",
        "print(net)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jgEOKcMQK7Hm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5SLowBWmPq74",
        "colab_type": "text"
      },
      "source": [
        "### MNIST"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uu9n-r20Pr1b",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import mnist\n",
        "\n",
        "\n",
        "images = mnist.train_images() / 255\n",
        "labels = mnist.train_labels()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zze-lKruoi3w",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "_, ax = plt.subplots(5, 5, figsize=(10, 10))\n",
        "for i in range(25):\n",
        "    ax[i // 5, i % 5].imshow(images[i], cmap=\"gray\")\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0C-svqMdo0ld",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "images[0].shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oZdRaWWIphZe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "pca = PCA(n_components=2)\n",
        "new_features = pca.fit_transform([i.reshape(-1) for i in images])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VEyM9qzMpXUZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "plt.figure(figsize=(10, 10))\n",
        "plt.scatter(new_features[:, 0], new_features[:, 1], c=labels)\n",
        "plt.grid()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rwdeElhWqJva",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X_train, X_val, y_train, y_val = train_test_split(images, labels)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gAjrtSersbOC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Optimizer params\n",
        "optimizer_config = {'learning_rate' : 1e-2}\n",
        "optimizer_state = {}\n",
        "\n",
        "# Looping params\n",
        "n_epoch = 20\n",
        "batch_size = 512"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qczTQ6oqpyEu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "net = Sequential()\n",
        "net.add(Linear(28*28, 10))\n",
        "net.add(LogSoftMax())\n",
        "\n",
        "criterion = ClassNLLCriterion()\n",
        "\n",
        "print(net)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uRAGNc6nqCVh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def train():\n",
        "    train_loss_history = []\n",
        "    val_loss_history = [0,]\n",
        "    val_acc_history = [0,]\n",
        "    steps = [0,]\n",
        "\n",
        "    for i in range(n_epoch):\n",
        "        steps.append(steps[-1])\n",
        "        for x_batch, y_batch in get_batches((X_train, y_train), batch_size):\n",
        "            \n",
        "            net.zeroGradParameters()\n",
        "            x_batch = x_batch.reshape(-1, 28*28)\n",
        "            \n",
        "            # Forward\n",
        "            predictions = net.forward(x_batch)\n",
        "            loss = criterion.forward(predictions, y_batch)\n",
        "        \n",
        "            # Backward\n",
        "            dp = criterion.backward(predictions, y_batch)\n",
        "            net.backward(x_batch, dp)\n",
        "            \n",
        "            # Update weights\n",
        "            simple_sgd(net.getParameters(), \n",
        "                    net.getGradParameters(), \n",
        "                    optimizer_config,\n",
        "                    optimizer_state)      \n",
        "            \n",
        "            train_loss_history.append(loss)\n",
        "            steps[-1] += 1\n",
        "\n",
        "        sum_loss = 0\n",
        "        sum_acc = 0\n",
        "        count_val_steps = 0\n",
        "        for x_batch, y_batch in get_batches((X_val, y_val), batch_size):\n",
        "            x_batch = x_batch.reshape(-1, 28*28)\n",
        "            predictions = net.forward(x_batch)\n",
        "            loss = criterion.forward(predictions, y_batch)\n",
        "            sum_loss += loss\n",
        "            sum_acc += accuracy_score(y_batch, np.argmax(predictions, axis=1))\n",
        "            count_val_steps += 1\n",
        "\n",
        "        val_loss_history.append(sum_loss / count_val_steps)\n",
        "        val_acc_history.append(sum_acc / count_val_steps)\n",
        "\n",
        "        # Visualize\n",
        "        display.clear_output(wait=True)\n",
        "        fig, ax = plt.subplots(1, 3, figsize=(15, 5))\n",
        "            \n",
        "        ax[0].set_title(\"Training loss\")\n",
        "        ax[0].set_xlabel(\"#iteration\")\n",
        "        ax[0].set_ylabel(\"loss\")\n",
        "        ax[0].plot(train_loss_history, 'b')\n",
        "        ax[0].grid()\n",
        "\n",
        "        ax[1].set_title(\"Validation loss\")\n",
        "        ax[1].set_xlabel(\"#iteration\")\n",
        "        ax[1].set_ylabel(\"loss\")\n",
        "        ax[1].plot(steps, val_loss_history, 'b')\n",
        "        ax[1].grid()\n",
        "\n",
        "        ax[2].set_title(\"Validation Accuracy\")\n",
        "        ax[2].set_xlabel(\"#iteration\")\n",
        "        ax[2].set_ylabel(\"accuracy\")\n",
        "        ax[2].plot(steps, val_acc_history, 'b')\n",
        "        ax[2].grid()\n",
        "\n",
        "        plt.show()\n",
        "        \n",
        "        print('Current loss: %f' % (sum_loss / count_val_steps))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0SrSQjT9qZA1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BVblbAuIqaQL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}