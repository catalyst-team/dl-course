{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.9"
    },
    "colab": {
      "name": "transfer",
      "provenance": [],
      "collapsed_sections": []
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "XO75kadjJpQ2"
      },
      "source": [
        "!pip install catalyst transformers datasets nlpaug"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r3XJ3KvFJpQ4"
      },
      "source": [
        "# Seminar\n",
        "Hi! Today we are build simple pipeline for a sentiment analysis task. Our target dataset will be IMDB, that contains movie reviews. We try to solve the task by transformer model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fSzyeMyBJpQ5"
      },
      "source": [
        "import torch\n",
        "from catalyst.utils import set_global_seed, get_device\n",
        "\n",
        "set_global_seed(42)\n",
        "device = \"cuda:0\"\n",
        "# device = get_device()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V16xBXQAJpQ6"
      },
      "source": [
        "To work with dataset we use [datasets](https://github.com/huggingface/datasets) by ðŸ¤— `huggingface`. It can work with a custom dataset. But the dataset \"IMDB\" will be downloaded:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sk95HsyKJpQ6"
      },
      "source": [
        "from datasets import load_dataset\n",
        "\n",
        "\n",
        "imdb_dataset = load_dataset(\"imdb\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H0509FM7JpQ6"
      },
      "source": [
        "Look at the dataset methods and features:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U0WUFUyIJpQ7"
      },
      "source": [
        "imdb_dataset"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YtqNigA4JpQ7"
      },
      "source": [
        "imdb_dataset[\"train\"][0]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hK4u9X8sJpQ7"
      },
      "source": [
        "test = imdb_dataset[\"train\"][0][\"text\"]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZYih5DYJJpQ8"
      },
      "source": [
        "To tokenize texts, we will use pretrained BPE tokenizer."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LXtbh8CpJpQ8"
      },
      "source": [
        "from transformers import BertTokenizer\n",
        "\n",
        "tokenizer = BertTokenizer.from_pretrained(\"google/bert_uncased_L-6_H-256_A-4\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lAo7PsMcJpQ8"
      },
      "source": [
        "Examples of text tokenization, encoding, etc:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vEZ5JJe-JpQ8"
      },
      "source": [
        "print(tokenizer.tokenize(test))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JPVpJvwrJpQ8"
      },
      "source": [
        "print(tokenizer.encode(test))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HKE_YAGJJpQ9"
      },
      "source": [
        "Tokenizer has additional functions to create attention masks, get offsets mapping or token types to train transformer models."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QVwbvRUYJpQ9"
      },
      "source": [
        "print(tokenizer.encode_plus(test))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LgMmV7dOJpQ9"
      },
      "source": [
        "print(tokenizer.encode_plus(test, max_length=64, truncation=True, padding=\"max_length\"))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ifxNg3ZbJpQ9"
      },
      "source": [
        "Tokenizer can change return type."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8TpH45nvJpQ9"
      },
      "source": [
        "\n",
        "print(tokenizer.encode_plus(test, max_length=64, truncation=True, padding=\"max_length\", return_tensors=\"pt\"))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IXf6SXziJpQ-"
      },
      "source": [
        "Use information about tokenizer, create train_dataloader."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ND5Lmvb1JpQ-"
      },
      "source": [
        "import torch\n",
        "\n",
        "from catalyst.utils import get_loader\n",
        "\n",
        "\n",
        "def text_data_transforms(row):\n",
        "    tokens = tokenizer.encode_plus(row[\"text\"], max_length=64, truncation=True, padding=\"max_length\", return_tensors=\"pt\")\n",
        "    tokens = {k: v[0] for k, v in tokens.items()}\n",
        "    tokens.update({\"targets\": row[\"label\"]})\n",
        "    return tokens\n",
        "    \n",
        "\n",
        "train_dataloader = get_loader(\n",
        "    imdb_dataset[\"train\"],\n",
        "    open_fn=lambda x: x,\n",
        "    dict_transform=text_data_transforms,\n",
        "    batch_size=256,\n",
        "    num_workers=4,\n",
        "    shuffle=True,\n",
        "    drop_last=True,\n",
        ")\n",
        "\n",
        "valid_dataloader = get_loader(\n",
        "    imdb_dataset[\"test\"],\n",
        "    open_fn=lambda x: x,\n",
        "    dict_transform=text_data_transforms,\n",
        "    batch_size=256,\n",
        "    num_workers=4,\n",
        "    shuffle=True,\n",
        "    drop_last=True,\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RIIusrtvJpQ-"
      },
      "source": [
        "loaders = {\n",
        "    \"train\": train_dataloader,\n",
        "    \"valid\": valid_dataloader\n",
        "}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0WBrtdmlJpQ-"
      },
      "source": [
        "Load BERT model for SequenceClassification. We need models smaller, than `bert-uncased-base`. List of the all model: [model names](https://huggingface.co/models)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ludROL3vJpQ-"
      },
      "source": [
        "from transformers import BertForSequenceClassification\n",
        "\n",
        "model = BertForSequenceClassification.from_pretrained(\"google/bert_uncased_L-6_H-256_A-4\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fSYyvmVRJpQ-"
      },
      "source": [
        "Usual train code:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2GqGPzxgJpQ_"
      },
      "source": [
        "from catalyst.contrib.nn import RAdam\n",
        "from torch.nn import CrossEntropyLoss\n",
        "\n",
        "\n",
        "optimizer = RAdam(model.parameters(), lr=2e-4)\n",
        "criterion = CrossEntropyLoss()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jdXVRY6lJpQ_"
      },
      "source": [
        "from datetime import datetime\n",
        "from pathlib import Path\n",
        "\n",
        "\n",
        "logdir=Path(\"logs\") / datetime.now().strftime(\"%Y%m%d-%H%M%S\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N51pSGfxJpQ_"
      },
      "source": [
        "from catalyst.dl import SupervisedRunner\n",
        "\n",
        "\n",
        "class BertRunner(SupervisedRunner):\n",
        "    def handle_batch(self, batch):\n",
        "        output = self.model(**{k: self.batch[k] for k in [\"input_ids\", \"attention_mask\"]}, return_dict=True)\n",
        "        self.batch.update(output)\n",
        "\n",
        "\n",
        "runner = BertRunner()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RXRYhPrmJpQ_"
      },
      "source": [
        "from catalyst.dl import AccuracyCallback\n",
        "\n",
        "\n",
        "runner.train(\n",
        "    model=model,\n",
        "    optimizer=optimizer,\n",
        "    criterion=criterion,\n",
        "    loaders=loaders,\n",
        "    logdir=logdir,\n",
        "    num_epochs=3,\n",
        "    verbose=True,\n",
        "    callbacks=[AccuracyCallback(\"logits\", \"targets\", num_classes=2)],\n",
        "    valid_loader = \"valid\",\n",
        "    valid_metric = \"accuracy\"\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dtsaVBT1JpQ_"
      },
      "source": [
        "## Text Augmentation\n",
        "\n",
        "To improve our sentiment analyser, we need more data. One way to get new samples is data augmentation methods. For text we can change characters, words or sentences. Our tool for text augmentation will be [nlpaug](https://github.com/makcedward/nlpaug) library. \n",
        "\n",
        "Our example is a little simple sentence:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fn7RXLYlJpRA"
      },
      "source": [
        "import nlpaug.augmenter.char as nac\n",
        "import nlpaug.augmenter.word as naw\n",
        "\n",
        "text = 'The quick brown fox jumps over the lazy dog .'\n",
        "print(text)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Er8PwcInJpRA"
      },
      "source": [
        "Let's try to change characters by random. Probabilities of swaping between two characters are made by keybord distance (on QWERTY keybord)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EX3fVNoIJpRA"
      },
      "source": [
        "aug = nac.KeyboardAug()\n",
        "augmented_text = aug.augment(text)\n",
        "print(\"Original:\")\n",
        "print(text)\n",
        "print(\"Augmented Text:\")\n",
        "print(augmented_text)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HtrQedqmJpRA"
      },
      "source": [
        "Another way to change sentence is a replacing a word with its synonim:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7a0s-JIkhXmD"
      },
      "source": [
        "import nltk\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "nltk.download('wordnet')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2CBhHa5JJpRA"
      },
      "source": [
        "aug = naw.SynonymAug(aug_src='wordnet')\n",
        "augmented_text = aug.augment(text)\n",
        "print(\"Original:\")\n",
        "print(text)\n",
        "print(\"Augmented Text:\")\n",
        "print(augmented_text)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r1thcVLhJpRB"
      },
      "source": [
        "More accurate way to swap words can be done by pretrained Language Model. We can work with BERT-like models. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "78MspxvGJpRB"
      },
      "source": [
        "aug = naw.ContextualWordEmbsAug(\n",
        "    model_path='bert-base-uncased', \n",
        "    action=\"substitute\"\n",
        ")\n",
        "augmented_text = aug.augment(text)\n",
        "print(\"Original:\")\n",
        "print(text)\n",
        "print(\"Augmented Text:\")\n",
        "print(augmented_text)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CHItLJfgJpRC"
      },
      "source": [
        "In our seminar, we try to work with smaller version of BERT:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xKAs3_VYJpRC"
      },
      "source": [
        "aug = nac.KeyboardAug()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eFcYjMzFJpRC"
      },
      "source": [
        "Create a new `dict_transform` function. It need to work with text and change it."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3kMaPyhmJpRC"
      },
      "source": [
        "def aug_text_data_transforms(row):\n",
        "    # Because this augmentation is pretty slow\n",
        "    # we need to truncate working text.\n",
        "    # It's better to generate examples offline,\n",
        "    # and than fit a model with the bigger dataset.\n",
        "    sentence = aug.augment(row[\"text\"])\n",
        "    tokens = tokenizer.encode_plus(\n",
        "        sentence,\n",
        "        max_length=64,\n",
        "        truncation=True,\n",
        "        padding=\"max_length\",\n",
        "        return_tensors=\"pt\",\n",
        "    )\n",
        "    tokens = {k: v[0] for k, v in tokens.items()}\n",
        "    tokens.update({\"targets\": row[\"label\"]})\n",
        "    return tokens\n",
        "\n",
        "\n",
        "aug_train_dataloader = get_loader(\n",
        "    imdb_dataset[\"train\"],\n",
        "    open_fn=lambda x: x,\n",
        "    dict_transform=aug_text_data_transforms,\n",
        "    batch_size=256,\n",
        "    num_workers=2,\n",
        "    shuffle=True,\n",
        "    drop_last=True,\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0chmgZSaJpRD"
      },
      "source": [
        "aug_loaders = {\n",
        "    \"train\": aug_train_dataloader,\n",
        "    \"valid\": valid_dataloader\n",
        "}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7950mAHPJpRD"
      },
      "source": [
        "model = BertForSequenceClassification.from_pretrained(\"google/bert_uncased_L-6_H-256_A-4\")\n",
        "optimizer = RAdam(model.parameters(), lr=2e-4)\n",
        "criterion = CrossEntropyLoss()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TLz7dH9wJpRD"
      },
      "source": [
        "logdir = Path(\"logs\") / datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
        "runner = BertRunner()\n",
        "runner.train(\n",
        "    model=model,\n",
        "    optimizer=optimizer,\n",
        "    criterion=criterion,\n",
        "    loaders=aug_loaders,\n",
        "    logdir=logdir,\n",
        "    num_epochs=3,\n",
        "    verbose=True,\n",
        "    callbacks=[AccuracyCallback(\"logits\", \"targets\", num_classes=2)],\n",
        "    valid_loader=\"valid\",\n",
        "    valid_metric=\"accuracy\",\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5yAV4V4HJpRD"
      },
      "source": [
        "**Note** \n",
        "\n",
        "The best method of text data augmentation is Back Translation. But we need trained model from one language to another to work. `nlpaug` uses huge transfomer models (~ 10Gb), that why I don't use them in the seminar."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f1kheHrmJpRD"
      },
      "source": [
        "## Domain adaptation\n",
        "\n",
        "Instead of adding new examples by Data Augmentation, we can add new example from similar task. The Sentiment Analysis task has several datasets, like SST-2, YELP, AMAZON-Review. To increase the model performance, try to add some samples from SST-2 dataset:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e3McLdYgJpRE"
      },
      "source": [
        "sst_dataset = load_dataset(\"glue\", \"sst2\", split=\"train[:10%]\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yPtDG9aQJpRE"
      },
      "source": [
        "sst_dataset[0]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RFZFPqdmJpRE"
      },
      "source": [
        "Prepare text and train a model:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wJwCJA-aJpRE"
      },
      "source": [
        "def sst_text_data_transforms(row):\n",
        "    tokens = tokenizer.encode_plus(\n",
        "        row[\"sentence\"],\n",
        "        max_length=64,\n",
        "        truncation=True,\n",
        "        padding=\"max_length\",\n",
        "        return_tensors=\"pt\",\n",
        "    )\n",
        "    tokens = {k: v[0] for k, v in tokens.items()}\n",
        "    tokens.update({\"targets\": row[\"label\"]})\n",
        "    return tokens\n",
        "\n",
        "\n",
        "sst_train_dataloader = get_loader(\n",
        "    sst_dataset,\n",
        "    open_fn=lambda x: x,\n",
        "    dict_transform=sst_text_data_transforms,\n",
        "    batch_size=256,\n",
        "    num_workers=4,\n",
        "    shuffle=True,\n",
        "    drop_last=True,\n",
        ")\n",
        "\n",
        "\n",
        "sst_loaders = {\n",
        "    \"train\": sst_train_dataloader,\n",
        "}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FhdTQMpKJpRE"
      },
      "source": [
        "model = BertForSequenceClassification.from_pretrained(\"google/bert_uncased_L-6_H-256_A-4\")\n",
        "optimizer = RAdam(model.parameters(), lr=2e-4)\n",
        "criterion = CrossEntropyLoss()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-bojG_EBJpRF"
      },
      "source": [
        "logdir=Path(\"logs\") / datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
        "runner = BertRunner()\n",
        "runner.train(\n",
        "    model=model,\n",
        "    optimizer=optimizer,\n",
        "    criterion=criterion,\n",
        "    loaders=sst_loaders,\n",
        "    logdir=logdir,\n",
        "    num_epochs=3,\n",
        "    verbose=True,\n",
        "    callbacks=[AccuracyCallback(\"logits\", \"targets\", num_classes=2)]\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mihlrugSJpRG"
      },
      "source": [
        "Retrain our model on target dataset (IMDB):"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "urbU47ANJpRG"
      },
      "source": [
        "runner.train(\n",
        "    model=model,\n",
        "    optimizer=optimizer,\n",
        "    criterion=criterion,\n",
        "    loaders=loaders,\n",
        "    logdir=logdir,\n",
        "    num_epochs=3,\n",
        "    verbose=True,\n",
        "    callbacks=[AccuracyCallback(\"logits\", \"targets\", num_classes=2)],\n",
        "    valid_loader = \"valid\",\n",
        "    valid_metric = \"accuracy\"\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d-0CvtK0JpRG"
      },
      "source": [
        "Training model on randomly choosen samples haven't lead us to better perfomance. So, we need to find useful examples. To do this, we will use pretrained BERT model to get vector representation for each sample. We will compare vectors from the source dataset (SST-2) and the target dataset (IMDB) by cosine metric. Finally, we'll select examples from the source, that has the highest metric."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "crejEJXGJpRG"
      },
      "source": [
        "sst_dataset = load_dataset(\"glue\", \"sst2\", split=\"train\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9BB7FYGDJpRG"
      },
      "source": [
        "from transformers import BertModel\n",
        "\n",
        "\n",
        "model = BertModel.from_pretrained(\"google/bert_uncased_L-2_H-256_A-4\").to(device)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dCVu4Yl9JpRG"
      },
      "source": [
        "from tqdm.notebook import tqdm\n",
        "\n",
        "\n",
        "imdb_vectors = []\n",
        "\n",
        "\n",
        "with torch.no_grad():\n",
        "    for row in tqdm(imdb_dataset[\"train\"]):\n",
        "        row = text_data_transforms(row)\n",
        "        vector = model(\n",
        "            input_ids=row[\"input_ids\"].unsqueeze(0).to(device), \n",
        "            attention_mask=row[\"attention_mask\"].unsqueeze(0).to(device)\n",
        "        )[0][0, 0].cpu().numpy()\n",
        "        imdb_vectors.append(vector)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kXarjkPJJpRG"
      },
      "source": [
        "import numpy as np\n",
        "imdb_vectors = np.array(imdb_vectors)\n",
        "\n",
        "imdb_vectors_norm = imdb_vectors/np.linalg.norm(imdb_vectors, axis=1, keepdims=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U577arokJpRH"
      },
      "source": [
        "sst_scores = []\n",
        "\n",
        "with torch.no_grad():\n",
        "    for row in tqdm(sst_dataset):\n",
        "        row = sst_text_data_transforms(row)\n",
        "        vector = model(\n",
        "            input_ids=row[\"input_ids\"].unsqueeze(0).to(device), \n",
        "            attention_mask=row[\"attention_mask\"].unsqueeze(0).to(device)\n",
        "        )[0][0, 0].cpu().numpy()\n",
        "        sst_scores.append(np.mean(imdb_vectors_norm @ vector / np.linalg.norm(vector)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RWSRAtdmJpRH"
      },
      "source": [
        "Look at the scores distribution."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H4rOP4fiJpRH"
      },
      "source": [
        "%matplotlib inline\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "plt.hist(sst_scores)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vZKvvuSdJpRH"
      },
      "source": [
        "Our values are located in the interval from `0.3` to `0.8`. Choose threshold value to filter SST-2 samples:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ypLSmyOJJpRH"
      },
      "source": [
        "thr = 0.65\n",
        "\n",
        "indeces = [i for i, value in enumerate(sst_scores) if value > thr]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fqMxJll2JpRI"
      },
      "source": [
        "Repeat the model training procedure."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-q5dLgZIJpRI"
      },
      "source": [
        "sst_train_dataloader = get_loader(\n",
        "    sst_dataset.select(indeces),\n",
        "    open_fn=lambda x: x,\n",
        "    dict_transform=sst_text_data_transforms,\n",
        "    batch_size=256,\n",
        "    num_workers=4,\n",
        "    shuffle=True,\n",
        "    drop_last=True,\n",
        ")\n",
        "\n",
        "\n",
        "sst_loaders = {\n",
        "    \"train\": sst_train_dataloader,\n",
        "}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xyKTcdlrJpRI"
      },
      "source": [
        "model = BertForSequenceClassification.from_pretrained(\"google/bert_uncased_L-6_H-256_A-4\")\n",
        "optimizer = RAdam(model.parameters(), lr=2e-4)\n",
        "criterion = CrossEntropyLoss()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zyvF-QdlJpRI"
      },
      "source": [
        "logdir=Path(\"logs\") / datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
        "runner = BertRunner()\n",
        "runner.train(\n",
        "    model=model,\n",
        "    optimizer=optimizer,\n",
        "    criterion=criterion,\n",
        "    loaders=sst_loaders,\n",
        "    logdir=logdir,\n",
        "    num_epochs=3,\n",
        "    verbose=True,\n",
        "    callbacks=[AccuracyCallback(\"logits\", \"targets\", num_classes=2)]\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yOciPLBsJpRI"
      },
      "source": [
        "runner.train(\n",
        "    model=model,\n",
        "    optimizer=optimizer,\n",
        "    criterion=criterion,\n",
        "    loaders=loaders,\n",
        "    logdir=logdir,\n",
        "    num_epochs=3,\n",
        "    verbose=True,\n",
        "    callbacks=[AccuracyCallback(\"logits\", \"targets\", num_classes=2)],\n",
        "    valid_loader = \"valid\",\n",
        "    valid_metric = \"accuracy\"\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}