{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install -Uq catalyst==20.12 gym==0.17.3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Seminar. RL, DQN.\n",
    "\n",
    "Hi! In the first part of the seminar, we are going to introduce one of the main algorithm in the Reinforcment Learning domain. Deep Q-Network is the pioneer algorithm, that amalmagates Q-Learning and Deep Neural Networks. And there is small review on gym enviroments, where our bots will play in games."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the beginning, look at the algorithm:\n",
    "\n",
    "![DQN algorithm](https://i.stack.imgur.com/Jnyff.jpg)\n",
    "There are several differences between the usual DL and RL routines. Our bots are trained by his actions, that he has done in the past. We don't have infinity memory, but we can save some actions in the buffer. Let's code it!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from catalyst.utils import set_global_seed, get_device\n",
    "\n",
    "\n",
    "set_global_seed(42)\n",
    "device = get_device()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import deque, namedtuple\n",
    "import random\n",
    "import numpy as np\n",
    "import gym\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import typing as tp\n",
    "from collections import deque, namedtuple\n",
    "\n",
    "\n",
    "Transition = namedtuple(\n",
    "    'Transition', \n",
    "    field_names=[\n",
    "        'state', \n",
    "        'action', \n",
    "        'reward',\n",
    "        'done', \n",
    "        'next_state'\n",
    "    ]\n",
    ")\n",
    "\n",
    "\n",
    "class ReplayBuffer:\n",
    "    def __init__(self, capacity: int):\n",
    "        self.buffer = deque(maxlen=capacity)\n",
    "    \n",
    "    def append(self, transition: Transition):\n",
    "        self.buffer.append(transition)\n",
    "    \n",
    "    def sample(self, size: int) -> tp.Sequence[np.array]:\n",
    "        indices = np.random.choice(\n",
    "            len(self.buffer),\n",
    "            size,\n",
    "            replace=size > len(self.buffer)\n",
    "        )\n",
    "        states, actions, rewards, dones, next_states = \\\n",
    "            zip(*[self.buffer[idx] for idx in indices])\n",
    "        states, actions, rewards, dones, next_states = (\n",
    "            np.array(states, dtype=np.float32),\n",
    "            np.array(actions, dtype=np.int64),\n",
    "            np.array(rewards, dtype=np.float32),\n",
    "            np.array(dones, dtype=np.bool),\n",
    "            np.array(next_states, dtype=np.float32)\n",
    "        )\n",
    "        return states, actions, rewards, dones, next_states\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return len(self.buffer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To work well with Catalyst train loops, implement intermedeate abstraction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data.dataset import IterableDataset\n",
    "\n",
    "# as far as RL does not have some predefined dataset, \n",
    "# we need to specify epoch lenght by ourselfs\n",
    "class ReplayDataset(IterableDataset):\n",
    "\n",
    "    def __init__(self, buffer: ReplayBuffer, epoch_size: int = int(1e3)):\n",
    "        self.buffer = buffer\n",
    "        self.epoch_size = epoch_size\n",
    "\n",
    "    def __iter__(self) -> tp.Iterator[tp.Sequence[np.array]]:\n",
    "        states, actions, rewards, dones, next_states = \\\n",
    "            self.buffer.sample(self.epoch_size)\n",
    "        for i in range(len(dones)):\n",
    "            yield states[i], actions[i], rewards[i], dones[i], next_states[i]\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return self.epoch_size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After creating a Buffer, we need to gather action-value-state and save it in the buffer. We create one function, that asks model for action, and another function to communicate with the enviroment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_action(\n",
    "        env,\n",
    "        network: nn.Module,\n",
    "        state: np.array,\n",
    "        epsilon: float = -1\n",
    "    ) ->  int:\n",
    "    if np.random.random() < epsilon:\n",
    "        action = env.action_space.sample()\n",
    "    else:\n",
    "        state = torch.tensor(state[None], dtype=torch.float32)\n",
    "        q_values = network(state).detach().cpu().numpy()[0]\n",
    "        action = np.argmax(q_values)\n",
    "\n",
    "    return int(action)\n",
    "\n",
    "\n",
    "def generate_session(\n",
    "        env,\n",
    "        network: nn.Module,\n",
    "        t_max: int = 1000,\n",
    "        epsilon: float = -1,\n",
    "        replay_buffer: tp.Optional[ReplayBuffer] = None,\n",
    "    ) -> tp.Tuple[float, int]:\n",
    "    total_reward = 0\n",
    "    state = env.reset()\n",
    "\n",
    "    for t in range(t_max):\n",
    "        action = get_action(env, network, state=state, epsilon=epsilon)\n",
    "        next_state, reward, done, _ = env.step(action)\n",
    "\n",
    "        if replay_buffer is not None:\n",
    "            transition = Transition(\n",
    "                state, action, reward, done, next_state)\n",
    "            replay_buffer.append(transition)\n",
    "\n",
    "        total_reward += reward\n",
    "        state = next_state\n",
    "        if done:\n",
    "            break\n",
    "\n",
    "    return total_reward, t\n",
    "\n",
    "def generate_sessions(\n",
    "        env,\n",
    "        network: nn.Module,\n",
    "        t_max: int = 1000,\n",
    "        epsilon:float = -1,\n",
    "        replay_buffer: ReplayBuffer = None,\n",
    "        num_sessions: int = 100,\n",
    "    ) -> tp.Tuple[float, int]:\n",
    "    sessions_reward, sessions_steps = 0, 0\n",
    "    for i_episone in range(num_sessions):\n",
    "        r, t = generate_session(\n",
    "            env=env, \n",
    "            network=network,\n",
    "            t_max=t_max,\n",
    "            epsilon=epsilon,\n",
    "            replay_buffer=replay_buffer,\n",
    "        )\n",
    "        sessions_reward += r\n",
    "        sessions_steps += t\n",
    "    return sessions_reward, sessions_steps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we look closely into algorithm, we'll see that we need two networks. They looks the same, but one updates weights by gradients algorithm and second one by moving average with the first. This process helps to get stable training by REINFORCE."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def soft_update(target: nn.Module, source: nn.Module, tau: float):\n",
    "    \"\"\"Updates the target data with smoothing by ``tau``\"\"\"\n",
    "    for target_param, param in zip(target.parameters(), source.parameters()):\n",
    "        target_param.data.copy_(\n",
    "            target_param.data * (1.0 - tau) + param.data * tau\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To communicate with the Buffer, Catalyst's Runner requires adiitional Callback."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from catalyst import dl\n",
    "\n",
    "\n",
    "class GameCallback(dl.Callback):\n",
    "    \n",
    "    def __init__(\n",
    "        self, \n",
    "        *, \n",
    "        env, \n",
    "        replay_buffer: ReplayBuffer,\n",
    "        session_period: int,\n",
    "        epsilon: float,\n",
    "        epsilon_k: int,\n",
    "        actor_key,\n",
    "    ):\n",
    "        super().__init__(order=0)\n",
    "        self.env = env\n",
    "        self.replay_buffer = replay_buffer\n",
    "        self.session_period = session_period\n",
    "        self.epsilon = epsilon\n",
    "        self.epsilon_k = epsilon_k\n",
    "        self.actor_key = actor_key\n",
    "    \n",
    "    def on_stage_start(self, runner: dl.IRunner):\n",
    "        self.actor = runner.model[self.actor_key]\n",
    "        \n",
    "        self.actor.eval()\n",
    "        generate_sessions(\n",
    "            env=self.env, \n",
    "            network=self.actor,\n",
    "            epsilon=self.epsilon,\n",
    "            replay_buffer=self.replay_buffer,\n",
    "            num_sessions=1000,\n",
    "        )\n",
    "        self.actor.train()\n",
    "\n",
    "    def on_epoch_start(self, runner: dl.IRunner):\n",
    "        self.epsilon *= self.epsilon_k\n",
    "        self.session_counter = 0\n",
    "        self.session_steps = 0\n",
    "    \n",
    "    def on_batch_end(self, runner: dl.IRunner):\n",
    "        if runner.global_batch_step % self.session_period == 0:\n",
    "            self.actor.eval()\n",
    "            \n",
    "            session_reward, session_steps = generate_session(\n",
    "                env=self.env, \n",
    "                network=self.actor,\n",
    "                epsilon=self.epsilon,\n",
    "                replay_buffer=self.replay_buffer\n",
    "            )\n",
    "\n",
    "            self.session_counter += 1\n",
    "            self.session_steps += session_steps\n",
    "\n",
    "            runner.batch_metrics.update({\"s_reward\": session_reward})\n",
    "            runner.batch_metrics.update({\"s_steps\": session_steps})\n",
    "            \n",
    "            self.actor.train()\n",
    "\n",
    "    def on_epoch_end(self, runner: dl.IRunner):\n",
    "        num_sessions = 100\n",
    "        \n",
    "        self.actor.eval()\n",
    "        valid_rewards, valid_steps = generate_sessions(\n",
    "            env=self.env, \n",
    "            network=self.actor,\n",
    "            num_sessions=num_sessions\n",
    "        )\n",
    "        self.actor.train()\n",
    "        \n",
    "        valid_rewards /= num_sessions\n",
    "        runner.epoch_metrics[\"train_num_samples\"] = self.session_steps\n",
    "        runner.epoch_metrics[\"train_updates_per_sample\"] = \\\n",
    "            runner.loader_sample_step / self.session_steps\n",
    "        runner.epoch_metrics[\"train_v_reward\"] = valid_rewards\n",
    "        runner.epoch_metrics[\"train_epsilon\"] = self.epsilon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomRunner(dl.Runner):\n",
    "    \n",
    "    def __init__(\n",
    "        self, \n",
    "        *, \n",
    "        gamma: float,\n",
    "        tau: flaot,\n",
    "        tau_period: int = 1,\n",
    "        **kwargs,\n",
    "    ):\n",
    "        super().__init__(**kwargs)\n",
    "        self.gamma = gamma\n",
    "        self.tau = tau\n",
    "        self.tau_period = tau_period\n",
    "    \n",
    "    def on_stage_start(self, runner: dl.IRunner):\n",
    "        super().on_stage_start(runner)\n",
    "        soft_update(self.model[\"target\"], self.model[\"origin\"], 1.0)\n",
    "\n",
    "    def _handle_batch(self, batch: tp.Sequence[np.array]):\n",
    "        # model train/valid step\n",
    "        states, actions, rewards, dones, next_states = batch\n",
    "        network, target_network = self.model[\"origin\"], self.model[\"target\"]\n",
    "\n",
    "        # get q-values for all actions in current states\n",
    "        state_qvalues = network(states)\n",
    "        # select q-values for chosen actions\n",
    "        state_action_qvalues = \\\n",
    "            state_qvalues.gather(1, actions.unsqueeze(-1)).squeeze(-1)\n",
    "        \n",
    "        # compute q-values for all actions in next states\n",
    "        # compute V*(next_states) using predicted next q-values\n",
    "        # at the last state we shall use simplified formula: \n",
    "        # Q(s,a) = r(s,a) since s' doesn't exist\n",
    "        with torch.no_grad():\n",
    "            next_state_qvalues = target_network(next_states)\n",
    "            next_state_values = next_state_qvalues.max(1)[0]\n",
    "            next_state_values[dones] = 0.0\n",
    "            next_state_values = next_state_values.detach()\n",
    "\n",
    "        # compute \"target q-values\" for loss, \n",
    "        # it's what's inside square parentheses in the above formula.\n",
    "        target_state_action_qvalues = \\\n",
    "            next_state_values * self.gamma + rewards\n",
    "\n",
    "        # mean squared error loss to minimize\n",
    "        loss = self.criterion(\n",
    "            state_action_qvalues,\n",
    "            target_state_action_qvalues.detach()\n",
    "        )\n",
    "        self.batch_metrics.update({\"loss\": loss})\n",
    "\n",
    "        if self.is_train_loader:\n",
    "            loss.backward()\n",
    "            self.optimizer.step()\n",
    "            self.optimizer.zero_grad()\n",
    "\n",
    "            if self.global_batch_step % self.tau_period == 0:\n",
    "                soft_update(target_network, network, self.tau)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from catalyst import utils\n",
    "\n",
    "\n",
    "def get_network(env, num_hidden: int = 128):\n",
    "    inner_fn = utils.get_optimal_inner_init(nn.ReLU)\n",
    "    outer_fn = utils.outer_init\n",
    "    \n",
    "    network = torch.nn.Sequential(\n",
    "        nn.Linear(env.observation_space.shape[0], num_hidden),\n",
    "        nn.ReLU(),\n",
    "        nn.Linear(num_hidden, num_hidden),\n",
    "        nn.ReLU(),\n",
    "    )\n",
    "    head = nn.Linear(num_hidden, env.action_space.n)\n",
    "    \n",
    "    network.apply(inner_fn)\n",
    "    head.apply(outer_fn)\n",
    "\n",
    "    return torch.nn.Sequential(network, head)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data\n",
    "batch_size = 64\n",
    "epoch_size = int(1e3) * batch_size\n",
    "buffer_size = int(1e5)\n",
    "# runner settings, ~training\n",
    "gamma = 0.99\n",
    "tau = 0.01\n",
    "tau_period = 1 # in batches\n",
    "# callback, ~exploration\n",
    "session_period = 100 # in batches\n",
    "epsilon = 0.98\n",
    "epsilon_k = 0.9\n",
    "# optimization\n",
    "lr = 3e-4\n",
    "\n",
    "# env_name = \"LunarLander-v2\"\n",
    "env_name = \"CartPole-v1\"\n",
    "env = gym.make(env_name)\n",
    "replay_buffer = ReplayBuffer(buffer_size)\n",
    "\n",
    "network, target_network = get_network(env), get_network(env)\n",
    "utils.set_requires_grad(target_network, requires_grad=False)\n",
    "\n",
    "models = {\"origin\": network, \"target\": target_network}\n",
    "criterion = torch.nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(network.parameters(), lr=lr)\n",
    "\n",
    "loaders = {\n",
    "    \"train\": DataLoader(\n",
    "        ReplayDataset(replay_buffer, epoch_size=epoch_size), \n",
    "        batch_size=batch_size,\n",
    "    ),\n",
    "}\n",
    "\n",
    "\n",
    "runner = CustomRunner(\n",
    "    gamma=gamma, \n",
    "    tau=tau,\n",
    "    tau_period=tau_period,\n",
    "    \n",
    ")\n",
    "\n",
    "runner.train(\n",
    "    model=models,\n",
    "    criterion=criterion,\n",
    "    optimizer=optimizer,\n",
    "    loaders=loaders,\n",
    "    logdir=\"./logs_dqn\",\n",
    "    num_epochs=10,\n",
    "    verbose=True,\n",
    "    main_metric=\"v_reward\",\n",
    "    minimize_metric=False,\n",
    "    load_best_on_end=True,\n",
    "    callbacks=[\n",
    "        GameCallback(\n",
    "            env=env, \n",
    "            replay_buffer=replay_buffer, \n",
    "            session_period=session_period,\n",
    "            epsilon=epsilon,\n",
    "            epsilon_k=epsilon_k,\n",
    "            actor_key=\"origin\",\n",
    "        )\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And we can watch how our model plays in the games!\n",
    "\n",
    "\\* to run cells below, you should update your python environment. Instruction depends on your system specification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# record sessions\n",
    "import gym.wrappers\n",
    "\n",
    "\n",
    "env = gym.wrappers.Monitor(\n",
    "    gym.make(env_name),\n",
    "    directory=\"videos_dqn\", \n",
    "    force=True)\n",
    "generate_sessions(\n",
    "    env=env, \n",
    "    network=runner.model[\"origin\"],\n",
    "    num_sessions=100\n",
    ")\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# show video\n",
    "from IPython.display import HTML\n",
    "import os\n",
    "\n",
    "video_names = list(\n",
    "    filter(lambda s: s.endswith(\".mp4\"), os.listdir(\"./videos_dqn/\")))\n",
    "\n",
    "HTML(\"\"\"\n",
    "<video width=\"640\" height=\"480\" controls>\n",
    "  <source src=\"{}\" type=\"video/mp4\">\n",
    "</video>\n",
    "\"\"\".format(\"./videos/\"+video_names[-1]))  # this may or may not be _last_ video. Try other indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}