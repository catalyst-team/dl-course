{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BlJW0uN7h8aC"
   },
   "source": [
    "# 4 Seminar\n",
    "\n",
    "Hi! Today we are going to learn about image segmentation and object detection. We'll write and train some simple neural models for image segmentation, and look how object detection models work.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LB63wvFvW66a"
   },
   "outputs": [],
   "source": [
    "!pip install catalyst==20.12 albumentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LDDH1t_XW8cu"
   },
   "outputs": [],
   "source": [
    "from catalyst.utils import set_global_seed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TczGStMXXShp"
   },
   "outputs": [],
   "source": [
    "set_global_seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uKfCJVszWolP"
   },
   "source": [
    "## Segmentation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YoZUN7H0h8X5"
   },
   "source": [
    "### Dataset\n",
    "\n",
    "Look at the today task: cell segmentation! It's the dataset [ADDI](https://www.fc.up.pt/addi/ph2%20database.html). The dataset contained medical photo of skin with a mole or a melonoma. We'll just segment them on photo, without classification.\n",
    "\n",
    "<table><tr><td><img src=\"data:image/jpeg;base64,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\"></td><td><img src=\"data:image/jpeg;base64,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\"></td></tr></table>\n",
    "\n",
    "Download the archive and uncompess it.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qVGsvRA8m14_"
   },
   "outputs": [],
   "source": [
    "! wget https://www.dropbox.com/s/k88qukc20ljnbuo/PH2Dataset.rar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dLVEctpWm2GS"
   },
   "outputs": [],
   "source": [
    "get_ipython().system_raw(\"unrar x PH2Dataset.rar\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gfjhBOdVxqER"
   },
   "source": [
    "// File structure\n",
    "\n",
    "// Let's show the images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QrTSCMaYm2Du"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "from cv2 import erode\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from skimage.transform import rotate, rescale\n",
    "\n",
    "import catalyst\n",
    "from catalyst import dl\n",
    "from catalyst.utils import imread"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZBYFMYF1clIH"
   },
   "outputs": [],
   "source": [
    "images = []\n",
    "lesions = []\n",
    "\n",
    "root = 'PH2Dataset'\n",
    "\n",
    "for root, dirs, files in os.walk(os.path.join(root, 'PH2 Dataset images')):\n",
    "    if root.endswith('_Dermoscopic_Image'):\n",
    "        images.append(imread(os.path.join(root, files[0])))\n",
    "    if root.endswith('_lesion'):\n",
    "        lesions.append(imread(os.path.join(root, files[0])))\n",
    "\n",
    "images = np.array(images)\n",
    "lesions = np.array(lesions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fNtKdrNbeOC3"
   },
   "outputs": [],
   "source": [
    "_, ax = plt.subplots(1, 2, figsize=(15, 10))\n",
    "\n",
    "ax[0].imshow(images[0])\n",
    "ax[1].imshow(lesions[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "foqbxeyWeN-w"
   },
   "outputs": [],
   "source": [
    "thr = 0.5\n",
    "\n",
    "example_lesion = (lesions[0] / 255).mean(2)\n",
    "transformed_lesion = (rotate(example_lesion, 40) > thr) * 1.0\n",
    "\n",
    "_, ax = plt.subplots(1, 2, figsize=(15, 10))\n",
    "\n",
    "ax[0].imshow(example_lesion, cmap=\"gray\")\n",
    "ax[1].imshow(transformed_lesion, cmap=\"gray\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bQcWQq6-yDeh"
   },
   "source": [
    "### IoU\n",
    "\n",
    "![iou](https://pyimagesearch.com/wp-content/uploads/2016/09/iou_equation.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "y8EVe03mfYb4"
   },
   "outputs": [],
   "source": [
    "intersection = # Calculate intersection\n",
    "\n",
    "_, ax = plt.subplots(1, 3, figsize=(15, 10))\n",
    "\n",
    "red_true = np.concatenate(\n",
    "    [example_lesion[:, :, np.newaxis], np.zeros(example_lesion.shape + (2,))],\n",
    "    axis=2,\n",
    ")\n",
    "blue_fake = np.concatenate(\n",
    "    [\n",
    "        np.zeros(example_lesion.shape + (2,)),\n",
    "        transformed_lesion[:, :, np.newaxis],\n",
    "    ],\n",
    "    axis=2,\n",
    ")\n",
    "\n",
    "ax[0].imshow(red_true)\n",
    "ax[1].imshow((intersection[:,:,np.newaxis] * 0.5 + red_true + blue_fake))\n",
    "ax[2].imshow(blue_fake)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DiiINYPifYY7"
   },
   "outputs": [],
   "source": [
    "union = # Calculate union\n",
    "\n",
    "_, ax = plt.subplots(1, 3, figsize=(15, 10))\n",
    "\n",
    "red_true = np.concatenate(\n",
    "    [example_lesion[:, :, np.newaxis], np.zeros(example_lesion.shape + (2,))],\n",
    "    axis=2,\n",
    ")\n",
    "blue_fake = np.concatenate(\n",
    "    [\n",
    "        np.zeros(example_lesion.shape + (2,)),\n",
    "        transformed_lesion[:, :, np.newaxis],\n",
    "    ],\n",
    "    axis=2,\n",
    ")\n",
    "\n",
    "ax[0].imshow(red_true)\n",
    "ax[1].imshow(union, cmap=\"gray\")\n",
    "ax[2].imshow(blue_fake)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6-iG09QmfYR7"
   },
   "outputs": [],
   "source": [
    "iou = np.sum(intersection) / np.sum(union)\n",
    "print(f\"Current IoU: {iou}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "c9g4nVsQy3pf"
   },
   "outputs": [],
   "source": [
    "# Is it loss?\n",
    "intersection_l = # Calculate \"smooth\" intersection\n",
    "union_l = # Calculate \"smooth\" union\n",
    "iou_l = np.sum(intersection_l) / np.sum(union_l)\n",
    "\n",
    "print(f\"IoU: {iou_l}, Current IoU Loss: {1 - iou_l}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lHL5XDwhuVQI"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from catalyst.utils import metric\n",
    "\n",
    "\n",
    "print(f\"IoU {metric.iou(torch.from_numpy(transformed_lesion), torch.from_numpy(example_lesion), activation=None)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hcHqpB1uyDbr"
   },
   "source": [
    "### Dice\n",
    "\n",
    "![](https://miro.medium.com/max/858/1*yUd5ckecHjWZf6hGrdlwzA.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "u2F2a8f5y4Or"
   },
   "outputs": [],
   "source": [
    "intersection = # Calculate intersection\n",
    "\n",
    "recall = # Calculate recall\n",
    "precision = # Calculate precision\n",
    "print(f\"Recall: {recall}. Precision: {precision}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2SZuMEV-nPCf"
   },
   "outputs": [],
   "source": [
    "f1 = # Calculate f1\n",
    "print(f\"F1: {f1}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bfzTr5ownPAU"
   },
   "outputs": [],
   "source": [
    "f1 = (\n",
    "    # Rewrite formula with intersection, predictions and ground truth\n",
    ")\n",
    "print(f\"Another F1: {f1}. It's DICE!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LL4XcKaNv0x_"
   },
   "outputs": [],
   "source": [
    "print(f\"Dice {metric.dice(torch.from_numpy(transformed_lesion), torch.from_numpy(example_lesion), activation=None)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PNsNmI6kyDWe"
   },
   "source": [
    "### Morphological Transformations\n",
    "\n",
    "\n",
    "Morphological transformation is a useful toolkit to enhance maskes or semantic maps. If a mask is noisy, we can remove error points by erosion."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cPO3IfjGp_bl"
   },
   "outputs": [],
   "source": [
    "thr =  0.5\n",
    "\n",
    "transformed_lesion = rotate(example_lesion, 40) * 1.0\n",
    "noise = np.random.randn(*(s // 20 for s in example_lesion.shape))\n",
    "noise = rescale(noise, 30)[\n",
    "    : example_lesion.shape[0], : example_lesion.shape[1]\n",
    "]\n",
    "noised_mask = 0.7 * transformed_lesion + 0.3 * noise\n",
    "noised_mask = (noised_mask - noised_mask.min()) / (\n",
    "    noised_mask.max() - noised_mask.min()\n",
    ")\n",
    "\n",
    "_, ax = plt.subplots(1, 3, figsize=(15, 10))\n",
    "\n",
    "ax[0].imshow(example_lesion, cmap=\"gray\")\n",
    "ax[1].imshow(transformed_lesion > thr, cmap=\"gray\")\n",
    "ax[2].imshow(noised_mask > thr, cmap=\"gray\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GnKzII3jyvUm"
   },
   "source": [
    "How erosion works?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "J4X_AkVZyuzI"
   },
   "outputs": [],
   "source": [
    "kernel = np.ones((3, 3)).astype(np.uint8)\n",
    "print(f\"Kernel:\\n{kernel}\")\n",
    "\n",
    "example = np.ones((10, 10))\n",
    "example[3, 4] = 0\n",
    "print(f\"Example:\\n{example}\")\n",
    "\n",
    "padded_example = np.zeros((12, 12))\n",
    "padded_example[1:11, 1:11] = example\n",
    "result = np.zeros((10, 10)) \n",
    "for i in range(10):\n",
    "    for j in range(10):\n",
    "        result[i, j] = np.min(padded_example[i:i + 3, j:j + 3] * kernel)\n",
    "\n",
    "print(f\"Result:\\n{result}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "G3q7WmZpxDvu"
   },
   "source": [
    "Let's use it.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sD9di6NcwxVi"
   },
   "outputs": [],
   "source": [
    "kernel = np.ones((5, 5)).astype(np.uint8)\n",
    "eroded = (\n",
    "    erode((noised_mask * 255).astype(np.uint8), kernel, iterations=5) / 255\n",
    ")\n",
    "\n",
    "_, ax = plt.subplots(1, 3, figsize=(15, 10))\n",
    "\n",
    "ax[0].imshow(example_lesion, cmap=\"gray\")\n",
    "ax[1].imshow(noised_mask > thr, cmap=\"gray\")\n",
    "ax[2].imshow(eroded > thr, cmap=\"gray\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sY4hzUjpMfuA"
   },
   "source": [
    "## Data Augmentation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RTcVtI0dMkgk"
   },
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "\n",
    "\n",
    "class SegmentationDataset(Dataset):\n",
    "    def __init__(self, images=None, masks=None, transforms=None) -> None:\n",
    "        self.images = images\n",
    "        self.masks = masks\n",
    "        self.transforms = transforms\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return len(self.images)\n",
    "\n",
    "    def __getitem__(self, idx: int) -> dict:\n",
    "        image = self.images[idx]\n",
    "\n",
    "        result = {\"image\": image}\n",
    "\n",
    "        if self.masks is not None:\n",
    "            result[\"mask\"] = self.masks[idx].mean(2).astype(int) // 255\n",
    "\n",
    "        if self.transforms is not None:\n",
    "            result = self.transforms(**result)\n",
    "\n",
    "        return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "EL_uxA1sMkfU"
   },
   "outputs": [],
   "source": [
    "import albumentations as albu\n",
    "from albumentations.pytorch import ToTensorV2 as ToTensor\n",
    "\n",
    "\n",
    "def pre_transform(image_size: int = 224):\n",
    "    return albu.Resize(224, 224, p=1)\n",
    "\n",
    "\n",
    "def augmentations(image_size: int = 224):\n",
    "    channel_augs = [\n",
    "        albu.HueSaturationValue(p=0.5),\n",
    "        albu.ChannelShuffle(p=0.5),\n",
    "    ]\n",
    "\n",
    "    result = [\n",
    "        albu.OneOf(\n",
    "            [albu.IAAAdditiveGaussianNoise(), albu.GaussNoise(),], p=0.5\n",
    "        ),\n",
    "        albu.OneOf(\n",
    "            [\n",
    "                albu.MotionBlur(blur_limit=3, p=0.7),\n",
    "                albu.MedianBlur(blur_limit=3, p=1.0),\n",
    "                albu.Blur(blur_limit=3, p=0.7),\n",
    "            ],\n",
    "            p=0.5,\n",
    "        ),\n",
    "        albu.OneOf(channel_augs),\n",
    "        albu.OneOf(\n",
    "            [albu.CLAHE(clip_limit=2), albu.IAASharpen(), albu.IAAEmboss(),],\n",
    "            p=0.5,\n",
    "        ),\n",
    "        albu.RandomBrightnessContrast(\n",
    "            brightness_limit=0.5, contrast_limit=0.5, p=0.5\n",
    "        ),\n",
    "        albu.RandomGamma(p=0.5),\n",
    "        albu.OneOf([albu.MedianBlur(p=0.5), albu.MotionBlur(p=0.5)]),\n",
    "        albu.RandomGamma(gamma_limit=(85, 115), p=0.5),\n",
    "    ]\n",
    "    return albu.Compose(result)\n",
    "\n",
    "\n",
    "def post_transform():\n",
    "    return albu.Compose([albu.Normalize(), ToTensor()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XGZ3fTI0OJWA"
   },
   "outputs": [],
   "source": [
    "train_transformation = albu.Compose(\n",
    "    # Create train pipeline\n",
    ")\n",
    "\n",
    "valid_transformation = albu.Compose(\n",
    "    # Create valid pipeline\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BYz5COQSMkby"
   },
   "outputs": [],
   "source": [
    "_, ax = plt.subplots(2, 2, figsize=(15, 15))\n",
    "\n",
    "for i in range(4):\n",
    "    aug_image = train_transformation(image=images[0])[\"image\"].permute(1, 2, 0).numpy()\n",
    "    ax[i % 2][i // 2].imshow(aug_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FH56ygxjPOdm"
   },
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader, random_split\n",
    "\n",
    "\n",
    "batch_size = 8\n",
    "num_workers = 4\n",
    "\n",
    "\n",
    "indexes = np.arange(len(images))\n",
    "np.random.shuffle(indexes)\n",
    "train_indexes = indexes[:int(0.8 * len(images))]\n",
    "valid_indexes = indexes[int(0.8 * len(images)):]\n",
    "\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    # Create train loader\n",
    ")\n",
    "\n",
    "valid_loader = DataLoader(\n",
    "    # Create valid loader\n",
    ")\n",
    "\n",
    "loaders = {\"train\": train_loader, \"valid\": valid_loader}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sXVEL1AHUuXP"
   },
   "source": [
    "### U-Net\n",
    "\n",
    "![](https://www.researchgate.net/profile/Alan_Jackson9/publication/323597886/figure/fig2/AS:601386504957959@1520393124691/Convolutional-neural-network-CNN-architecture-based-on-UNET-Ronneberger-et-al.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "My3AHjPSy-AB"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "\n",
    "class UNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # Write model code !\n",
    "\n",
    "    def forward(self, image):\n",
    "        # Propagate image through model\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "a3PNdUKWK_uI"
   },
   "outputs": [],
   "source": [
    "from catalyst.contrib.nn.optimizers.radam import RAdam\n",
    "from catalyst.contrib.nn.criterion import DiceLoss, IoULoss\n",
    "\n",
    "\n",
    "model = UNet()\n",
    "criterion = {\n",
    "    \"dice\": DiceLoss(),\n",
    "    \"iou\": IoULoss(),\n",
    "    \"bce\": nn.BCEWithLogitsLoss()\n",
    "}\n",
    "optimizer = RAdam(model.parameters(), lr=1e-3)\n",
    "callbacks = [\n",
    "    dl.CriterionCallback(\n",
    "        input_key=\"mask\", prefix=\"loss_dice\", criterion_key=\"dice\"\n",
    "    ),\n",
    "    dl.CriterionCallback(\n",
    "        input_key=\"mask\", prefix=\"loss_iou\", criterion_key=\"iou\"\n",
    "    ),\n",
    "    dl.CriterionCallback(\n",
    "        input_key=\"mask\", prefix=\"loss_bce\", criterion_key=\"bce\"\n",
    "    ),\n",
    "    dl.MetricAggregationCallback(\n",
    "        prefix=\"loss\",\n",
    "        mode=\"weighted_sum\",\n",
    "        metrics={\"loss_dice\": 1.0, \"loss_iou\": 1.0, \"loss_bce\": 0.8},\n",
    "    ),\n",
    "    dl.DiceCallback(input_key=\"mask\"),\n",
    "    dl.IouCallback(input_key=\"mask\"),\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "EgzBrj9FCOJ5"
   },
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "\n",
    "runner = dl.SupervisedRunner(input_key=\"image\", input_target_key=\"mask\")\n",
    "\n",
    "\n",
    "runner.train(\n",
    "    model=model,\n",
    "    criterion=criterion,\n",
    "    optimizer=optimizer,\n",
    "    loaders=loaders,\n",
    "    callbacks=callbacks,\n",
    "    logdir=Path(\"logs\") / datetime.now().strftime(\"%Y%m%d-%H%M%S\"),\n",
    "    num_epochs=10,\n",
    "    main_metric=\"iou\",\n",
    "    minimize_metric=False,\n",
    "    verbose=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rEggRn3XUvs7"
   },
   "source": [
    "Let's look at the mask!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VHPpZJzvy919"
   },
   "outputs": [],
   "source": [
    "from torch.nn.functional import interpolate\n",
    "\n",
    "\n",
    "pt_image = valid_transformation(image=images[0])[\"image\"].unsqueeze(0).cuda()\n",
    "model.eval()\n",
    "\n",
    "pt_mask = torch.sigmoid(model(pt_image).detach().cpu())\n",
    "pt_mask = interpolate(pt_mask, images[0].shape[:-1]).squeeze()\n",
    "example_mask = pt_mask.squeeze().numpy()\n",
    "\n",
    "\n",
    "_, ax = plt.subplots(1, 3, figsize=(15, 10))\n",
    "\n",
    "ax[0].imshow(images[0])\n",
    "ax[1].imshow(lesions[0])\n",
    "ax[2].imshow(example_mask, cmap='gray')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "41MnzMWKUrgr"
   },
   "source": [
    "### FCN & Transfer Learning\n",
    "\n",
    "Using a pretrained ResNet model as a backbone for our segmentation model.\n",
    "\n",
    "![FCN](http://deeplearning.net/tutorial/_images/cat_segmentation.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "L4hcth0lzFzs"
   },
   "outputs": [],
   "source": [
    "class FCN(nn.Module):\n",
    "    def __init__(\n",
    "        self, pretrained_model, output_channel, image_size=224, p=0.1\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        self.pretrained_model = pretrained_model\n",
    "        self.output_channel = output_channel\n",
    "        self.image_size = image_size\n",
    "        self.p = p\n",
    "\n",
    "        self.deconv = nn.Sequential(\n",
    "            *(\n",
    "                self.make_up_block_(self.output_channel // d)\n",
    "                for d in [1, 4, 16, 64, 256]\n",
    "            )\n",
    "        )\n",
    "\n",
    "        self.output = nn.Conv2d(1, 1, kernel_size=7, stride=1, padding=3, bias=False)\n",
    "\n",
    "    def make_up_block_(self, output_channel):\n",
    "        return # Create upsampling block!\n",
    "\n",
    "    def forward(self, image):\n",
    "        features = self.pretrained_model(image)\n",
    "        mask = self.deconv(features)\n",
    "        return self.output(mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "htgMsTNpL5Nj"
   },
   "outputs": [],
   "source": [
    "from torchvision.models import mobilenet_v2\n",
    "\n",
    "\n",
    "pretrained_model = mobilenet_v2(pretrained=True)\n",
    "model = FCN(pretrained_model.features, 1280, 512)\n",
    "optimizer = RAdam(model.parameters(), lr=1e-3)\n",
    "\n",
    "runner = dl.SupervisedRunner(input_key=\"image\", input_target_key=\"mask\")\n",
    "runner.train(\n",
    "    model=model,\n",
    "    criterion=criterion,\n",
    "    optimizer=optimizer,\n",
    "    loaders=loaders,\n",
    "    callbacks=callbacks,\n",
    "    logdir=Path(\"logs\") / datetime.now().strftime(\"%Y%m%d-%H%M%S\"),\n",
    "    num_epochs=10,\n",
    "    main_metric=\"iou\",\n",
    "    minimize_metric=False,\n",
    "    verbose=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yh0U_xmqL0Zl"
   },
   "outputs": [],
   "source": [
    "pt_image = valid_transformation(image=images[0])[\"image\"].unsqueeze(0).cuda()\n",
    "model.eval()\n",
    "\n",
    "pt_mask = torch.sigmoid(model(pt_image).detach().cpu())\n",
    "pt_mask = interpolate(pt_mask, images[0].shape[:-1]).squeeze()\n",
    "example_mask = pt_mask.squeeze().numpy()\n",
    "\n",
    "\n",
    "_, ax = plt.subplots(1, 3, figsize=(15, 10))\n",
    "\n",
    "ax[0].imshow(images[0])\n",
    "ax[1].imshow(lesions[0])\n",
    "ax[2].imshow(example_mask, cmap='gray')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "owh-2zWePrEo"
   },
   "source": [
    "### Catalyst.Contrib\n",
    "\n",
    "Catalyst already has a segmentation models generator.\n",
    "There is a huge variety of model architectures, let's try one of them!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "deSZVEWFPujw"
   },
   "outputs": [],
   "source": [
    "from catalyst.contrib.models.cv.segmentation.unet import Unet\n",
    "\n",
    "model = Unet()\n",
    "optimizer = RAdam(model.parameters(), lr=1e-3)\n",
    "\n",
    "runner = dl.SupervisedRunner(input_key=\"image\", input_target_key=\"mask\")\n",
    "runner.train(\n",
    "    model=model,\n",
    "    criterion=criterion,\n",
    "    optimizer=optimizer,\n",
    "    loaders=loaders,\n",
    "    callbacks=callbacks,\n",
    "    logdir=Path(\"logs\") / datetime.now().strftime(\"%Y%m%d-%H%M%S\"),\n",
    "    num_epochs=10,\n",
    "    main_metric=\"iou\",\n",
    "    minimize_metric=False,\n",
    "    verbose=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "U7liNuzqQqd0"
   },
   "outputs": [],
   "source": [
    "pt_image = valid_transformation(image=images[0])[\"image\"].unsqueeze(0).cuda()\n",
    "model.eval()\n",
    "\n",
    "pt_mask = torch.sigmoid(model(pt_image).detach().cpu())\n",
    "pt_mask = interpolate(pt_mask, images[0].shape[:-1]).squeeze()\n",
    "example_mask = pt_mask.squeeze().numpy()\n",
    "\n",
    "\n",
    "_, ax = plt.subplots(1, 3, figsize=(15, 10))\n",
    "\n",
    "ax[0].imshow(images[0])\n",
    "ax[1].imshow(lesions[0])\n",
    "ax[2].imshow(example_mask, cmap='gray')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1XQFtAKeh8Vf"
   },
   "source": [
    "## Object Detection\n",
    "\n",
    "This part based on tutorial from [github](https://github.com/sgrvinod/a-PyTorch-Tutorial-to-Object-Detection).\n",
    "\n",
    " Firstly download all necessary files. They contain a codebase, finetuned model's state dict and fonts for showing bounding boxes.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1b8Fy32vpanC"
   },
   "outputs": [],
   "source": [
    "!git clone https://github.com/sgrvinod/a-PyTorch-Tutorial-to-Object-Detection\n",
    "!mv a-PyTorch-Tutorial-to-Object-Detection/* .\n",
    "!gdown https://drive.google.com/uc?id=1zmx5FXcnE9WC_rIZhh0qsy8Zom32YwNz&export=download\n",
    "!unzip Calibri_Font_Family.zip\n",
    "!gdown https://drive.google.com/uc?id=1bvJfF6r_zYl2xZEpYXxgb7jLQHFZ01Qe&export=download"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9_o1ZJA6_cci"
   },
   "source": [
    "Choose the victim. Object detection usualy is used for self-driving cars, so try to detect pedectrians on a  photo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UhuJCS-qtu9W"
   },
   "outputs": [],
   "source": [
    "!wget https://www1.nyc.gov/html/dot/images/pedestrians/ped-ramps-bay-ridge-ave.jpg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "aBsYYNFp4FSn"
   },
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "from copy import deepcopy\n",
    "\n",
    "original_image = Image.open('ped-ramps-bay-ridge-ave.jpg', mode='r')\n",
    "original_image = original_image.convert('RGB')\n",
    "\n",
    "plt.figure(figsize=(15, 10))\n",
    "plt.imshow(original_image)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KluwJXe2Appe"
   },
   "source": [
    "Prepare the image and the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dAYq3WOW4XKe"
   },
   "outputs": [],
   "source": [
    "from torchvision import transforms\n",
    "from utils import *\n",
    "from PIL import Image, ImageDraw, ImageFont\n",
    "\n",
    "from model import SSD300\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Load model checkpoint\n",
    "checkpoint = 'checkpoint_ssd300.pth.tar'\n",
    "checkpoint = torch.load(checkpoint, map_location_device=device)\n",
    "start_epoch = checkpoint['epoch'] + 1\n",
    "print('\\nLoaded checkpoint from epoch %d.\\n' % start_epoch)\n",
    "model = SSD300(checkpoint['model'].n_classes)\n",
    "model.load_state_dict(checkpoint['model'].state_dict())\n",
    "model = model.to(device)\n",
    "model.eval()\n",
    "\n",
    "# Transforms\n",
    "resize = # Create resize transformer image to (300, 300)\n",
    "to_tensor = # Create torch transformer\n",
    "normalize = # Create normalizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "source": [
    "Get model predictions. Object detection models usually returns class probabilities and bounding boxes predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "r0wmChLe4pQO"
   },
   "outputs": [],
   "source": [
    "image = normalize(to_tensor(resize(original_image)))\n",
    "\n",
    "# Move to default device\n",
    "image = image.to(device)\n",
    "\n",
    "# Forward prop.\n",
    "predicted_locs, predicted_scores = model(image.unsqueeze(0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vIFpdihEAvvB"
   },
   "source": [
    "Let's look into predicted locs and labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "EDoQ42F0AzmC"
   },
   "outputs": [],
   "source": [
    "print(f\"Locs shape: {predicted_locs.size()[1:]}, scores shape: {predicted_scores.size()[1:]}\")\n",
    "\n",
    "annotated_image = deepcopy(original_image)\n",
    "draw = ImageDraw.Draw(annotated_image)\n",
    "\n",
    "decoded_locs = cxcy_to_xy(gcxgcy_to_cxcy(predicted_locs[0], model.priors_cxcy)).cpu().detach()\n",
    "\n",
    "original_dims = torch.FloatTensor(\n",
    "    [original_image.width, original_image.height, original_image.width, original_image.height]).unsqueeze(0)\n",
    "decoded_locs = # Map locations to original image\n",
    "\n",
    "for box_loc in decoded_locs[::10]:\n",
    "    box_loc = box_loc.numpy()\n",
    "    draw.rectangle(xy=[l + 1. for l in box_loc], outline=\"black\") \n",
    "\n",
    "plt.figure(figsize=(15, 10))\n",
    "plt.imshow(annotated_image)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hALE76JBM_71"
   },
   "source": [
    "Filter boxes by on model confidence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jz7uaxnfGKDt"
   },
   "outputs": [],
   "source": [
    "probs = F.softmax(predicted_scores[0], dim=1)\n",
    "# First class – background\n",
    "best_scores, best_cls = # Find the best class for each box\n",
    "\n",
    "filtered = decoded_locs[best_scores >= 0.2]\n",
    "print(f\"Filtered locations: {len(filtered)}\")\n",
    "\n",
    "annotated_image = deepcopy(original_image)\n",
    "draw = ImageDraw.Draw(annotated_image)\n",
    "\n",
    "for box_loc in filtered:\n",
    "    box_loc = box_loc.numpy()\n",
    "    draw.rectangle(xy=[l for l in box_loc], outline=\"black\")\n",
    "\n",
    "\n",
    "plt.figure(figsize=(15, 10))\n",
    "plt.imshow(annotated_image)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eBSy1ne2NEtV"
   },
   "source": [
    "Calculate IoU(or Jaccard index, same things) to find overlapping boxes.\n",
    "This is the Non-Maximum Suppression algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PTdMj2HlHPPP"
   },
   "outputs": [],
   "source": [
    "annotated_image = deepcopy(original_image)\n",
    "draw = ImageDraw.Draw(annotated_image)\n",
    "\n",
    "filtered_scores = best_scores[best_scores > 0.2]\n",
    "filtered_cls = best_cls[best_scores > 0.2]\n",
    "\n",
    "result_box = []\n",
    "result_cls = []\n",
    "\n",
    "for c in range(0, model.n_classes):\n",
    "    _, indeces = # Filter scores and sort them from bigger to smaller\n",
    "    cls_boxes = filtered[filtered_cls == c]\n",
    "    cls_boxes = cls_boxes[indeces]\n",
    "    overlap = find_jaccard_overlap(cls_boxes, cls_boxes)\n",
    "\n",
    "    suppress = torch.zeros(cls_boxes.size(0), dtype=torch.uint8)\n",
    "    for i in range(cls_boxes.size(0)):\n",
    "        if suppress[i] == 1:\n",
    "            continue\n",
    "\n",
    "        condition = overlap[i] > 0.5\n",
    "        condition = torch.tensor(condition, dtype=torch.uint8)\n",
    "        suppress = torch.max(suppress, condition) \n",
    "\n",
    "        suppress[i] = 0\n",
    "\n",
    "    for s, box in zip(suppress, cls_boxes):\n",
    "        if s < 1:\n",
    "            result_box.append(box)\n",
    "            result_cls.append(c)\n",
    "\n",
    "print(f\"After another filter: {len(result_box)} boxes\")\n",
    "for box_loc in result_box:\n",
    "    box_loc = box_loc.numpy()\n",
    "    draw.rectangle(xy=[l for l in box_loc], outline=\"black\") \n",
    "\n",
    "plt.figure(figsize=(15, 10))\n",
    "plt.imshow(annotated_image)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ftltg40iNZq9"
   },
   "source": [
    "And the last task is creating annotations!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ePd73OrE6wbb"
   },
   "outputs": [],
   "source": [
    "annotated_image = deepcopy(original_image)\n",
    "draw = ImageDraw.Draw(annotated_image)\n",
    "font = ImageFont.truetype(\"./Calibri 400.ttf\", 15)\n",
    "\n",
    "for i in range(len(result_box)):\n",
    "    box_location = result_box[i].numpy()\n",
    "    label = rev_label_map[result_cls[i] + 1]\n",
    "    draw.rectangle(xy=box_location, outline=label_color_map[label])\n",
    "    draw.rectangle(xy=[l + 1. for l in box_location], outline=label_color_map[label]) \n",
    "\n",
    "    text_size = font.getsize(label.upper())\n",
    "    text_location = [box_location[0] + 2., box_location[1] - text_size[1]]\n",
    "    textbox_location = [box_location[0], box_location[1] - text_size[1], box_location[0] + text_size[0] + 4.,\n",
    "                        box_location[1]]\n",
    "    draw.rectangle(xy=textbox_location, fill=label_color_map[label])\n",
    "    draw.text(xy=text_location, text=label.upper(), fill='white',\n",
    "                font=font)\n",
    "    \n",
    "plt.figure(figsize=(15, 10))\n",
    "plt.imshow(annotated_image)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Extras\n",
    "Change model from the old SSD300 to the modern EfficientDet.\n",
    "Implementation and pretrained model's weights you can find in [@toandaominh1997/EfficientDet.Pytorch](https://github.com/toandaominh1997/EfficientDet.Pytorch)."
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "seminar.ipynb",
   "private_outputs": true,
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
