{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9HZHH8tnBiSU"
   },
   "outputs": [],
   "source": [
    "# ! pip install -Uq catalyst==20.12 gym==0.17.3 recsim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Seminar. RL meets RecSym\n",
    "\n",
    "Hi! In this part you are going to create a recommender bot by neural networks. We will use RL methods to train our bot. But we need to import and create a lot of things. Move on!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from catalyst.utils import set_global_seed, get_device\n",
    "\n",
    "\n",
    "set_global_seed(42)\n",
    "device = get_device()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "a6vyaDMhBiSU"
   },
   "outputs": [],
   "source": [
    "from collections import deque, namedtuple\n",
    "import random\n",
    "import numpy as np\n",
    "import gym\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from gym import spaces"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instead of gym, now we will use special library to create a bot's enviroment. It's called \"recsim\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from recsim import document, user\n",
    "from recsim.choice_model import AbstractChoiceModel\n",
    "from recsim.simulator import recsim_gym, environment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How will the bot work? Look at the picture:\n",
    "\n",
    "\n",
    "![Detailed view of RecSim](https://github.com/google-research/recsim/blob/master/recsim/colab/figures/simulator.png?raw=true)\n",
    "\n",
    "Green and blue boxes show the enviroment. We need to implement special classes, User and Document. Our bot(\"Agent\") have to choose from several documents the most relevant for the user. The user can move to the offered document if he accepts it, to random document overwise or stay on the current document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import typing as tp\n",
    "\n",
    "from catalyst import utils\n",
    "\n",
    "\n",
    "device = utils.get_device()\n",
    "utils.set_global_seed(42)\n",
    "\n",
    "DOC_NUM = 10\n",
    "EMB_SIZE = 4\n",
    "P_EXIT_ACCEPTED = 0.1\n",
    "P_EXIT_NOT_ACCEPTED = 0.2\n",
    "\n",
    "# let's define a matrix W for simulation of users' respose\n",
    "# (based on the section 7.3 of the paper https://arxiv.org/pdf/1512.07679.pdf)\n",
    "# W_ij defines the probability that a user will accept recommendation j\n",
    "# given that he is consuming item i at the moment\n",
    "\n",
    "W = (np.ones((DOC_NUM, DOC_NUM)) - np.eye(DOC_NUM)) * \\\n",
    "     np.random.uniform(0.0, P_EXIT_NOT_ACCEPTED, (DOC_NUM, DOC_NUM)) + \\\n",
    "     np.diag(np.random.uniform(1.0 - P_EXIT_ACCEPTED, 1.0, DOC_NUM))\n",
    "W = W[:, np.random.permutation(DOC_NUM)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Document(document.AbstractDocument):\n",
    "\n",
    "    def __init__(self, doc_id: int):\n",
    "        super().__init__(doc_id)\n",
    "\n",
    "    def create_observation(self):\n",
    "        return (self._doc_id,)\n",
    "\n",
    "    @staticmethod\n",
    "    def observation_space():\n",
    "        return spaces.Discrete(DOC_NUM)\n",
    "\n",
    "    def __str__(self):\n",
    "        return \"Document #{}\".format(self._doc_id)\n",
    "\n",
    "\n",
    "class DocumentSampler(document.AbstractDocumentSampler):\n",
    "\n",
    "    def __init__(self, doc_ctor=Document):\n",
    "        super().__init__(doc_ctor)\n",
    "        self._doc_count = 0\n",
    "\n",
    "    def sample_document(self) -> Document:\n",
    "        doc = self._doc_ctor(self._doc_count % DOC_NUM)\n",
    "        self._doc_count += 1\n",
    "        return doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class UserState(user.AbstractUserState):\n",
    "    def __init__(self, user_id: int, current: int, active_session=True):\n",
    "        self.user_id = user_id\n",
    "        self.current = current\n",
    "        self.active_session = active_session\n",
    "\n",
    "    def create_observation(self):\n",
    "        return (self.current,)\n",
    "\n",
    "    def __str__(self):\n",
    "        return \"User #{}\".format(self.user_id)\n",
    "\n",
    "    @staticmethod\n",
    "    def observation_space():\n",
    "        return spaces.Discrete(DOC_NUM)\n",
    "\n",
    "    def score_document(self, doc_obs):\n",
    "        return W[self.current, doc_obs[0]]\n",
    "\n",
    "\n",
    "class StaticUserSampler(user.AbstractUserSampler):\n",
    "    def __init__(self, user_ctor=UserState):\n",
    "        super().__init__(user_ctor)\n",
    "        self.user_count = 0\n",
    "\n",
    "    def sample_user(self):\n",
    "        self.user_count += 1\n",
    "        sampled_user = self._user_ctor(\n",
    "            self.user_count, np.random.randint(DOC_NUM))\n",
    "        return sampled_user\n",
    "\n",
    "\n",
    "class Response(user.AbstractResponse):\n",
    "    def __init__(self, accept: bool = False):\n",
    "        self.accept = accept\n",
    "\n",
    "    def create_observation(self) -> tp.Tuple[int]:\n",
    "        return (int(self.accept),)\n",
    "\n",
    "    @classmethod\n",
    "    def response_space(cls):\n",
    "        return spaces.Discrete(2)\n",
    "\n",
    "\n",
    "class UserChoiceModel(AbstractChoiceModel):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self._score_no_click = P_EXIT_ACCEPTED\n",
    "\n",
    "    def score_documents(self, user_state, doc_obs):\n",
    "        if len(doc_obs) != 1:\n",
    "            raise ValueError(\n",
    "                \"Expecting single document, but got: {}\".format(doc_obs))\n",
    "        self._scores = np.array(\n",
    "            [user_state.score_document(doc) for doc in doc_obs])\n",
    "\n",
    "    def choose_item(self) -> tp.Optional[int]:\n",
    "        if np.random.random() < self.scores[0]:\n",
    "            return 0\n",
    "\n",
    "\n",
    "class UserModel(user.AbstractUserModel):\n",
    "    def __init__(self):\n",
    "        super().__init__(Response, StaticUserSampler(), 1)\n",
    "        self.choice_model = UserChoiceModel()\n",
    "\n",
    "    def simulate_response(self, slate_documents):\n",
    "        if len(slate_documents) != 1:\n",
    "            raise ValueError(\"Expecting single document, but got: {}\".format(\n",
    "                slate_documents))\n",
    "\n",
    "        responses = [self._response_model_ctor() for _ in slate_documents]\n",
    "\n",
    "        self.choice_model.score_documents(\n",
    "            self._user_state,\n",
    "            [doc.create_observation() for doc in slate_documents]\n",
    "        )\n",
    "        selected_index = self.choice_model.choose_item()\n",
    "\n",
    "        if selected_index is not None:\n",
    "            responses[selected_index].accept = True\n",
    "\n",
    "        return responses\n",
    "\n",
    "    def update_state(self, slate_documents, responses):\n",
    "        if len(slate_documents) != 1:\n",
    "            raise ValueError(\n",
    "                f\"Expecting single document, but got: {slate_documents}\"\n",
    "            )\n",
    "\n",
    "        response = responses[0]\n",
    "        doc = slate_documents[0]\n",
    "        if response.accept:\n",
    "            self._user_state.current = doc.doc_id()\n",
    "            self._user_state.active_session = bool(\n",
    "                np.random.binomial(1, 1 - P_EXIT_ACCEPTED))\n",
    "        else:\n",
    "            self._user_state.current = np.random.choice(DOC_NUM)\n",
    "            self._user_state.active_session = bool(\n",
    "                np.random.binomial(1, 1 - P_EXIT_NOT_ACCEPTED))\n",
    "\n",
    "    def is_terminal(self):\n",
    "        \"\"\"Returns a boolean indicating if the session is over.\"\"\"\n",
    "        return not self._user_state.active_session\n",
    "\n",
    "\n",
    "def clicked_reward(responses):\n",
    "    reward = 0.0\n",
    "    for response in responses:\n",
    "        if response.accept:\n",
    "            reward += 1\n",
    "    return reward"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After implementing enviroment's block, create the enviroment!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_env():\n",
    "    env = recsim_gym.RecSimGymEnv(\n",
    "        environment.Environment(\n",
    "            UserModel(), \n",
    "            DocumentSampler(), \n",
    "            DOC_NUM, \n",
    "            1, \n",
    "            resample_documents=False\n",
    "        ),\n",
    "        clicked_reward\n",
    "    )\n",
    "    return env"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next step: the bot. Our choice is Wolpertinger!\n",
    "\n",
    "<img src=\"https://upload.wikimedia.org/wikipedia/commons/2/28/Wolpertinger.jpg\" width=200 height=200>\n",
    "\n",
    "\n",
    "It's a policy architecture, that's built upon of the actor-critic framework. Look at the diagram below:\n",
    "\n",
    "<img src=\"https://github.com/Scitator/RL-intro/blob/master/2020/presets/wolpertinger_scheme.png?raw=true\" width=400 height=800>\n",
    "\n",
    "\n",
    "The actor is a simple NN, that generate embedding action vector based on current state."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from catalyst.contrib.nn import Normalize\n",
    "\n",
    "\n",
    "inner_fn = utils.get_optimal_inner_init(nn.ReLU)\n",
    "outer_fn = utils.outer_init\n",
    "\n",
    "\n",
    "class ActorModel(nn.Module):\n",
    "    def __init__(self, hidden=64, doc_num=10, doc_emb_size=4):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.actor = nn.Sequential(\n",
    "            nn.Linear(doc_num, hidden),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden, hidden),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "        self.head = nn.Sequential(\n",
    "            nn.Linear(hidden, doc_emb_size),\n",
    "            Normalize()\n",
    "        )\n",
    "        \n",
    "        self.actor.apply(inner_fn)\n",
    "        self.head.apply(outer_fn)\n",
    "        \n",
    "        self.doc_num = doc_num\n",
    "        self.doc_emb_size = doc_emb_size\n",
    "        \n",
    "    def forward(self, states):\n",
    "        return self.head(self.actor(states))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The critic model is more complicated. In our implementation, we need action embeddings. Our actions is a picking a document. So, we just need a embedding vector for each document. They can be trained as well as a critic model. And we have to implement choosing process by choosing top-k variants and calculate q-value on them. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CriticModel(nn.Module):\n",
    "    def __init__(self, hidden=64, doc_num=10, doc_emb_size=4):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.critic = nn.Sequential(\n",
    "            nn.Linear(doc_num + doc_emb_size, hidden),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden, hidden),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "        \n",
    "        self.head = nn.Linear(hidden, 1)\n",
    "        \n",
    "        self.critic.apply(inner_fn)\n",
    "        self.head.apply(outer_fn)\n",
    "        \n",
    "        self.doc_embs = nn.Sequential(\n",
    "            nn.Embedding(doc_num, doc_emb_size),\n",
    "            Normalize()\n",
    "        )\n",
    "        \n",
    "        self.doc_num = doc_num\n",
    "        self.doc_emb_size = doc_emb_size\n",
    "        \n",
    "    def _generate_input(self, states, proto_actions):\n",
    "        return torch.cat([states, proto_actions], 1)\n",
    "    \n",
    "    def forward(self, states, proto_actions):\n",
    "        inputs = self._generate_input(states, proto_actions)\n",
    "        return self.head(self.critic(inputs))\n",
    "    \n",
    "    def get_topk(self, states, proto_actions, top_k=1):\n",
    "        # Instead of kNN algorithm we can calculate distance across all of the objects.\n",
    "        dist = torch.cdist(proto_actions, self.doc_embs[0].weight)\n",
    "        indexes = torch.topk(dist, k=top_k, largest=False)[1]\n",
    "        return torch.cat([self.doc_embs(index).unsqueeze(0) for index in indexes]), indexes\n",
    "    \n",
    "    def get_best(self, states, proto_actions, top_k=1):\n",
    "        doc_embs, indexes = self.get_topk(states, proto_actions, top_k)\n",
    "        top_k = doc_embs.size(1)\n",
    "        best_values = torch.empty(states.size(0)).to(states.device)\n",
    "        best_indexes = torch.empty(states.size(0)).to(states.device)\n",
    "        for num, (state, actions, idx) in enumerate(zip(states, doc_embs, indexes)):\n",
    "            new_states = state.repeat(top_k, 1)\n",
    "            # for each pair of state and action we use critic to calculate values\n",
    "            values = self(new_states, actions)\n",
    "            best = values.max(0)[1].item()\n",
    "            best_values[num] = values[best]\n",
    "            best_indexes[num] = idx[best]\n",
    "        return best_indexes, best_values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next code is usual for a RL training pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Uta8S6jKBiSU"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from collections import deque, namedtuple\n",
    "\n",
    "Transition = namedtuple(\n",
    "    'Transition', \n",
    "    field_names=[\n",
    "        'state', \n",
    "        'action', \n",
    "        'reward',\n",
    "        'done', \n",
    "        'next_state'\n",
    "    ]\n",
    ")\n",
    "\n",
    "class ReplayBuffer:\n",
    "    def __init__(self, capacity: int):\n",
    "        self.buffer = deque(maxlen=capacity)\n",
    "    \n",
    "    def append(self, transition: Transition):\n",
    "        self.buffer.append(transition)\n",
    "    \n",
    "    def sample(self, batch_size: int):\n",
    "        indices = np.random.choice(\n",
    "            len(self.buffer), \n",
    "            batch_size, \n",
    "            replace=batch_size > len(self.buffer)\n",
    "        )\n",
    "        states, actions, rewards, dones, next_states = \\\n",
    "            zip(*[self.buffer[idx] for idx in indices])\n",
    "        return (\n",
    "            np.array(states, dtype=np.float32), \n",
    "            np.array(actions, dtype=np.int64), \n",
    "            np.array(rewards, dtype=np.float32),\n",
    "            np.array(dones, dtype=np.bool), \n",
    "            np.array(next_states, dtype=np.float32)\n",
    "        )\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.buffer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9u81h1z5BiSU"
   },
   "outputs": [],
   "source": [
    "from torch.utils.data.dataset import IterableDataset\n",
    "\n",
    "\n",
    "class ReplayDataset(IterableDataset):\n",
    "\n",
    "    def __init__(self, buffer: ReplayBuffer, epoch_size: int = int(1e3)):\n",
    "        self.buffer = buffer\n",
    "        self.epoch_size = epoch_size\n",
    "\n",
    "    def __iter__(self):\n",
    "        states, actions, rewards, dones, next_states = \\\n",
    "            self.buffer.sample(self.epoch_size)\n",
    "        for i in range(len(dones)):\n",
    "            yield states[i], actions[i], rewards[i], dones[i], next_states[i]\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.epoch_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "iUckVz-yBiSU"
   },
   "outputs": [],
   "source": [
    "def extract_state(env, state):\n",
    "    user_space = env.observation_space.spaces[\"user\"]\n",
    "    return spaces.flatten(user_space, state[\"user\"])\n",
    "\n",
    "def get_action(env, actor, critic, state, top_k=10, epsilon=None):\n",
    "    # Our framework is created by PG process and it must be trained with \n",
    "    # a noise added to the actor's output.\n",
    "    # But in our framework it's better to sample action from the enviroment.\n",
    "    state = torch.tensor(state, dtype=torch.float32).to(device).unsqueeze(0)\n",
    "    if epsilon is None or random.random() < epsilon:\n",
    "        proto_action = actor(state)\n",
    "        action = critic.get_best(state, proto_action, top_k)[0]\n",
    "        action = action.detach().cpu().numpy().astype(int)\n",
    "    else:\n",
    "        action = env.action_space.sample()\n",
    "    return action\n",
    "\n",
    "\n",
    "def generate_session(\n",
    "    env, \n",
    "    actor,\n",
    "    critic,\n",
    "    replay_buffer=None,\n",
    "    epsilon=None,\n",
    "    top_k=10\n",
    "):\n",
    "    total_reward = 0\n",
    "    s = env.reset()\n",
    "    s = extract_state(env, s)\n",
    "\n",
    "    for t in range(1000):\n",
    "        a = get_action(env, actor, critic, epsilon=epsilon, state=s, top_k=top_k)\n",
    "        next_s, r, done, _ = env.step(a)\n",
    "        next_s = extract_state(env, next_s)\n",
    "\n",
    "        if replay_buffer is not None:\n",
    "            transition = Transition(s, a, r, done, next_s)\n",
    "            replay_buffer.append(transition)\n",
    "\n",
    "        total_reward += r\n",
    "        s = next_s\n",
    "        if done:\n",
    "            break\n",
    "\n",
    "    return total_reward\n",
    "\n",
    "def generate_sessions(\n",
    "    env, \n",
    "    actor,\n",
    "    critic,\n",
    "    replay_buffer=None,\n",
    "    num_sessions=100,\n",
    "    epsilon=None,\n",
    "    top_k=10\n",
    "):\n",
    "    sessions_reward = 0\n",
    "    for i_episone in range(num_sessions):\n",
    "        reward = generate_session(\n",
    "            env=env, \n",
    "            actor=actor,\n",
    "            critic=critic,\n",
    "            epsilon=epsilon,\n",
    "            replay_buffer=replay_buffer,\n",
    "            top_k=top_k\n",
    "        )\n",
    "        sessions_reward += reward\n",
    "    sessions_reward /= num_sessions\n",
    "    return sessions_reward\n",
    "\n",
    "def soft_update(target, source, tau):\n",
    "    \"\"\"Updates the target data with smoothing by ``tau``\"\"\"\n",
    "    for target_param, param in zip(target.parameters(), source.parameters()):\n",
    "        target_param.data.copy_(\n",
    "            target_param.data * (1.0 - tau) + param.data * tau\n",
    "        )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from catalyst import dl\n",
    "\n",
    "\n",
    "# It's a standart GameCallback!\n",
    "class RecSimCallback(dl.Callback):\n",
    "    def __init__(self, order=0, session_period=1):\n",
    "        super().__init__(order=0)\n",
    "        self.session_period = session_period\n",
    "        \n",
    "    def on_stage_start(self, runner: dl.IRunner):\n",
    "        generate_sessions(\n",
    "            env=runner.env, \n",
    "            actor=runner.model[\"origin_actor\"],\n",
    "            critic=runner.model[\"origin_critic\"],\n",
    "            replay_buffer=runner.replay_buffer,\n",
    "            top_k=runner.k,\n",
    "            epsilon=runner.epsilon,\n",
    "        )\n",
    "        \n",
    "    def on_batch_end(self, runner: dl.IRunner):\n",
    "        if runner.global_batch_step % self.session_period == 0:\n",
    "            session_reward = generate_session(\n",
    "                env=runner.env, \n",
    "                actor=runner.model[\"origin_actor\"],\n",
    "                critic=runner.model[\"origin_critic\"],\n",
    "                replay_buffer=runner.replay_buffer,\n",
    "                top_k=runner.k,\n",
    "                epsilon=runner.epsilon,\n",
    "            )\n",
    "            runner.batch_metrics.update({\"s_reward\": session_reward})\n",
    "            \n",
    "    def on_epoch_end(self, runner: dl.IRunner):\n",
    "        valid_reward = generate_sessions(\n",
    "            env=runner.env, \n",
    "            actor=runner.model[\"origin_actor\"],\n",
    "            critic=runner.model[\"origin_critic\"],\n",
    "            top_k=runner.k,\n",
    "            epsilon=None\n",
    "        )\n",
    "        runner.epoch_metrics[\"train_v_reward\"] = valid_reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_S0oeLknBiSU"
   },
   "outputs": [],
   "source": [
    "class CustomRunner(dl.Runner):\n",
    "    \n",
    "    def __init__(self, *, env, replay_buffer, gamma, tau, epsilon=0.2, tau_period=1, k=5, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.env = env\n",
    "        self.replay_buffer = replay_buffer\n",
    "        self.gamma = gamma\n",
    "        self.tau = tau\n",
    "        self.tau_period = tau_period\n",
    "        self.epsilon = epsilon\n",
    "        self.k = k\n",
    "    \n",
    "    def on_stage_start(self, runner: dl.IRunner):\n",
    "        super().on_stage_start(runner)\n",
    "        soft_update(self.model[\"origin_actor\"], self.model[\"target_actor\"], 1.0)\n",
    "        soft_update(self.model[\"origin_critic\"], self.model[\"target_critic\"], 1.0)\n",
    "\n",
    "    def _handle_batch(self, batch):\n",
    "        # model train/valid step\n",
    "        states, actions, rewards, dones, next_states = batch\n",
    "        \n",
    "        proto_actions = self.model[\"origin_actor\"](states)\n",
    "        policy_loss = (-self.model[\"origin_critic\"](states, proto_actions)).mean()\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            target_proto_actions = self.model[\"target_actor\"](next_states)\n",
    "            target_values = self.model[\"target_critic\"].get_best(next_states, target_proto_actions, self.k)[1].detach()\n",
    "\n",
    "        dones = dones * 1.0\n",
    "        expected_values = target_values * self.gamma * (1 - dones) + rewards\n",
    "        actions = self.model[\"origin_critic\"].doc_embs(actions.squeeze())\n",
    "        values = self.model[\"origin_critic\"](states, actions).squeeze()\n",
    "        \n",
    "        value_loss = self.criterion(\n",
    "            values,\n",
    "            expected_values\n",
    "        )\n",
    "        \n",
    "        self.batch_metrics.update(\n",
    "            {\n",
    "                \"critic_loss\": value_loss, \n",
    "                \"actor_loss\": policy_loss,\n",
    "            }\n",
    "        )\n",
    "\n",
    "        if self.is_train_loader:\n",
    "            self.optimizer[\"actor\"].zero_grad()\n",
    "            policy_loss.backward()\n",
    "            self.optimizer[\"actor\"].step()\n",
    "            \n",
    "            self.optimizer[\"critic\"].zero_grad()\n",
    "            value_loss.backward()\n",
    "            self.optimizer[\"critic\"].step()\n",
    "            \n",
    "            if self.global_batch_step % self.tau_period == 0:\n",
    "                soft_update(self.model[\"target_critic\"], self.model[\"origin_critic\"], self.tau)\n",
    "                soft_update(self.model[\"target_actor\"], self.model[\"origin_actor\"], self.tau)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's train our model and check the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "SaeB5fi2BiSU",
    "outputId": "27c01282-9f39-4444-db26-040c7f717a09",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "utils.set_global_seed(42)\n",
    "\n",
    "env = make_env()\n",
    "replay_buffer = ReplayBuffer(int(1e5))\n",
    "gamma = 0.99\n",
    "tau = 0.001\n",
    "tau_period = 1\n",
    "session_period = 1\n",
    "epoch_size = int(1e4)\n",
    "\n",
    "\n",
    "models = {\n",
    "    \"origin_actor\": ActorModel(doc_num=DOC_NUM, doc_emb_size=EMB_SIZE),\n",
    "    \"origin_critic\": CriticModel(doc_num=DOC_NUM, doc_emb_size=EMB_SIZE),\n",
    "    \"target_actor\": ActorModel(doc_num=DOC_NUM, doc_emb_size=EMB_SIZE),\n",
    "    \"target_critic\": CriticModel(doc_num=DOC_NUM, doc_emb_size=EMB_SIZE),\n",
    "}\n",
    "models[\"origin_critic\"].doc_embs.weight.copy_(models[\"target_critic\"].doc_embs)\n",
    "\n",
    "utils.set_requires_grad(models[\"target_actor\"], requires_grad=False)\n",
    "utils.set_requires_grad(models[\"target_critic\"], requires_grad=False)\n",
    "\n",
    "criterion = torch.nn.MSELoss()\n",
    "optimizer = {\n",
    "    \"actor\": torch.optim.Adam(models[\"origin_actor\"].parameters(), lr=1e-3),\n",
    "    \"critic\": torch.optim.Adam(models[\"origin_critic\"].parameters(), lr=1e-3),\n",
    "}\n",
    "\n",
    "loaders = {\n",
    "    \"train\": DataLoader(\n",
    "        ReplayDataset(replay_buffer, epoch_size=epoch_size), \n",
    "        batch_size=32,\n",
    "    ),\n",
    "}\n",
    "\n",
    "\n",
    "runner = CustomRunner(\n",
    "    env=env, \n",
    "    replay_buffer=replay_buffer, \n",
    "    gamma=gamma, \n",
    "    tau=tau,\n",
    "    tau_period=tau_period,\n",
    "    device=device,\n",
    ")\n",
    "\n",
    "runner.train(\n",
    "    model=models,\n",
    "    criterion=criterion,\n",
    "    optimizer=optimizer,\n",
    "    loaders=loaders,\n",
    "    logdir=\"./logs_rl\",\n",
    "    num_epochs=20,\n",
    "    verbose=False,\n",
    "    main_metric=\"v_reward\",\n",
    "    minimize_metric=False,\n",
    "    callbacks=[RecSimCallback(order=0, session_period=session_period)]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In our case, we can compare RL bot results with the optimal recommender agent. The agent can be built by the relation matrix `W`. We need to chose an index with the maximum value in the column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from recsim.agent import AbstractEpisodicRecommenderAgent\n",
    "\n",
    "class OptimalRecommender(AbstractEpisodicRecommenderAgent):\n",
    "\n",
    "    def __init__(self, environment, W):\n",
    "        super().__init__(environment.action_space)\n",
    "        self._observation_space = environment.observation_space\n",
    "        self._W = W\n",
    "\n",
    "    def step(self, reward, observation):\n",
    "        return [self._W[observation[\"user\"], :].argmax()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_agent(\n",
    "    env, \n",
    "    agent, \n",
    "    num_steps: int = int(1e4), \n",
    "    log_every: int = int(1e3)\n",
    "):\n",
    "    reward_history = []\n",
    "    step, episode = 1, 1\n",
    "\n",
    "    observation = env.reset()\n",
    "    while step < num_steps:\n",
    "        action = agent.begin_episode(observation)\n",
    "        episode_reward = 0\n",
    "        while True:\n",
    "            observation, reward, done, info = env.step(action)\n",
    "            episode_reward += reward\n",
    "\n",
    "            if step % log_every == 0:\n",
    "                print(step, np.mean(reward_history[-50:]))\n",
    "            step += 1\n",
    "            if done:\n",
    "                break\n",
    "            else:\n",
    "                action = agent.step(reward, observation)\n",
    "\n",
    "        agent.end_episode(reward, observation)\n",
    "        reward_history.append(episode_reward)\n",
    "\n",
    "    return reward_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = make_env()\n",
    "agent = OptimalRecommender(env, W)\n",
    "\n",
    "reward_history = run_agent(env, agent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Catalyst.RL.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}