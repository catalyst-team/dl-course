{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "name": "classification_baseline.ipynb",
   "provenance": [],
   "private_outputs": true,
   "toc_visible": true
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  },
  "accelerator": "GPU"
 },
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cWGO2uFHpubx",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Classification task\n",
    "\n",
    "Hi! It's a classification task baseline notebook.\n",
    "It include a data reader, baseline model and submission generator.\n",
    "\n",
    "You should use GPU to train your model, so we recommend using [Kaggle Notebooks](https://www.kaggle.com/docs/notebooks).\n",
    "To get maximum score of the task, your model should have accuracy greater than `0.8`.\n",
    "\n",
    "You can use everything, that suits into the rules in `README.md`."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "id": "9LjO1j6ERPgw"
   },
   "source": [
    "!pip install -Uq catalyst==20.12 albumentations"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "Sh__96Cqp1mo"
   },
   "source": [
    "# For Colab user: download dataset and upload zip files.\n",
    "# If you use Kaggle Notebooks, you already have the dataset in a hard drive.\n",
    "\n",
    "\n",
    "# !gdown https://drive.google.com/uc?id=1xD8Qx33LeefTXe_KNzERM3TdA4r-UZkA&export=download\n",
    "# !gdown https://drive.google.com/uc?id=1WWiuL8sXlMoBnpbkbqv3tDFgR1Rk_nPE&export=download\n",
    "# !unzip train.zip -d train\n",
    "# !unzip test.zip -d test"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "eHtKdA2dqGEk"
   },
   "source": [
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils import data\n",
    "\n",
    "import catalyst\n",
    "from catalyst import dl\n",
    "from catalyst.utils import metrics"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "v949FHgsQy62"
   },
   "source": [
    "from catalyst.utils import set_global_seed, get_device\n",
    "\n",
    "\n",
    "set_global_seed(42)\n",
    "device = get_device()"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BgU_DOniQR1p",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Dataset\n",
    "\n",
    "This code will help you to generate dataset. If your data have the following folder structure:\n",
    "\n",
    "```\n",
    "dataset/\n",
    "    class_1/\n",
    "        *.ext\n",
    "        ...\n",
    "    class_2/\n",
    "        *.ext\n",
    "        ...\n",
    "    ...\n",
    "    class_N/\n",
    "        *.ext\n",
    "        ...\n",
    "```\n",
    "First of all `create_dataset` function goes through a given directory and creates a dictionary `Dict[class_name, List[image]]`.\n",
    "Then `create_dataframe` function creates typical `pandas.DataFrame` for further analysis.\n",
    "After that, `prepare_dataset_labeling` creates a numerical label for each unique class name.\n",
    "Finally, to add a column with a numerical label value to the DataFrame, we can use `map_dataframe` function.\n",
    "\n",
    "Additionaly let's save the `class_names` for further usage."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "id": "rFIDQmEORPh1"
   },
   "source": [
    "from catalyst.utils import (\n",
    "    create_dataset, create_dataframe, get_dataset_labeling, map_dataframe\n",
    ")\n",
    "\n",
    "\n",
    "dataset = create_dataset(dirs=f\"train/*\", extension=\"*.jpg\")\n",
    "df = create_dataframe(dataset, columns=[\"class\", \"filepath\"])\n",
    "\n",
    "tag_to_label = get_dataset_labeling(df, \"class\")\n",
    "class_names = [\n",
    "    name for name, id_ in sorted(tag_to_label.items(), key=lambda x: x[1])\n",
    "]\n",
    "\n",
    "df_with_labels = map_dataframe(\n",
    "    df, \n",
    "    tag_column=\"class\", \n",
    "    class_column=\"label\", \n",
    "    tag2class=tag_to_label, \n",
    "    verbose=False\n",
    ")\n",
    "df_with_labels.head()"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    },
    "id": "PnFtDBQjRPiD"
   },
   "source": [
    "And you should split data in `train / valid / test` parts.\n",
    "There are only `train` and `valid` parts, so you must load test data as shows in a code cell."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "sEhBS2sqQZ1z"
   },
   "source": [
    "from catalyst.utils import split_dataframe_train_test\n",
    "\n",
    "\n",
    "train_data, valid_data = split_dataframe_train_test(\n",
    "    df_with_labels, test_size=0.2, random_state=42\n",
    ")\n",
    "train_data, valid_data = (\n",
    "    train_data.to_dict(\"records\"),\n",
    "    valid_data.to_dict(\"records\"),\n",
    ")"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "-1upnlv4RV2m"
   },
   "source": [
    "from catalyst.data.cv.reader import ImageReader\n",
    "from catalyst.dl import utils\n",
    "from catalyst.data import ScalarReader, ReaderCompose\n",
    "\n",
    "\n",
    "num_classes = len(tag_to_label)\n",
    "\n",
    "open_fn = ReaderCompose(\n",
    "    [\n",
    "        ImageReader(\n",
    "            input_key=\"filepath\", output_key=\"features\", rootpath=\"train\"\n",
    "        ),\n",
    "        ScalarReader(\n",
    "            input_key=\"label\",\n",
    "            output_key=\"targets\",\n",
    "            default_value=-1,\n",
    "            dtype=np.int64,\n",
    "        ),\n",
    "        ScalarReader(\n",
    "            input_key=\"label\",\n",
    "            output_key=\"targets_one_hot\",\n",
    "            default_value=-1,\n",
    "            dtype=np.int64,\n",
    "            one_hot_classes=num_classes,\n",
    "        ),\n",
    "    ]\n",
    ")"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    },
    "id": "fiFQL6w7RPib"
   },
   "source": [
    "## Augmentation\n",
    "\n",
    "Use some augmentations to generate more images for training process."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "G1f7StMdRV0S"
   },
   "source": [
    "import albumentations as albu\n",
    "from albumentations.pytorch import ToTensorV2 as ToTensor\n",
    "\n",
    "\n",
    "IMAGE_SIZE = 224\n",
    "\n",
    "train_transform = albu.Compose([\n",
    "    albu.HorizontalFlip(p=0.5),\n",
    "    albu.LongestMaxSize(IMAGE_SIZE),\n",
    "    albu.PadIfNeeded(IMAGE_SIZE, IMAGE_SIZE, border_mode=0),\n",
    "    albu.RandomResizedCrop(IMAGE_SIZE, IMAGE_SIZE, p=0.3),\n",
    "    albu.Normalize(),\n",
    "    ToTensor()\n",
    "])\n",
    "\n",
    "valid_transform = albu.Compose([\n",
    "    albu.LongestMaxSize(IMAGE_SIZE),\n",
    "    albu.PadIfNeeded(IMAGE_SIZE, IMAGE_SIZE, border_mode=0),\n",
    "    albu.Normalize(),\n",
    "    ToTensor()\n",
    "])\n"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "wJ1hwc8NRVxB"
   },
   "source": [
    "from catalyst.data import Augmentor\n",
    "\n",
    "\n",
    "train_data_transform = Augmentor(\n",
    "    dict_key=\"features\", augment_fn=lambda x: train_transform(image=x)[\"image\"]\n",
    ")\n",
    "\n",
    "valid_data_transform = Augmentor(\n",
    "    dict_key=\"features\", augment_fn=lambda x: valid_transform(image=x)[\"image\"]\n",
    ")\n"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "f-4HVfMiRuxM",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Don't forget to create test loader."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "id": "R5SAJcJvRPiz"
   },
   "source": [
    "batch_size = 256\n",
    "num_workers = 4\n",
    "\n",
    "train_loader = utils.get_loader(\n",
    "    train_data,\n",
    "    open_fn=open_fn,\n",
    "    dict_transform=train_data_transform,\n",
    "    batch_size=batch_size,\n",
    "    num_workers=num_workers,\n",
    "    shuffle=True,\n",
    "    sampler=None,\n",
    "    drop_last=True,\n",
    ")\n",
    "\n",
    "valid_loader = utils.get_loader(\n",
    "    valid_data,\n",
    "    open_fn=open_fn,\n",
    "    dict_transform=valid_data_transform,\n",
    "    batch_size=batch_size,\n",
    "    num_workers=num_workers,\n",
    "    shuffle=False, \n",
    "    sampler=None,\n",
    "    drop_last=True,\n",
    ")\n",
    "\n",
    "loaders = {\n",
    "    \"train\": train_loader,\n",
    "    \"valid\": valid_loader\n",
    "}"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    },
    "id": "wOki67UiRPjK"
   },
   "source": [
    "## Model\n",
    "\n",
    "For the baseline, we will use a ResNet model, we already have examined in the seminar.\n",
    "Enhance the model, use any* instruments or module as you like.\n",
    "\n",
    "*(Don't forget about the rules!)"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "OpOxdntTrAwP"
   },
   "source": [
    "class ResNetBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, stride, p=0.1):\n",
    "        super().__init__()\n",
    "\n",
    "        self.input = nn.Sequential(\n",
    "            nn.Conv2d(\n",
    "                in_channels,\n",
    "                out_channels,\n",
    "                kernel_size=3,\n",
    "                stride=stride,\n",
    "                padding=1,\n",
    "            ),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(\n",
    "                out_channels, out_channels, kernel_size=3, stride=1, padding=1\n",
    "            ),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "        )\n",
    "        self.res = nn.Conv2d(\n",
    "            in_channels, out_channels, kernel_size=1, stride=stride\n",
    "        )\n",
    "        self.output = nn.Sequential(nn.BatchNorm2d(out_channels), nn.ReLU())\n",
    "\n",
    "    def forward(self, x):\n",
    "        input = self.input(x)\n",
    "        res = self.res(x)\n",
    "        return self.output(res + input)\n",
    "\n",
    "\n",
    "class BaselineModel(nn.Module):\n",
    "    def __init__(self, channels=3, in_features=64, num_classes=10, p=0.1):\n",
    "        super().__init__()\n",
    "\n",
    "        self.input = nn.Sequential(\n",
    "            nn.Conv2d(\n",
    "                channels, in_features, kernel_size=7, stride=2, padding=3\n",
    "            ),\n",
    "            nn.BatchNorm2d(in_features),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=3, stride=2),\n",
    "        )\n",
    "\n",
    "        self.layer_0 = self._make_layer(in_features, 1)\n",
    "        self.layer_1 = self._make_layer(in_features)\n",
    "        in_features *= 2\n",
    "        self.layer_2 = self._make_layer(in_features)\n",
    "        in_features *= 2\n",
    "        self.layer_3 = self._make_layer(in_features)\n",
    "\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.AdaptiveAvgPool2d((1, 1)),\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(2 * in_features, num_classes),\n",
    "        )\n",
    "\n",
    "    def _make_layer(self, in_features, multiplier=2, p=0.1):\n",
    "        return nn.Sequential(\n",
    "            ResNetBlock(in_features, in_features * multiplier, stride=2, p=p),\n",
    "            ResNetBlock(\n",
    "                in_features * multiplier,\n",
    "                in_features * multiplier,\n",
    "                stride=1,\n",
    "                p=p,\n",
    "            ),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.input(x)\n",
    "        x = self.layer_0(x)\n",
    "        x = self.layer_1(x)\n",
    "        x = self.layer_2(x)\n",
    "        x = self.layer_3(x)\n",
    "        return self.fc(x)"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "B26FN-CLq4cF",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "source": [
    "from catalyst.dl import SupervisedRunner\n",
    "\n",
    "\n",
    "class ClassificationRunner(SupervisedRunner):\n",
    "    def predict_batch(self, batch):\n",
    "        prediction = {\n",
    "            \"filepath\": batch[\"filepath\"],\n",
    "            \"log_probs\": self.model(batch[self.input_key].to(self.device))\n",
    "        }\n",
    "        return prediction"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "HOfvcgRNrNWh"
   },
   "source": [
    "model = BaselineModel()\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "runner = ClassificationRunner(input_key=\"features\", input_target_key=\"targets\", device=device)\n",
    "runner.train(\n",
    "    model=model,\n",
    "    optimizer=optimizer,\n",
    "    criterion=criterion,\n",
    "    loaders=loaders,\n",
    "    logdir=Path(\"logs\") / datetime.now().strftime(\"%Y%m%d-%H%M%S\"),\n",
    "    num_epochs=10,\n",
    "    verbose=True,\n",
    "    load_best_on_end=True,\n",
    "    callbacks={\n",
    "        \"optimizer\": dl.OptimizerCallback(\n",
    "            metric_key=\"loss\", accumulation_steps=1, grad_clip_params=None,\n",
    "        ),\n",
    "        \"criterion\": dl.CriterionCallback(\n",
    "            input_key=\"targets\", output_key=\"logits\", prefix=\"loss\",\n",
    "        ),\n",
    "        \"accuracy\": dl.AccuracyCallback(num_classes=10),\n",
    "    },\n",
    ")"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    },
    "id": "d_bCKBjHRPjh"
   },
   "source": [
    "This code below will generate a submission.\n",
    "It reads images from `test` folder and gathers prediction from the trained model.\n",
    "Check your submission before uploading it into `Kaggle`."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "KgNpIL-uS5KB"
   },
   "source": [
    "import pandas as pd\n",
    "from PIL import Image\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "submission = {\"Id\": [], \"Category\": []}\n",
    "model.eval()\n",
    "\n",
    "test_dataset = create_dataset(dirs=f\"test\", extension=\"*.jpg\")\n",
    "test_data = list({\"filepath\": filepath} for filepath in test_dataset[\"test\"])\n",
    "\n",
    "test_open_fn = ReaderCompose(\n",
    "    [\n",
    "        ImageReader(\n",
    "            input_key=\"filepath\", output_key=\"features\", rootpath=\"\"\n",
    "        ),\n",
    "        ScalarReader(\n",
    "            input_key=\"filepath\",\n",
    "            output_key=\"filepath\",\n",
    "            default_value=\"\",\n",
    "            dtype=str,\n",
    "        ),\n",
    "    ]\n",
    ")\n",
    "\n",
    "test_loader = utils.get_loader(\n",
    "    test_data,\n",
    "    open_fn=test_open_fn,\n",
    "    dict_transform=valid_data_transform,\n",
    "    batch_size=batch_size,\n",
    "    num_workers=num_workers,\n",
    "    shuffle=False,\n",
    "    sampler=None,\n",
    "    drop_last=False,\n",
    ")\n",
    "\n",
    "for prediction in runner.predict_loader(loader=test_loader):\n",
    "    prediction[\"labels\"] = [class_names[c] for c in torch.max(prediction[\"log_probs\"], axis=1)[1]]\n",
    "    submission[\"Id\"].extend(f.split(\"/\")[1].split(\".\")[0] for f in prediction[\"filepath\"])\n",
    "    submission[\"Category\"].extend(prediction[\"labels\"])"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "k8XOQD15S5Gh"
   },
   "source": [
    "import pandas as pd\n",
    "\n",
    "\n",
    "pd.DataFrame(submission).to_csv(\"baseline.csv\", index=False)"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "vgXHVCS_TgrS"
   },
   "source": [
    ""
   ],
   "execution_count": null,
   "outputs": []
  }
 ]
}