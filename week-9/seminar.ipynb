{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip -q install catalyst==20.10.1 torchtext==0.7.0 youtokentome nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from catalyst.utils import set_global_seed, get_device\n",
    "\n",
    "set_global_seed(42)\n",
    "device = \"cuda:0\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "jgH3CAcubM4z"
   },
   "source": [
    "# Seminar\n",
    "\n",
    "Hi! Today we are going to learn a new tokenization algorithm, seq2seq metrics and a machine translation task. We will be acquainted with an attention mechanism."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BPE. YouTokenToMe\n",
    "\n",
    "Previously we have discussed a text preprocessing pipeline. We used `WordPunctTokenizer`, that tokenize text to words and punctuations. But this tokenization algorithm isn't perfect. Some languages have many word-forms. Many languages have words modification, like prefixes and suffixes. We want to save morphology information in text, but save every possible word-form isn't memory-efficient and isn't easy to train. However, we can create tokenziation mechanism, that will tokenize every word by subword morphology. And there is unsupervised algorithm to do it. It's called Byte Pair Encoding. How it works:\n",
    "\n",
    "![](https://lena-voita.github.io/resources/lectures/seq2seq/bpe/build_merge_table.gif)\n",
    "\n",
    "1. We split texts into characters\n",
    "2. Count bigrams on characters\n",
    "3. Merge the most popular pair\n",
    "4. Continue until we reach given vocabulary size.\n",
    "\n",
    "It's easy algorithm, and we have several implementations:\n",
    "- SentencePiece\n",
    "- fastBPE\n",
    "- Tokenizers by ü§ó\n",
    "- YouTokenToMe\n",
    "\n",
    "The fastes one is YouTokenToMe by VK Team. Let's look how it works:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Tuple\n",
    "\n",
    "import youtokentome as yttm\n",
    "from torchtext.experimental.vocab import Vocab\n",
    "from torchtext.experimental.datasets import WMT14\n",
    "from torchtext.utils import download_from_url, extract_archive"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Download WMT14 dataset. It have pair texts on English and German languages, processed by Google Brain."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "wmt_url = 'https://drive.google.com/uc?export=download&id=0B_bZck-ksdkpM25jRUN2X2UxMm8'\n",
    "dataset_tar = download_from_url(wmt_url, root=\"wmt14\")\n",
    "extracted = extract_archive(dataset_tar)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use `newtest2016` data us train part in training pipeline. Now we need to train BPE tokenizers for English and German languages. Consider vocabulary size as 10000 tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<youtokentome.youtokentome.BPE at 0x7f2d7ac61ef0>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "<youtokentome.youtokentome.BPE at 0x7f2d7ac61ef0>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data_en_path = \"wmt14/newstest2016.en\"\n",
    "tokenizer_en_path = \"en.tok\"\n",
    "\n",
    "yttm.BPE.train(\n",
    "    data=train_data_en_path, vocab_size=10000, model=tokenizer_en_path\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<youtokentome.youtokentome.BPE at 0x7f2d7a758588>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "<youtokentome.youtokentome.BPE at 0x7f2d7a758588>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data_de_path = \"wmt14/newstest2016.de\"\n",
    "tokenizer_de_path = \"de.tok\"\n",
    "\n",
    "yttm.BPE.train(\n",
    "    data=train_data_de_path, vocab_size=10000, model=tokenizer_de_path\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training procedure in `YTTM` run in a background. We need to load tokenizers to work with them:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer_en = yttm.BPE(model=tokenizer_en_path)\n",
    "tokenizer_de = yttm.BPE(model=tokenizer_de_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our text example will be:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_text = \"Tinkoff loves VK!\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Try to get tokens, ids, add special tokens:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['‚ñÅT', 'ink', 'off', '‚ñÅloves', '‚ñÅV', 'K', '!']]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "[['‚ñÅT', 'ink', 'off', '‚ñÅloves', '‚ñÅV', 'K', '!']]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer_en.encode([test_text], output_type=yttm.OutputType.SUBWORD)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[146, 548, 2115, 8624, 510, 63, 86]]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "[[146, 548, 2115, 8624, 510, 63, 86]]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer_en.encode([test_text], output_type=yttm.OutputType.ID)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['<BOS>', '‚ñÅT', 'ink', 'off', '‚ñÅloves', '‚ñÅV', 'K', '!', '<EOS>']]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "[['<BOS>', '‚ñÅT', 'ink', 'off', '‚ñÅloves', '‚ñÅV', 'K', '!', '<EOS>']]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer_en.encode(\n",
    "    [test_text], output_type=yttm.OutputType.SUBWORD, bos=True, eos=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[2, 146, 548, 2115, 8624, 510, 63, 86, 3]]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "[[2, 146, 548, 2115, 8624, 510, 63, 86, 3]]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer_en.encode(\n",
    "    [test_text], output_type=yttm.OutputType.ID, bos=True, eos=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To join `YTTM` tokenizer and `TorchText` dataset abstraction we need to code couple functions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code them\n",
    "\n",
    "def tokenize_de(text: str) -> List[str]:\n",
    "    return tokenizer_de.encode(\n",
    "        [text], \n",
    "        output_type=yttm.OutputType.SUBWORD, \n",
    "        bos=True, \n",
    "        eos=True\n",
    "    )[0]\n",
    "\n",
    "\n",
    "def tokenize_en(text: str) -> List[str]:\n",
    "    return tokenizer_en.encode(\n",
    "        [text], \n",
    "        output_type=yttm.OutputType.SUBWORD, \n",
    "        bos=True, \n",
    "        eos=True\n",
    "    )[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2478lines [00:00, 159289.57lines/s]\n",
      "0lines [00:00, ?lines/s]\n",
      "2478lines [00:00, 199517.89lines/s]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "(train_dataset, valid_dataset, test_dataset) = WMT14(\n",
    "    train_filenames=(\"newstest2016.en\", \"newstest2016.de\"),\n",
    "    valid_filenames=(\"newstest2010.en\", \"newstest2010.de\"),\n",
    "    test_filenames=(\"newstest2009.en\", \"newstest2009.de\"),\n",
    "    tokenizer=(tokenize_en, tokenize_de),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check how `dataset` works:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([   3, 1131, 4352, 2207,    4]),\n",
       " tensor([   2, 1931, 1624, 7273, 2396,    3]))"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "(tensor([   3, 1131, 4352, 2207,    4]),\n",
       " tensor([   2, 1931, 1624, 7273, 2396,    3]))"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<BOS>‚ñÅObama‚ñÅreceives‚ñÅNetanyahu<EOS>'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "'<BOS>‚ñÅObama‚ñÅreceives‚ñÅNetanyahu<EOS>'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens = [train_dataset.get_vocab()[0].itos[i] for i in train_dataset[0][0]]\n",
    "\"\".join(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<BOS>‚ñÅObama‚ñÅempf√§ngt‚ñÅNetanyahu<EOS>'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "'<BOS>‚ñÅObama‚ñÅempf√§ngt‚ñÅNetanyahu<EOS>'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens = [train_dataset.get_vocab()[1].itos[i] for i in train_dataset[0][1]]\n",
    "\"\".join(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's code special function to decode input ids into human-readable text:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# code function to decode input ids to pretty output\n",
    "\n",
    "def decoding(input_ids: torch.Tensor, vocab: Vocab) -> str:\n",
    "    result_text = \"\"\n",
    "    for input_id in input_ids:\n",
    "        if input_id == vocab.stoi[\"<EOS>\"]:\n",
    "            break\n",
    "        elif input_id != vocab.stoi[\"<BOS>\"]:\n",
    "            result_text += vocab.itos[input_id]\n",
    "    return \"\".join(t if t != \"‚ñÅ\" else \" \" for t in result_text )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' Obama receives Netanyahu'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "' Obama receives Netanyahu'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decoding(train_dataset[0][0], train_dataset.get_vocab()[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' Obama empf√§ngt Netanyahu'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "' Obama empf√§ngt Netanyahu'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decoding(train_dataset[0][1], train_dataset.get_vocab()[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to code padding code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "PAD_ID_src = train_dataset.get_vocab()[0].stoi[\"<PAD>\"]\n",
    "PAD_ID_trg = train_dataset.get_vocab()[1].stoi[\"<PAD>\"]\n",
    "max_length = 64 # 128\n",
    "\n",
    "def collate_fn(batch: Tuple[torch.Tensor]) -> Tuple[torch.Tensor]:\n",
    "    max_len_src = min(max(b[0].size(0) for b in batch), max_length)\n",
    "    max_len_trg = min(max(b[1].size(0) for b in batch), max_length)\n",
    "    all_src = torch.zeros(max_len_src, len(batch)) + PAD_ID_src\n",
    "    all_trg = torch.zeros(max_len_trg, len(batch)) + PAD_ID_trg\n",
    "    \n",
    "    for num, (src, trg) in enumerate(batch):\n",
    "        all_src[:src.size(0), num] = src[:max_length]\n",
    "        all_trg[:trg.size(0), num] = trg[:max_length]\n",
    "    return all_src.type(torch.LongTensor), all_trg.type(torch.LongTensor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And bucketing sampler! It's special sampler, that will reduce padding in batches. We need to sort our text by lens in tokens, and form batches using text order. We'll implement this by `SortedSampler` and `RandomSubsetSampler`:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Any, Callable, Iterable\n",
    "\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data.sampler import Sampler\n",
    "\n",
    "\n",
    "class SortedSampler(Sampler):\n",
    "    def __init__(self, data: Dataset, sort_key: Callable[[Any], Any] = lambda x: x):\n",
    "        super().__init__(data)\n",
    "        self.data = data\n",
    "        self.sort_key = sort_key\n",
    "        zip_ = [(i, self.sort_key(row)) for i, row in enumerate(self.data)]\n",
    "        zip_ = sorted(zip_, key=lambda r: r[1])\n",
    "        self.sorted_indexes = [item[0] for item in zip_]\n",
    "\n",
    "    def __iter__(self) -> Iterable[int]:\n",
    "        return iter(self.sorted_indexes)\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return len(self.data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`BucketBatchSampler`'s algorithm is this:\n",
    "\n",
    "- Create buckets, subsets on random order.\n",
    "- Sort data in each bucket\n",
    "- Generate sample by getting items from buckets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "from typing import Generator, List\n",
    "\n",
    "from torch.utils.data.sampler import BatchSampler\n",
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "\n",
    "\n",
    "class BucketBatchSampler(BatchSampler):\n",
    "    def __init__(\n",
    "        self,\n",
    "        sampler: Sampler,\n",
    "        batch_size: int,\n",
    "        drop_last: bool,\n",
    "        sort_key: Callable[[Any], Any] = lambda x: x,\n",
    "        bucket_size_multiplier: int = 100\n",
    "    ):\n",
    "        super().__init__(sampler, batch_size, drop_last)\n",
    "        self.sort_key = sort_key\n",
    "        self.bucket_sampler = BatchSampler(\n",
    "            sampler,\n",
    "            min(batch_size * bucket_size_multiplier, len(sampler)),\n",
    "            False\n",
    "        )\n",
    "\n",
    "    def __iter__(self) -> Generator[List[int], None, None]:\n",
    "        for bucket in self.bucket_sampler:\n",
    "            sorted_sampler = SortedSampler(bucket, self.sort_key)\n",
    "            for batch in SubsetRandomSampler(\n",
    "                list(\n",
    "                    BatchSampler(\n",
    "                        sorted_sampler, \n",
    "                        self.batch_size, \n",
    "                        self.drop_last\n",
    "                    )\n",
    "                )\n",
    "            ):\n",
    "                yield [bucket[i] for i in batch]\n",
    "\n",
    "    def __len__(self):\n",
    "        if self.drop_last:\n",
    "            return len(self.sampler) // self.batch_size\n",
    "        else:\n",
    "            return math.ceil(len(self.sampler) / self.batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And now we just need to create data loaders:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data.sampler import RandomSampler\n",
    "\n",
    "\n",
    "batch_size = 128\n",
    "\n",
    "\n",
    "train_sampler = RandomSampler(train_dataset)\n",
    "sort_key = lambda row: len(train_dataset[row][0])\n",
    "train_batch_sampler = BucketBatchSampler(\n",
    "    train_sampler, \n",
    "    batch_size=batch_size,\n",
    "    drop_last=True,\n",
    "    sort_key=sort_key\n",
    ")\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_sampler=train_batch_sampler,\n",
    "    collate_fn=collate_fn\n",
    ")\n",
    "valid_loader = DataLoader(\n",
    "    valid_dataset,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=False,\n",
    "    drop_last=False,\n",
    "    collate_fn=collate_fn\n",
    ")\n",
    "test_loader = DataLoader(\n",
    "    test_dataset,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=False,\n",
    "    drop_last=False,\n",
    "    collate_fn=collate_fn\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BLEU\n",
    "\n",
    "In this section we will discuss metrics for Seq2Seq models. There are several metrics: BLEU, ROUGE, METEOR, WER, etc. They used to understand how well model solve any task with generating texts with target text. Let's look on to BLEU.\n",
    "\n",
    "BLEU stands for \"BIlingual Evaluation Understudy\". To compute it, we need n-grams for predicted text(hypothesis) and target text(references) and compare them. And BLEU would be a number of n-grams from predicted text, appears in target text. Let's look at the example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_target = \"Die Prager B√∂rse st√ºrzt gegen Gesch√§ftsschluss ins Minus\"\n",
    "test_predicted = \"Das Prager B√∂rse st√ºrzt gegest Gesch√§ftschlus uns Minus\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_tokens = test_target.split() # Simple way to get tokens\n",
    "\n",
    "unigrams_target = [(t_0,) for t_0 in target_tokens]\n",
    "bigrams_target = [\n",
    "    (t_0, t_1) for t_0, t_1 in zip(target_tokens[:-1], target_tokens[1:])\n",
    "]\n",
    "trigrams_target = [\n",
    "    (t_0, t_1, t_2)\n",
    "    for t_0, t_1, t_2 in zip(\n",
    "        target_tokens[:-2], target_tokens[1:-1], target_tokens[2:]\n",
    "    )\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_tokens = test_predicted.split()\n",
    "\n",
    "\n",
    "# find ngrams for predicted text\n",
    "unigrams_predicted = [(t_0,) for t_0 in predicted_tokens]\n",
    "bigrams_predicted = [\n",
    "    (t_0, t_1) for t_0, t_1 in zip(predicted_tokens[:-1], predicted_tokens[1:])\n",
    "]\n",
    "trigrams_predicted = [\n",
    "    (t_0, t_1, t_2)\n",
    "    for t_0, t_1, t_2 in zip(\n",
    "        predicted_tokens[:-2], predicted_tokens[1:-1], predicted_tokens[2:]\n",
    "    )\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Count number of n-grams appeard in target text:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Uni: 0.5\n",
      "Bi: 0.2857142857142857\n",
      "Tri: 0.16666666666666666\n",
      "Uni: 0.5\n",
      "Bi: 0.2857142857142857\n",
      "Tri: 0.16666666666666666\n"
     ]
    }
   ],
   "source": [
    "count_unigrams = sum(\n",
    "    uni in unigrams_target for uni in unigrams_predicted\n",
    ") / len(unigrams_predicted)\n",
    "\n",
    "# Count statistic for bigrams and trigrams\n",
    "count_bigrams = sum(\n",
    "    bi in bigrams_target for bi in bigrams_predicted\n",
    ") / len(bigrams_predicted)\n",
    "count_trigrams = sum(\n",
    "    tri in trigrams_target for tri in trigrams_predicted\n",
    ") / len(trigrams_predicted)\n",
    "print(f\"Uni: {count_unigrams}\\nBi: {count_bigrams}\\nTri: {count_trigrams}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Our BLEU: 0.31746031746031744\n",
      "Our BLEU: 0.31746031746031744\n"
     ]
    }
   ],
   "source": [
    "bleu = (count_unigrams + count_bigrams + count_trigrams) / 3\n",
    "print(f\"Our BLEU: {bleu}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We don't need to implement BLEU score from scratch. In `nltk` we have algorithms to calculate it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.translate.bleu_score import corpus_bleu\n",
    "\n",
    "\n",
    "def compute_bleu(predicted, target):\n",
    "    return corpus_bleu([[ref] for ref in target], predicted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8012748472980956"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "0.8012748472980956"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "compute_bleu([test_predicted], [test_target])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Seq2Seq. Translation\n",
    "\n",
    "Translation is one of the task, where we need to have Seq2Seq models, that consist of an Encoder and a Decoder. An encoder should return an informative vector, that will represent an input text. A decoder should generate translation, based on the vector. We will use a Recurrent Neural Network with additional component called attention.\n",
    "\n",
    "### RNN + Attention \n",
    "\n",
    "There are two famous attention formulation in the nlp. One of them is [by Luong](https://arxiv.org/pdf/1508.04025.pdf). Another one is [by Bahdanau](https://arxiv.org/pdf/1409.0473.pdf). We will implement an aproximation of Luong attention, that can be showed like this:\n",
    "\n",
    "![](https://lena-voita.github.io/resources/lectures/seq2seq/attention/luong_model-min.png)\n",
    "\n",
    "Let's code this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        vocab_size: int,\n",
    "        emb_size: int,\n",
    "        hidden_size: int,\n",
    "        num_layers: int,\n",
    "        dropout: float,\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        self.vocab_size = vocab_size\n",
    "        self.emb_size = emb_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "\n",
    "        self.embedding = nn.Embedding(vocab_size, emb_size)\n",
    "\n",
    "        self.rnn = nn.LSTM(\n",
    "            emb_size, hidden_size, num_layers=num_layers, dropout=dropout\n",
    "        )\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(\n",
    "            self,\n",
    "            src: torch.Tensor\n",
    "    ) -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor]:\n",
    "        embedded = self.embedding(src)\n",
    "        embedded = self.dropout(embedded)\n",
    "        outputs, (hidden, cell) = self.rnn(embedded)\n",
    "        return outputs, hidden, cell\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Attention(nn.Module):\n",
    "    def __init__(self, hidden_size: int):\n",
    "        super().__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "\n",
    "        # Instead from one matrix we will use two linear modules\n",
    "        self.enc_linear = nn.Linear(hidden_size, hidden_size)\n",
    "        self.dec_linear = nn.Linear(hidden_size, hidden_size)\n",
    "\n",
    "    def forward(\n",
    "        self, last_hidden: torch.Tensor, encoder_outputs: torch.Tensor\n",
    "    ) -> torch.Tensor:\n",
    "        bs = last_hidden.size(1)\n",
    "        \n",
    "        # Prepare our examples\n",
    "        encoder_outputs = self.enc_linear(encoder_outputs).reshape(\n",
    "            bs, -1, self.hidden_size\n",
    "        )\n",
    "        last_hidden = self.dec_linear(last_hidden).reshape(\n",
    "            bs, self.hidden_size, 1\n",
    "        )\n",
    "\n",
    "        # Compute logits by batch matrix multiplication\n",
    "        logits = torch.bmm(encoder_outputs, last_hidden)\n",
    "\n",
    "        attn = torch.softmax(logits, 1).reshape(-1, bs, 1)\n",
    "        return attn\n",
    "        \n",
    "\n",
    "class DecoderAttn(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        vocab_size: int,\n",
    "        emb_size: int,\n",
    "        hidden_size: int,\n",
    "        num_layers: int,\n",
    "        attention: Attention,\n",
    "        dropout: float,\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        self.emb_size = emb_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.vocab_size = vocab_size\n",
    "        self.num_layers = num_layers\n",
    "\n",
    "        self.attn = attention\n",
    "\n",
    "        self.embedding = nn.Embedding(vocab_size, emb_size)\n",
    "\n",
    "        self.rnn = nn.LSTM(\n",
    "            emb_size, hidden_size, num_layers=num_layers, dropout=dropout\n",
    "        )\n",
    "        self.out = nn.Linear(hidden_size, vocab_size)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        input_: torch.Tensor,\n",
    "        hidden: torch.Tensor, # hidden_state from t-1\n",
    "        cell: torch.Tensor,\n",
    "        encoder_output: torch.Tensor,\n",
    "    ) -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor]:\n",
    "        embedded = self.embedding(input_)\n",
    "        embedded = self.dropout(embedded)\n",
    "\n",
    "        attn = self.attn(hidden[-1:], encoder_output)\n",
    "        # Generating new cell state by attention and encoder output\n",
    "        new_cell = (encoder_output * attn).sum(0)\n",
    "        cell[-1] = new_cell\n",
    "\n",
    "        output, (hidden, cell) = self.rnn(embedded, (hidden, cell))\n",
    "        prediction = self.out(output)\n",
    "\n",
    "        return prediction, hidden, cell"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One important point about training Seq2Seq models it's adding target tokens in a Decoder training loop. While our model is not good enough, it's generating \"trash\" tokens, that hasn't any information for generating. That's why we try to feed the decoder. However, it's not good too! The decoder will generate text via its generated tokens. Fopr this purpose we try to train the Decoder with random decited tokens source (target or itself)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "from random import random\n",
    "\n",
    "\n",
    "BOS_IDX = train_dataset.get_vocab()[1].stoi[\"<BOS>\"]\n",
    "\n",
    "class Seq2Seq(nn.Module):\n",
    "    def __init__(self, encoder: Encoder, decoder: DecoderAttn):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        self.max_len = max_length\n",
    "    \n",
    "    def forward(\n",
    "        self, \n",
    "        src: torch.Tensor, \n",
    "        trg: torch.Tensor, \n",
    "        teacher_forcing_ratio: float = 0.5\n",
    "    ) -> torch.Tensor :\n",
    "        batch_size = src.shape[1]\n",
    "        max_len = trg.shape[0]\n",
    "        trg_vocab_size = self.decoder.vocab_size\n",
    "        \n",
    "        outputs = torch.zeros(max_len, batch_size, trg_vocab_size).to(device)\n",
    "        \n",
    "        enc_out, hidden, cell = self.encoder(src)\n",
    "        \n",
    "        input_ = torch.zeros(1, batch_size) + BOS_IDX\n",
    "        input_ = input_.type(torch.LongTensor).to(device)\n",
    "        \n",
    "        for t in range(1, max_len):\n",
    "            output, hidden, cell =  self.decoder(input_, hidden, cell, enc_out)\n",
    "            outputs[t] = output\n",
    "            teacher_force = random() < teacher_forcing_ratio\n",
    "            top1 = output.max(2)[1]\n",
    "            input_ = (trg[t] if teacher_force else top1).reshape(1, -1)\n",
    "\n",
    "        return outputs[1:]\n",
    "    \n",
    "    def translate(self, src: torch.Tensor) -> torch.Tensor:\n",
    "        batch_size = src.shape[1]\n",
    "        outputs = torch.zeros(self.max_len, batch_size).to(device)\n",
    "\n",
    "        enc_out, hidden, cell = self.encoder(src)\n",
    "        \n",
    "        input_ = torch.zeros(1, batch_size) + BOS_IDX\n",
    "        input_ = input_.type(torch.LongTensor).to(device)\n",
    "        \n",
    "        for t in range(1, self.max_len):\n",
    "            output, hidden, cell = self.decoder(input_, hidden, cell, enc_out)\n",
    "            top1 = output.max(2)[1].reshape(-1)\n",
    "            outputs[t] = top1\n",
    "            input_ = top1.reshape(1, -1)\n",
    "        \n",
    "        return outputs[1:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a model, special runner for Seq2Seq models and train the model!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "source_vocab, target_vocab = train_dataset.get_vocab()\n",
    "\n",
    "input_size = len(source_vocab)\n",
    "output_size = len(target_vocab)\n",
    "src_emb_size = tgt_emb_size = 100\n",
    "hidden_size = 300\n",
    "num_layers =  2\n",
    "dropout_p = 0.1\n",
    "\n",
    "enc = Encoder(input_size, src_emb_size, hidden_size, num_layers, dropout_p)\n",
    "attention = Attention(hidden_size)\n",
    "dec = DecoderAttn(\n",
    "    output_size, tgt_emb_size, hidden_size, num_layers, attention, dropout_p\n",
    ")\n",
    "model = Seq2Seq(enc, dec).to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To train model, we will compare generated tokens with a target for each source. To compare, use `CrossEntropyLoss`!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn.utils import clip_grad_norm_\n",
    "\n",
    "\n",
    "from catalyst.dl import Runner\n",
    "\n",
    "\n",
    "class Seq2SeqRunner(Runner):\n",
    "    def __init__(\n",
    "        self, source_vocab: Vocab, target_vocab: Vocab, *args, **kwargs\n",
    "    ):\n",
    "        self.source_vocab = source_vocab\n",
    "        self.target_vocab = target_vocab\n",
    "        super().__init__(*args, **kwargs)\n",
    "\n",
    "    def predict_batch(self, batch: Tuple[torch.Tensor, torch.Tensor]) -> torch.Tensor:\n",
    "        predictions = self.model.translate(batch[0]).type(torch.LongTensor)\n",
    "        translations = [\n",
    "            decoding(sentence, self.target_vocab)\n",
    "            for sentence in predictions.t()\n",
    "        ]\n",
    "        return translations\n",
    "\n",
    "    def _handle_batch(self, batch: Tuple[torch.Tensor, torch.Tensor]) -> None:\n",
    "        batch_metrics = {}\n",
    "        source, target = batch\n",
    "\n",
    "        prediction = self.model(source, target)\n",
    "        \n",
    "        target = target[1:].reshape(-1)\n",
    "        prediction = prediction.reshape(target.size(0), -1)\n",
    "        \n",
    "        batch_metrics[\"loss\"] = self.criterion(prediction, target)\n",
    "        if self.is_train_loader:\n",
    "            self.optimizer.zero_grad()\n",
    "            batch_metrics[\"loss\"].backward()\n",
    "            clip_grad_norm_(self.model.parameters(), 1)\n",
    "            self.optimizer.step()\n",
    "            \n",
    "        self.batch_metrics.update(**batch_metrics)\n",
    "        self.input = {\"source\": source, \"target\": target}\n",
    "        self.output = {\"predicted\": prediction}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To calculate BLEU score in train loop, we need to code Callback for this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "from catalyst.dl import Callback, CallbackOrder\n",
    "\n",
    "\n",
    "class BLEUCallback(Callback):\n",
    "    def __init__(self):\n",
    "        super().__init__(CallbackOrder.Metric)\n",
    "\n",
    "    def on_batch_end(self, runner: Runner) -> None:\n",
    "        if runner.is_valid_loader:\n",
    "            source = runner.input[\"source\"]\n",
    "            predicted = runner.predict_batch((source,))\n",
    "            target = [\n",
    "                decoding(sentence, runner.source_vocab)\n",
    "                for sentence in source.t()\n",
    "            ]\n",
    "            bleu = compute_bleu(predicted, target)\n",
    "            runner.batch_metrics.update(**{\"bleu\": bleu})\n",
    "        else:\n",
    "            runner.batch_metrics.update(**{\"bleu\": np.nan})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "from catalyst.contrib.nn import RAdam\n",
    "\n",
    "\n",
    "lr = 1e-2\n",
    "\n",
    "optimizer = RAdam(model.parameters(), lr=lr)\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=PAD_ID_trg)\n",
    "callbacks = [\n",
    "    BLEUCallback()\n",
    "]\n",
    "loaders = {\"train\": train_loader, \"valid\": valid_loader}\n",
    "\n",
    "\n",
    "runner = Seq2SeqRunner(\n",
    "    source_vocab=source_vocab, target_vocab=target_vocab, device=device\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "\n",
    "\n",
    "logdir = Path(\"logs\") / datetime.now().strftime(\"%Y%m%d-%H%M%S\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/10 * Epoch (train):   5% 1/19 [00:00<00:12,  1.47it/s, bleu=nan, loss=9.002]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/adchumachenko/.local/lib/python3.6/site-packages/catalyst/contrib/nn/optimizers/radam.py:85: UserWarning:\n",
      "\n",
      "This overload of addcmul_ is deprecated:\n",
      "\taddcmul_(Number value, Tensor tensor1, Tensor tensor2)\n",
      "Consider using one of the following signatures instead:\n",
      "\taddcmul_(Tensor tensor1, Tensor tensor2, *, Number value) (Triggered internally at  /pytorch/torch/csrc/utils/python_arg_parser.cpp:766.)\n",
      "\n",
      "/home/adchumachenko/.local/lib/python3.6/site-packages/catalyst/contrib/nn/optimizers/radam.py:85: UserWarning:\n",
      "\n",
      "This overload of addcmul_ is deprecated:\n",
      "\taddcmul_(Number value, Tensor tensor1, Tensor tensor2)\n",
      "Consider using one of the following signatures instead:\n",
      "\taddcmul_(Tensor tensor1, Tensor tensor2, *, Number value) (Triggered internally at  /pytorch/torch/csrc/utils/python_arg_parser.cpp:766.)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/10 * Epoch (train): 100% 19/19 [00:06<00:00,  2.87it/s, bleu=nan, loss=8.660]\n",
      "1/10 * Epoch (train): 100% 19/19 [00:06<00:00,  2.87it/s, bleu=nan, loss=8.660]\n",
      "1/10 * Epoch (valid): 100% 19/19 [00:09<00:00,  2.04it/s, bleu=0.000e+00, loss=8.391]\n",
      "[2020-11-07 17:31:11,432] \n",
      "1/10 * Epoch 1 (train): bleu=nan | loss=8.9658\n",
      "1/10 * Epoch 1 (valid): bleu=0.000e+00 | loss=8.3688\n",
      "\n",
      "[2020-11-07 17:31:11,432] \n",
      "1/10 * Epoch 1 (train): bleu=nan | loss=8.9658\n",
      "1/10 * Epoch 1 (valid): bleu=0.000e+00 | loss=8.3688\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:metrics_logger:\n",
      "1/10 * Epoch 1 (train): bleu=nan | loss=8.9658\n",
      "1/10 * Epoch 1 (valid): bleu=0.000e+00 | loss=8.3688\n",
      "INFO:metrics_logger:\n",
      "1/10 * Epoch 1 (train): bleu=nan | loss=8.9658\n",
      "1/10 * Epoch 1 (valid): bleu=0.000e+00 | loss=8.3688\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2/10 * Epoch (train): 100% 19/19 [00:06<00:00,  2.86it/s, bleu=nan, loss=7.830]\n",
      "2/10 * Epoch (valid):   0% 0/19 [00:00<?, ?it/s]\n",
      "2/10 * Epoch (valid): 100% 19/19 [00:09<00:00,  2.05it/s, bleu=0.000e+00, loss=7.682]\n",
      "[2020-11-07 17:31:27,719] \n",
      "2/10 * Epoch 2 (train): bleu=nan | loss=7.8675\n",
      "2/10 * Epoch 2 (valid): bleu=0.000e+00 | loss=7.6429\n",
      "2/10 * Epoch (valid): 100% 19/19 [00:09<00:00,  2.05it/s, bleu=0.000e+00, loss=7.682]\n",
      "[2020-11-07 17:31:27,719] \n",
      "2/10 * Epoch 2 (train): bleu=nan | loss=7.8675\n",
      "2/10 * Epoch 2 (valid): bleu=0.000e+00 | loss=7.6429\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:metrics_logger:\n",
      "2/10 * Epoch 2 (train): bleu=nan | loss=7.8675\n",
      "2/10 * Epoch 2 (valid): bleu=0.000e+00 | loss=7.6429\n",
      "INFO:metrics_logger:\n",
      "2/10 * Epoch 2 (train): bleu=nan | loss=7.8675\n",
      "2/10 * Epoch 2 (valid): bleu=0.000e+00 | loss=7.6429\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3/10 * Epoch (train): 100% 19/19 [00:06<00:00,  2.91it/s, bleu=nan, loss=7.676]\n",
      "3/10 * Epoch (valid):   0% 0/19 [00:00<?, ?it/s]\n",
      "3/10 * Epoch (valid): 100% 19/19 [00:09<00:00,  1.96it/s, bleu=0.003, loss=7.563]\n",
      "[2020-11-07 17:31:44,889] \n",
      "3/10 * Epoch 3 (train): bleu=nan | loss=7.6207\n",
      "3/10 * Epoch 3 (valid): bleu=0.0043 | loss=7.5396\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:metrics_logger:\n",
      "3/10 * Epoch 3 (train): bleu=nan | loss=7.6207\n",
      "3/10 * Epoch 3 (valid): bleu=0.0043 | loss=7.5396\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3/10 * Epoch (valid): 100% 19/19 [00:09<00:00,  1.96it/s, bleu=0.003, loss=7.563]\n",
      "[2020-11-07 17:31:44,889] \n",
      "3/10 * Epoch 3 (train): bleu=nan | loss=7.6207\n",
      "3/10 * Epoch 3 (valid): bleu=0.0043 | loss=7.5396\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:metrics_logger:\n",
      "3/10 * Epoch 3 (train): bleu=nan | loss=7.6207\n",
      "3/10 * Epoch 3 (valid): bleu=0.0043 | loss=7.5396\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4/10 * Epoch (train): 100% 19/19 [00:06<00:00,  2.90it/s, bleu=nan, loss=7.577]\n",
      "4/10 * Epoch (train): 100% 19/19 [00:06<00:00,  2.90it/s, bleu=nan, loss=7.577]\n",
      "4/10 * Epoch (valid): 100% 19/19 [00:11<00:00,  1.63it/s, bleu=0.014, loss=7.541]\n",
      "[2020-11-07 17:32:03,912] \n",
      "4/10 * Epoch 4 (train): bleu=nan | loss=7.5441\n",
      "4/10 * Epoch 4 (valid): bleu=0.0130 | loss=7.5186\n",
      "\n",
      "[2020-11-07 17:32:03,912] \n",
      "4/10 * Epoch 4 (train): bleu=nan | loss=7.5441\n",
      "4/10 * Epoch 4 (valid): bleu=0.0130 | loss=7.5186\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:metrics_logger:\n",
      "4/10 * Epoch 4 (train): bleu=nan | loss=7.5441\n",
      "4/10 * Epoch 4 (valid): bleu=0.0130 | loss=7.5186\n",
      "INFO:metrics_logger:\n",
      "4/10 * Epoch 4 (train): bleu=nan | loss=7.5441\n",
      "4/10 * Epoch 4 (valid): bleu=0.0130 | loss=7.5186\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5/10 * Epoch (train): 100% 19/19 [00:06<00:00,  2.95it/s, bleu=nan, loss=7.504]\n",
      "5/10 * Epoch (valid):   0% 0/19 [00:00<?, ?it/s]\n",
      "5/10 * Epoch (valid): 100% 19/19 [00:09<00:00,  1.96it/s, bleu=0.003, loss=7.528]\n",
      "[2020-11-07 17:32:21,082] \n",
      "5/10 * Epoch 5 (train): bleu=nan | loss=7.4858\n",
      "5/10 * Epoch 5 (valid): bleu=0.0074 | loss=7.5035\n",
      "\n",
      "[2020-11-07 17:32:21,082] \n",
      "5/10 * Epoch 5 (train): bleu=nan | loss=7.4858\n",
      "5/10 * Epoch 5 (valid): bleu=0.0074 | loss=7.5035\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:metrics_logger:\n",
      "5/10 * Epoch 5 (train): bleu=nan | loss=7.4858\n",
      "5/10 * Epoch 5 (valid): bleu=0.0074 | loss=7.5035\n",
      "INFO:metrics_logger:\n",
      "5/10 * Epoch 5 (train): bleu=nan | loss=7.4858\n",
      "5/10 * Epoch 5 (valid): bleu=0.0074 | loss=7.5035\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6/10 * Epoch (train): 100% 19/19 [00:06<00:00,  2.84it/s, bleu=nan, loss=7.531]\n",
      "6/10 * Epoch (valid):   0% 0/19 [00:00<?, ?it/s]\n",
      "6/10 * Epoch (valid): 100% 19/19 [00:09<00:00,  1.93it/s, bleu=0.007, loss=7.550]\n",
      "[2020-11-07 17:32:38,604] \n",
      "6/10 * Epoch 6 (train): bleu=nan | loss=7.4220\n",
      "6/10 * Epoch 6 (valid): bleu=0.0132 | loss=7.5294\n",
      "6/10 * Epoch (valid): 100% 19/19 [00:09<00:00,  1.91it/s, bleu=0.007, loss=7.550]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:metrics_logger:\n",
      "6/10 * Epoch 6 (train): bleu=nan | loss=7.4220\n",
      "6/10 * Epoch 6 (valid): bleu=0.0132 | loss=7.5294\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6/10 * Epoch (valid): 100% 19/19 [00:09<00:00,  1.93it/s, bleu=0.007, loss=7.550]\n",
      "[2020-11-07 17:32:38,604] \n",
      "6/10 * Epoch 6 (train): bleu=nan | loss=7.4220\n",
      "6/10 * Epoch 6 (valid): bleu=0.0132 | loss=7.5294\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:metrics_logger:\n",
      "6/10 * Epoch 6 (train): bleu=nan | loss=7.4220\n",
      "6/10 * Epoch 6 (valid): bleu=0.0132 | loss=7.5294\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early exiting                                                                 \n",
      "7/10 * Epoch (train):  16% 3/19 [00:01<00:07,  2.16it/s, bleu=nan, loss=7.391]Early exiting\n",
      "7/10 * Epoch (train):  16% 3/19 [00:01<00:07,  2.16it/s, bleu=nan, loss=7.391]"
     ]
    }
   ],
   "source": [
    "runner.train(\n",
    "    model=model,\n",
    "    optimizer=optimizer,\n",
    "    criterion=criterion,\n",
    "    loaders=loaders,\n",
    "    callbacks=callbacks,\n",
    "    num_epochs=10,\n",
    "    verbose=True,\n",
    "    logdir=logdir,\n",
    "    main_metric=\"loss\",\n",
    "    minimize_metric=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our model, trained on small data, is not well prepared to be a good translator. Anyway, let's test code and the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' Die ist der die die die der'"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "' Die ist der die die die der'"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test = \"A cat eats a fish\"\n",
    "test_input_ids = train_dataset.transforms[0](test)\n",
    "test_input_ids = test_input_ids.reshape(-1, 1).to(device)\n",
    "\n",
    "prediction = model.translate(test_input_ids).to(\"cpu\").type(torch.LongTensor)\n",
    "decoding(prediction, target_vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "[homework]seq2seq_and_attn.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
