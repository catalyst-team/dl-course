{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "bpe",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.9"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "TRd-D0NyEoDX"
      },
      "source": [
        "!pip install -U catalyst torch==1.6 torchtext==0.7.0 youtokentome nltk"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Sr712sIYEoDZ"
      },
      "source": [
        "import torch\n",
        "from catalyst.utils import set_global_seed, get_device\n",
        "\n",
        "set_global_seed(42)\n",
        "device = \"cuda:0\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jgH3CAcubM4z"
      },
      "source": [
        "# Seminar\n",
        "\n",
        "Hi! Today we are going to learn a new tokenization algorithm, seq2seq metrics and a machine translation task. We will be acquainted with an attention mechanism."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "78Eb69nlEoDa"
      },
      "source": [
        "## BPE. YouTokenToMe\n",
        "\n",
        "Previously we have discussed a text preprocessing pipeline. We used `WordPunctTokenizer`, that tokenize text to words and punctuations. But this tokenization algorithm isn't perfect. Some languages have many word-forms. Many languages have words modification, like prefixes and suffixes. We want to save morphology information in text, but save every possible word-form isn't memory-efficient and isn't easy to train. However, we can create tokenziation mechanism, that will tokenize every word by subword morphology. And there is unsupervised algorithm to do it. It's called Byte Pair Encoding. How it works:\n",
        "\n",
        "![](https://lena-voita.github.io/resources/lectures/seq2seq/bpe/build_merge_table.gif)\n",
        "\n",
        "1. We split texts into characters\n",
        "2. Count bigrams on characters\n",
        "3. Merge the most popular pair\n",
        "4. Continue until we reach given vocabulary size.\n",
        "\n",
        "It's easy algorithm, and we have several implementations:\n",
        "- SentencePiece\n",
        "- fastBPE\n",
        "- Tokenizers by 🤗\n",
        "- YouTokenToMe\n",
        "\n",
        "The fastes one is YouTokenToMe by VK Team. Let's look how it works:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q4QJJL3oEoDa"
      },
      "source": [
        "from typing import List, Tuple\n",
        "\n",
        "import youtokentome as yttm\n",
        "from torchtext.utils import download_from_url, extract_archive\n",
        "from torchtext.vocab import Vocab\n",
        "from torchtext.experimental.datasets import WMT14"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fxxyus04EoDb"
      },
      "source": [
        "Download WMT14 dataset. It have pair texts on English and German languages, processed by Google Brain."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "usx6ANWlEoDb"
      },
      "source": [
        "wmt_url = \"https://drive.google.com/uc?export=download&id=0B_bZck-ksdkpM25jRUN2X2UxMm8\"\n",
        "dataset_tar = download_from_url(wmt_url, root=\"wmt14\")\n",
        "extracted = extract_archive(dataset_tar)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IBjs2A29EoDb"
      },
      "source": [
        "We will use `newtest2016` data us train part in training pipeline. Now we need to train BPE tokenizers for English and German languages. Consider vocabulary size as 10000 tokens."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cKvdK5j7EoDb"
      },
      "source": [
        "train_data_en_path = \"wmt14/newstest2016.en\"\n",
        "tokenizer_en_path = \"en.tok\"\n",
        "\n",
        "yttm.BPE.train(\n",
        "    data=train_data_en_path, vocab_size=10000, model=tokenizer_en_path\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MmCesNXgEoDc"
      },
      "source": [
        "train_data_de_path = \"wmt14/newstest2016.de\"\n",
        "tokenizer_de_path = \"de.tok\"\n",
        "\n",
        "yttm.BPE.train(\n",
        "    data=train_data_de_path, vocab_size=10000, model=tokenizer_de_path\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "--tdMoc7EoDd"
      },
      "source": [
        "Training procedure in `YTTM` run in a background. We need to load tokenizers to work with them:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SRoKYXmgEoDd"
      },
      "source": [
        "tokenizer_en = yttm.BPE(model=tokenizer_en_path)\n",
        "tokenizer_de = yttm.BPE(model=tokenizer_de_path)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W_Lt4V0tEoDd"
      },
      "source": [
        "Our text example will be:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0G1Lh55XEoDd"
      },
      "source": [
        "test_text = \"Tinkoff loves VK!\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r7CUQX0LEoDd"
      },
      "source": [
        "Try to get tokens, ids, add special tokens:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "40G2ibMXEoDe"
      },
      "source": [
        "tokenizer_en.encode([test_text], output_type=yttm.OutputType.SUBWORD)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0fw_TXfBEoDe"
      },
      "source": [
        "tokenizer_en.encode([test_text], output_type=yttm.OutputType.ID)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J7W2OYTLEoDe"
      },
      "source": [
        "tokenizer_en.encode(\n",
        "    [test_text], output_type=yttm.OutputType.SUBWORD, bos=True, eos=True\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KOUhtyQLEoDe"
      },
      "source": [
        "tokenizer_en.encode(\n",
        "    [test_text], output_type=yttm.OutputType.ID, bos=True, eos=True\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MvpeWiwNEoDf"
      },
      "source": [
        "To join `YTTM` tokenizer and `TorchText` dataset abstraction we need to code couple functions:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uhL2nbbtEoDf"
      },
      "source": [
        "# Code them\n",
        "\n",
        "def tokenize_de(text: str) -> List[str]:\n",
        "    return tokenizer_de.encode(\n",
        "        [text], output_type=yttm.OutputType.SUBWORD, bos=True, eos=True\n",
        "    )[0]\n",
        "\n",
        "\n",
        "def tokenize_en(text: str) -> List[str]:\n",
        "    return tokenizer_en.encode(\n",
        "        [text], output_type=yttm.OutputType.SUBWORD, bos=True, eos=True\n",
        "    )[0]\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4AZFPiCNEoDf"
      },
      "source": [
        "(train_dataset, valid_dataset, test_dataset) = WMT14(\n",
        "    train_filenames=(\"newstest2016.en\", \"newstest2016.de\"),\n",
        "    valid_filenames=(\"newstest2010.en\", \"newstest2010.de\"),\n",
        "    test_filenames=(\"newstest2009.en\", \"newstest2009.de\"),\n",
        "    tokenizer=(tokenize_en, tokenize_de),\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ag0xDvFCEoDf"
      },
      "source": [
        "Check how `dataset` works:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BpxPvwglEoDf"
      },
      "source": [
        "train_dataset[0]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k7syxoPTEoDg"
      },
      "source": [
        "tokens = [train_dataset.get_vocab()[0].itos[i] for i in train_dataset[0][0]]\n",
        "\"\".join(tokens)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0GRP2Fh1EoDg"
      },
      "source": [
        "tokens = [train_dataset.get_vocab()[1].itos[i] for i in train_dataset[0][1]]\n",
        "\"\".join(tokens)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oH5WYG5sEoDg"
      },
      "source": [
        "Let's code special function to decode input ids into human-readable text:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vkK0ApjJEoDg"
      },
      "source": [
        "# code function to decode input ids to pretty output\n",
        "\n",
        "def decoding(input_ids: torch.Tensor, vocab: Vocab) -> str:\n",
        "    result_text = \"\"\n",
        "    for input_id in input_ids:\n",
        "        if input_id == vocab.stoi[\"<EOS>\"]:\n",
        "            break\n",
        "        elif input_id != vocab.stoi[\"<BOS>\"]:\n",
        "            result_text += vocab.itos[input_id]\n",
        "    return \"\".join(t if t != \"▁\" else \" \" for t in result_text )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zxhq8iWzEoDg"
      },
      "source": [
        "decoding(train_dataset[0][0], train_dataset.get_vocab()[0])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xYApNQo2EoDh"
      },
      "source": [
        "decoding(train_dataset[0][1], train_dataset.get_vocab()[1])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WRTBLStHEoDh"
      },
      "source": [
        "We need to code padding code:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a64TTp7-EoDh"
      },
      "source": [
        "PAD_ID_src = train_dataset.get_vocab()[0].stoi[\"<PAD>\"]\n",
        "PAD_ID_trg = train_dataset.get_vocab()[1].stoi[\"<PAD>\"]\n",
        "max_length = 64 # 128\n",
        "\n",
        "def collate_fn(batch: Tuple[torch.Tensor]) -> Tuple[torch.Tensor]:\n",
        "    max_len_src = min(max(b[0].size(0) for b in batch), max_length)\n",
        "    max_len_trg = min(max(b[1].size(0) for b in batch), max_length)\n",
        "    all_src = torch.zeros(max_len_src, len(batch)) + PAD_ID_src\n",
        "    all_trg = torch.zeros(max_len_trg, len(batch)) + PAD_ID_trg\n",
        "\n",
        "    for num, (src, trg) in enumerate(batch):\n",
        "        all_src[: src.size(0), num] = src[:max_length]\n",
        "        all_trg[: trg.size(0), num] = trg[:max_length]\n",
        "    return all_src.type(torch.LongTensor), all_trg.type(torch.LongTensor)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DRaqgMbqEoDh"
      },
      "source": [
        "And bucketing sampler! It's special sampler, that will reduce padding in batches. We need to sort our text by lens in tokens, and form batches using text order. We'll implement this by `SortedSampler` and `RandomSubsetSampler`:\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XFiVfJKAEoDi"
      },
      "source": [
        "from typing import Any, Callable, Iterable\n",
        "\n",
        "from torch.utils.data import Dataset\n",
        "from torch.utils.data.sampler import Sampler\n",
        "\n",
        "\n",
        "class SortedSampler(Sampler):\n",
        "    def __init__(self, data: Dataset, sort_key: Callable[[Any], Any] = lambda x: x):\n",
        "        super().__init__(data)\n",
        "        self.data = data\n",
        "        self.sort_key = sort_key\n",
        "        zip_ = [(i, self.sort_key(row)) for i, row in enumerate(self.data)]\n",
        "        zip_ = sorted(zip_, key=lambda r: r[1])\n",
        "        self.sorted_indexes = [item[0] for item in zip_]\n",
        "\n",
        "    def __iter__(self) -> Iterable[int]:\n",
        "        return iter(self.sorted_indexes)\n",
        "\n",
        "    def __len__(self) -> int:\n",
        "        return len(self.data)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Sc-3C-bdEoDi"
      },
      "source": [
        "`BucketBatchSampler`'s algorithm is this:\n",
        "\n",
        "- Create buckets, subsets on random order.\n",
        "- Sort data in each bucket\n",
        "- Generate sample by getting items from buckets"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QdSMA4nHEoDi"
      },
      "source": [
        "import math\n",
        "from typing import Generator, List\n",
        "\n",
        "from torch.utils.data.sampler import BatchSampler\n",
        "from torch.utils.data.sampler import SubsetRandomSampler\n",
        "\n",
        "\n",
        "class BucketBatchSampler(BatchSampler):\n",
        "    def __init__(\n",
        "        self,\n",
        "        sampler: Sampler,\n",
        "        batch_size: int,\n",
        "        drop_last: bool,\n",
        "        sort_key: Callable[[Any], Any] = lambda x: x,\n",
        "        bucket_size_multiplier: int = 100\n",
        "    ):\n",
        "        super().__init__(sampler, batch_size, drop_last)\n",
        "        self.sort_key = sort_key\n",
        "        self.bucket_sampler = BatchSampler(\n",
        "            sampler,\n",
        "            min(batch_size * bucket_size_multiplier, len(sampler)),\n",
        "            False\n",
        "        )\n",
        "\n",
        "    def __iter__(self) -> Generator[List[int], None, None]:\n",
        "        for bucket in self.bucket_sampler:\n",
        "            sorted_sampler = SortedSampler(bucket, self.sort_key)\n",
        "            for batch in SubsetRandomSampler(\n",
        "                list(\n",
        "                    BatchSampler(\n",
        "                        sorted_sampler, \n",
        "                        self.batch_size, \n",
        "                        self.drop_last\n",
        "                    )\n",
        "                )\n",
        "            ):\n",
        "                yield [bucket[i] for i in batch]\n",
        "\n",
        "    def __len__(self):\n",
        "        if self.drop_last:\n",
        "            return len(self.sampler) // self.batch_size\n",
        "        else:\n",
        "            return math.ceil(len(self.sampler) / self.batch_size)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gC0VEkgyEoDi"
      },
      "source": [
        "And now we just need to create data loaders:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0SAHuX17EoDi"
      },
      "source": [
        "from torch.utils.data import DataLoader\n",
        "from torch.utils.data.sampler import RandomSampler\n",
        "\n",
        "\n",
        "batch_size = 128\n",
        "\n",
        "\n",
        "train_sampler = RandomSampler(train_dataset)\n",
        "sort_key = lambda row: len(train_dataset[row][0])\n",
        "train_batch_sampler = BucketBatchSampler(\n",
        "    train_sampler, \n",
        "    batch_size=batch_size,\n",
        "    drop_last=True,\n",
        "    sort_key=sort_key\n",
        ")\n",
        "\n",
        "train_loader = DataLoader(\n",
        "    train_dataset,\n",
        "    batch_sampler=train_batch_sampler,\n",
        "    collate_fn=collate_fn\n",
        ")\n",
        "valid_loader = DataLoader(\n",
        "    valid_dataset,\n",
        "    batch_size=batch_size,\n",
        "    shuffle=False,\n",
        "    drop_last=False,\n",
        "    collate_fn=collate_fn\n",
        ")\n",
        "test_loader = DataLoader(\n",
        "    test_dataset,\n",
        "    batch_size=batch_size,\n",
        "    shuffle=False,\n",
        "    drop_last=False,\n",
        "    collate_fn=collate_fn\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zBHXhN1YEoDj"
      },
      "source": [
        "## BLEU\n",
        "\n",
        "In this section we will discuss metrics for Seq2Seq models. There are several metrics: BLEU, ROUGE, METEOR, WER, etc. They used to understand how well model solve any task with generating texts with target text. Let's look on to BLEU.\n",
        "\n",
        "BLEU stands for \"BIlingual Evaluation Understudy\". To compute it, we need n-grams for predicted text(hypothesis) and target text(references) and compare them. And BLEU would be a number of n-grams from predicted text, appears in target text. Let's look at the example:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wPlOqjLmEoDj"
      },
      "source": [
        "test_target = \"Die Prager Börse stürzt gegen Geschäftsschluss ins Minus\"\n",
        "test_predicted = \"Das Prager Börse stürzt gegest Geschäftschlus uns Minus\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZU6aWxrfEoDj"
      },
      "source": [
        "target_tokens = test_target.split() # Simple way to get tokens\n",
        "\n",
        "unigrams_target = [(t_0,) for t_0 in target_tokens]\n",
        "bigrams_target = [\n",
        "    (t_0, t_1) for t_0, t_1 in zip(target_tokens[:-1], target_tokens[1:])\n",
        "]\n",
        "trigrams_target = [\n",
        "    (t_0, t_1, t_2)\n",
        "    for t_0, t_1, t_2 in zip(\n",
        "        target_tokens[:-2], target_tokens[1:-1], target_tokens[2:]\n",
        "    )\n",
        "]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "frvgQNNtEoDj"
      },
      "source": [
        "predicted_tokens = test_predicted.split()\n",
        "\n",
        "\n",
        "# find ngrams for predicted text\n",
        "unigrams_predicted = [(t_0,) for t_0 in predicted_tokens]\n",
        "bigrams_predicted = [\n",
        "    (t_0, t_1) for t_0, t_1 in zip(predicted_tokens[:-1], predicted_tokens[1:])\n",
        "]\n",
        "trigrams_predicted = [\n",
        "    (t_0, t_1, t_2)\n",
        "    for t_0, t_1, t_2 in zip(\n",
        "        predicted_tokens[:-2], predicted_tokens[1:-1], predicted_tokens[2:]\n",
        "    )\n",
        "]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jzBOgiQZEoDk"
      },
      "source": [
        "Count number of n-grams appeard in target text:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CC_wBSn1EoDk"
      },
      "source": [
        "count_unigrams = sum(\n",
        "    uni in unigrams_target for uni in unigrams_predicted\n",
        ") / len(unigrams_predicted)\n",
        "\n",
        "# Count statistic for bigrams and trigrams\n",
        "count_bigrams = sum(\n",
        "    bi in bigrams_target for bi in bigrams_predicted\n",
        ") / len(bigrams_predicted)\n",
        "count_trigrams = sum(\n",
        "    tri in trigrams_target for tri in trigrams_predicted\n",
        ") / len(trigrams_predicted)\n",
        "print(f\"Uni: {count_unigrams}\\nBi: {count_bigrams}\\nTri: {count_trigrams}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7ibQszMtEoDk"
      },
      "source": [
        "bleu = (count_unigrams + count_bigrams + count_trigrams) / 3\n",
        "print(f\"Our BLEU: {bleu}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wgM6qq5HEoDk"
      },
      "source": [
        "We don't need to implement BLEU score from scratch. In `nltk` we have algorithms to calculate it:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wXHGDL6GEoDk"
      },
      "source": [
        "from nltk.translate.bleu_score import corpus_bleu\n",
        "\n",
        "\n",
        "def compute_bleu(predicted, target):\n",
        "    return corpus_bleu([[ref] for ref in target], predicted)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sl9qS_0SEoDk"
      },
      "source": [
        "compute_bleu([test_predicted], [test_target])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SzPwotorEoDl"
      },
      "source": [
        "## Seq2Seq. Translation\n",
        "\n",
        "Translation is one of the task, where we need to have Seq2Seq models, that consist of an Encoder and a Decoder. An encoder should return an informative vector, that will represent an input text. A decoder should generate translation, based on the vector. We will use a Recurrent Neural Network with additional component called attention.\n",
        "\n",
        "### RNN + Attention \n",
        "\n",
        "There are two famous attention formulation in the nlp. One of them is [by Luong](https://arxiv.org/pdf/1508.04025.pdf). Another one is [by Bahdanau](https://arxiv.org/pdf/1409.0473.pdf). We will implement an aproximation of Luong attention, that can be showed like this:\n",
        "\n",
        "![](https://lena-voita.github.io/resources/lectures/seq2seq/attention/luong_model-min.png)\n",
        "\n",
        "Let's code this."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xHraKn0LEoDl"
      },
      "source": [
        "import torch.nn as nn\n",
        "\n",
        "class Encoder(nn.Module):\n",
        "    def __init__(\n",
        "        self,\n",
        "        vocab_size: int,\n",
        "        emb_size: int,\n",
        "        hidden_size: int,\n",
        "        num_layers: int,\n",
        "        dropout: float,\n",
        "    ):\n",
        "        super().__init__()\n",
        "\n",
        "        self.vocab_size = vocab_size\n",
        "        self.emb_size = emb_size\n",
        "        self.hidden_size = hidden_size\n",
        "        self.num_layers = num_layers\n",
        "\n",
        "        self.embedding = nn.Embedding(vocab_size, emb_size)\n",
        "\n",
        "        self.rnn = nn.LSTM(\n",
        "            emb_size, hidden_size, num_layers=num_layers, dropout=dropout\n",
        "        )\n",
        "\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(\n",
        "        self, src: torch.Tensor\n",
        "    ) -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor]:\n",
        "        embedded = self.embedding(src)\n",
        "        embedded = self.dropout(embedded)\n",
        "        outputs, (hidden, cell) = self.rnn(embedded)\n",
        "        return outputs, hidden, cell\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eL_79BajEoDl"
      },
      "source": [
        "class Attention(nn.Module):\n",
        "    def __init__(self, hidden_size: int):\n",
        "        super().__init__()\n",
        "        self.hidden_size = hidden_size\n",
        "\n",
        "        # Instead from one matrix we will use two linear modules\n",
        "        self.enc_linear = nn.Linear(hidden_size, hidden_size)\n",
        "        self.dec_linear = nn.Linear(hidden_size, hidden_size)\n",
        "\n",
        "    def forward(\n",
        "        self, last_hidden: torch.Tensor, encoder_outputs: torch.Tensor\n",
        "    ) -> torch.Tensor:\n",
        "        bs = last_hidden.size(1)\n",
        "\n",
        "        # Prepare our examples\n",
        "        encoder_outputs = self.enc_linear(encoder_outputs).reshape(\n",
        "            bs, -1, self.hidden_size\n",
        "        )\n",
        "        last_hidden = self.dec_linear(last_hidden).reshape(\n",
        "            bs, self.hidden_size, 1\n",
        "        )\n",
        "\n",
        "        # Compute logits by batch matrix multiplication\n",
        "        logits = torch.bmm(encoder_outputs, last_hidden)\n",
        "\n",
        "        attn = torch.softmax(logits, 1).reshape(-1, bs, 1)\n",
        "        return attn\n",
        "        \n",
        "\n",
        "class DecoderAttn(nn.Module):\n",
        "    def __init__(\n",
        "        self,\n",
        "        vocab_size: int,\n",
        "        emb_size: int,\n",
        "        hidden_size: int,\n",
        "        num_layers: int,\n",
        "        attention: Attention,\n",
        "        dropout: float,\n",
        "    ):\n",
        "        super().__init__()\n",
        "\n",
        "        self.emb_size = emb_size\n",
        "        self.hidden_size = hidden_size\n",
        "        self.vocab_size = vocab_size\n",
        "        self.num_layers = num_layers\n",
        "\n",
        "        self.attn = attention\n",
        "\n",
        "        self.embedding = nn.Embedding(vocab_size, emb_size)\n",
        "\n",
        "        self.rnn = nn.LSTM(\n",
        "            emb_size, hidden_size, num_layers=num_layers, dropout=dropout\n",
        "        )\n",
        "        self.out = nn.Linear(hidden_size, vocab_size)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(\n",
        "        self,\n",
        "        input_: torch.Tensor,\n",
        "        hidden: torch.Tensor,  # hidden_state from t-1\n",
        "        cell: torch.Tensor,\n",
        "        encoder_output: torch.Tensor,\n",
        "    ) -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor]:\n",
        "        embedded = self.embedding(input_)\n",
        "        embedded = self.dropout(embedded)\n",
        "\n",
        "        attn = self.attn(hidden[-1:], encoder_output)\n",
        "        # Generating new cell state by attention and encoder output\n",
        "        new_cell = (encoder_output * attn).sum(0)\n",
        "        cell[-1] = new_cell\n",
        "\n",
        "        output, (hidden, cell) = self.rnn(embedded, (hidden, cell))\n",
        "        prediction = self.out(output)\n",
        "\n",
        "        return prediction, hidden, cell"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YGlCHiQpEoDl"
      },
      "source": [
        "One important point about training Seq2Seq models it's adding target tokens in a Decoder training loop. While our model is not good enough, it's generating \"trash\" tokens, that hasn't any information for generating. That's why we try to feed the decoder. However, it's not good too! The decoder will generate text via its generated tokens. Fopr this purpose we try to train the Decoder with random decited tokens source (target or itself)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k0dzxxe_EoDl"
      },
      "source": [
        "from random import random\n",
        "\n",
        "\n",
        "BOS_IDX = train_dataset.get_vocab()[1].stoi[\"<BOS>\"]\n",
        "\n",
        "class Seq2Seq(nn.Module):\n",
        "    def __init__(self, encoder: Encoder, decoder: DecoderAttn):\n",
        "        super().__init__()\n",
        "\n",
        "        self.encoder = encoder\n",
        "        self.decoder = decoder\n",
        "        self.max_len = max_length\n",
        "\n",
        "    def forward(\n",
        "        self,\n",
        "        src: torch.Tensor,\n",
        "        trg: torch.Tensor,\n",
        "        teacher_forcing_ratio: float = 0.1,\n",
        "    ) -> torch.Tensor:\n",
        "        batch_size = src.shape[1]\n",
        "        max_len = trg.shape[0]\n",
        "        trg_vocab_size = self.decoder.vocab_size\n",
        "\n",
        "        outputs = torch.zeros(max_len, batch_size, trg_vocab_size).to(device)\n",
        "\n",
        "        enc_out, hidden, cell = self.encoder(src)\n",
        "\n",
        "        input_ = torch.zeros(1, batch_size) + BOS_IDX\n",
        "        input_ = input_.type(torch.LongTensor).to(device)\n",
        "\n",
        "        for t in range(1, max_len):\n",
        "            output, hidden, cell = self.decoder(input_, hidden, cell, enc_out)\n",
        "            outputs[t] = output\n",
        "            teacher_force = random() < teacher_forcing_ratio\n",
        "            top1 = output.max(2)[1]\n",
        "            input_ = (trg[t] if teacher_force else top1).reshape(1, -1)\n",
        "\n",
        "        return outputs[1:]\n",
        "\n",
        "    def translate(self, src: torch.Tensor) -> torch.Tensor:\n",
        "        batch_size = src.shape[1]\n",
        "        outputs = torch.zeros(self.max_len, batch_size).to(device)\n",
        "\n",
        "        enc_out, hidden, cell = self.encoder(src)\n",
        "\n",
        "        input_ = torch.zeros(1, batch_size) + BOS_IDX\n",
        "        input_ = input_.type(torch.LongTensor).to(device)\n",
        "\n",
        "        for t in range(1, self.max_len):\n",
        "            output, hidden, cell = self.decoder(input_, hidden, cell, enc_out)\n",
        "            top1 = output.max(2)[1].reshape(-1)\n",
        "            outputs[t] = top1\n",
        "            input_ = top1.reshape(1, -1)\n",
        "\n",
        "        return outputs[1:]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5CTAGtZqEoDm"
      },
      "source": [
        "Create a model, special runner for Seq2Seq models and train the model!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "32-AWyFnEoDm"
      },
      "source": [
        "source_vocab, target_vocab = train_dataset.get_vocab()\n",
        "\n",
        "input_size = len(source_vocab)\n",
        "output_size = len(target_vocab)\n",
        "src_emb_size = tgt_emb_size = 100\n",
        "hidden_size = 300\n",
        "num_layers =  2\n",
        "dropout_p = 0.1\n",
        "\n",
        "enc = Encoder(input_size, src_emb_size, hidden_size, num_layers, dropout_p)\n",
        "attention = Attention(hidden_size)\n",
        "dec = DecoderAttn(\n",
        "    output_size, tgt_emb_size, hidden_size, num_layers, attention, dropout_p\n",
        ")\n",
        "model = Seq2Seq(enc, dec).to(device)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dA-KqftXEoDm"
      },
      "source": [
        "To train model, we will compare generated tokens with a target for each source. To compare, use `CrossEntropyLoss`!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R2kFWQsNEoDm"
      },
      "source": [
        "from catalyst.dl import Runner\n",
        "\n",
        "\n",
        "class Seq2SeqRunner(Runner):\n",
        "    def __init__(\n",
        "        self, source_vocab: Vocab, target_vocab: Vocab, *args, **kwargs\n",
        "    ):\n",
        "        super().__init__(*args, **kwargs)\n",
        "        self.source_vocab = source_vocab\n",
        "        self.target_vocab = target_vocab\n",
        "\n",
        "    def predict_batch(self, batch) -> torch.Tensor:\n",
        "        source, target = batch\n",
        "        predictions = self.model.translate(source).type(torch.LongTensor)\n",
        "        translations = [\n",
        "            decoding(sentence, self.target_vocab)\n",
        "            for sentence in predictions.t()\n",
        "        ]\n",
        "        return translations\n",
        "\n",
        "    def handle_batch(self, batch) -> None:\n",
        "        source, target = batch\n",
        "        self.batch = {}\n",
        "\n",
        "        if self.is_valid_loader:\n",
        "            target_decoded = [\n",
        "                decoding(sentence, runner.target_vocab)\n",
        "                for sentence in target.t()\n",
        "            ]\n",
        "            predicted = runner.predict_batch(batch)\n",
        "            self.batch[\"predicted\"] = predicted\n",
        "            self.batch[\"target_decoded\"] = target_decoded\n",
        "\n",
        "        logits = self.model(source, target)\n",
        "        target = target[1:].reshape(-1)\n",
        "        logits = logits.reshape(target.size(0), -1)\n",
        "        self.batch.update(\n",
        "            **{\"source\": source, \"target\": target, \"logits\": logits}\n",
        "        )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3W3eTQR6EoDm"
      },
      "source": [
        "To calculate BLEU score in train loop, we need to code Callback for this."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y0ADUcAyEoDm"
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "from catalyst.dl import Callback, CallbackOrder\n",
        "\n",
        "\n",
        "class BLEUCallback(Callback):\n",
        "    def __init__(self):\n",
        "        super().__init__(CallbackOrder.Metric)\n",
        "\n",
        "    def on_batch_end(self, runner: Runner) -> None:\n",
        "        if runner.is_valid_loader:\n",
        "            predicted = runner.batch[\"predicted\"]\n",
        "            target = runner.batch[\"target_decoded\"]\n",
        "            bleu = compute_bleu(predicted, target)\n",
        "            runner.batch_metrics.update(**{\"bleu\": bleu})"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sl_VJXCIEoDn"
      },
      "source": [
        "from catalyst.contrib.nn import RAdam\n",
        "from torch.nn.utils import clip_grad_norm_\n",
        "from catalyst.dl import CriterionCallback, OptimizerCallback\n",
        "\n",
        "\n",
        "lr = 1e-2\n",
        "\n",
        "optimizer = RAdam(model.parameters(), lr=lr)\n",
        "criterion = nn.CrossEntropyLoss(ignore_index=PAD_ID_trg)\n",
        "callbacks = [\n",
        "    CriterionCallback(\"logits\", \"target\", \"loss\"),\n",
        "    OptimizerCallback(\n",
        "        \"loss\", grad_clip_fn=clip_grad_norm_, grad_clip_params={\"max_norm\": 1}\n",
        "    ),\n",
        "    BLEUCallback(),\n",
        "]\n",
        "loaders = {\"train\": train_loader, \"valid\": valid_loader}\n",
        "\n",
        "\n",
        "runner = Seq2SeqRunner(source_vocab=source_vocab, target_vocab=target_vocab)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EDM_QircEoDn"
      },
      "source": [
        "from datetime import datetime\n",
        "from pathlib import Path\n",
        "\n",
        "\n",
        "logdir = Path(\"logs\") / datetime.now().strftime(\"%Y%m%d-%H%M%S\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cMLLXf47EoDn"
      },
      "source": [
        "runner.train(\n",
        "    model=model,\n",
        "    optimizer=optimizer,\n",
        "    criterion=criterion,\n",
        "    loaders=loaders,\n",
        "    callbacks=callbacks,\n",
        "    num_epochs=5,\n",
        "    verbose=True,\n",
        "    logdir=logdir,\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0NcgGlR0EoDn"
      },
      "source": [
        "Our model, trained on small data, is not well prepared to be a good translator. Anyway, let's test code and the model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zvukt1WlEoDn"
      },
      "source": [
        "test = \"A cat eats a fish\"\n",
        "test_input_ids = train_dataset.transforms[0](test)\n",
        "test_input_ids = test_input_ids.reshape(-1, 1).to(device)\n",
        "\n",
        "prediction = model.translate(test_input_ids).to(\"cpu\").type(torch.LongTensor)\n",
        "decoding(prediction, target_vocab)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iw8LmT2WJAvo"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}