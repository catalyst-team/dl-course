{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -Uq catalyst==20.12"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Seminar\n",
    "\n",
    "Hey! Today we are going to learn a recommendation system basis. We'll introduce metrics, an example dataset and couple of recommendation systems. \n",
    "\n",
    "Move on!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from catalyst.utils import set_global_seed, get_device\n",
    "\n",
    "\n",
    "set_global_seed(42)\n",
    "device = get_device()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Metrics\n",
    "\n",
    "Our example will be this. We have 6 documents, and our model predict some order on it. For example, we gave some users to say how relevant were these documents. Model prediction is `order`, and human score is `rel_score`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "order = np.array([1, 2, 3, 4, 5, 6])\n",
    "rel_score = np.array([3, 2, 3, 0, 1, 2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How good is our model? Check by Discounted Cumulative Gain and HitRate."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DCG\n",
    "\n",
    "It's most popular way to understand system perfomance. It's computed by formula:\n",
    "\n",
    "$$\n",
    "\\mathrm{DCG_{p}} = rel_1 + \\sum_{i=2}^{p} \\frac{rel_{i}}{\\log_{2}(i+1)}\n",
    "$$\n",
    "\n",
    "Implement it!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DCG_3 = rel_score[0] + np.sum(rel_score[1:3] / np.log2(order[1:3] + 1))\n",
    "DCG_6 = rel_score[0] + np.sum(rel_score[1:] / np.log2(order[1:] + 1))\n",
    "assert np.isclose(DCG_3, 5.7618595)\n",
    "assert np.isclose(DCG_6, 6.8611266)\n",
    "print(f\"DCG_3: {DCG_3}, DCG_6: {DCG_6}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The gain formula can be changed to the exponantial form. And we will get another DCG formulation.\n",
    "\n",
    "$$\n",
    "\\mathrm{DCG_{p}} = \\sum_{i=1}^{p} \\frac{2^{\\text{rel}_{i}} - 1}{\\log_{2}(i+1)}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DCG_6 = np.sum((2**rel_score - 1) / np.log2(order + 1))\n",
    "assert np.isclose(DCG_6, 13.8482636)\n",
    "print(f\"Exponantial DCG_6: {DCG_6}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Usually Normal DCG is used. Formula:\n",
    "\n",
    "$$ \n",
    "\\mathrm{nDCG_{p}} = \\frac{DCG_{p}}{IDCG_{p}}\n",
    "$$\n",
    "\n",
    "IDCG is ideal DCG. It's calculated when system order is gotten by human relevance score:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ideal_order = np.array([1, 4, 2, 6, 5, 3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "IDCG_6 = np.sum((2**rel_score - 1) / np.log2(ideal_order + 1))\n",
    "assert np.isclose(IDCG_6, 14.59539075)\n",
    "print(f\"IDCG_6: {IDCG_6}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NDCG_6 = DCG_6 / IDCG_6\n",
    "assert np.isclose(NDCG_6, 0.9488107)\n",
    "print(f\"NDCG_6: {NDCG_6}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "There is a implemented function to calculate ndcg in Catalyst."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "from catalyst import metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "our_score = 1 / order # Higher score – higher raiting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t_our_score = torch.tensor([our_score])\n",
    "t_rel_score = torch.tensor([rel_score])\n",
    "\n",
    "print(f\"NDCG: {metrics.ndcg(t_our_score, t_rel_score, top_k=[2, 3, 6])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hit Rate\n",
    "\n",
    "Another way to get the system performance is HitRate. To calculate it, we need to count how many times an item from the system order is relevent for user. Example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "one_user_rel_score = rel_score // 3 # <-- only two documents are relevent for one user\n",
    "print(f\"New rel_score: {one_user_rel_score}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "hitrate = one_user_rel_score.mean()\n",
    "assert np.isclose(hitrate, 0.33333)\n",
    "print(f\"HitRate: {hitrate}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "one_user_t_rel_score = t_rel_score // 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "print(f\"HitRate: {metrics.hitrate(t_our_score, one_user_t_rel_score)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Movie Lens Dataset\n",
    "\n",
    "\n",
    "MovieLens Dataset contains users score of some movies. `0` means that an user hasn't set raiting. An user can set raiting from `1` to `5`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "one_user_rel_score = rel_score // 3 # <-- only two documents are relevent for one user\n",
    "print(f\"New rel_score: {one_user_rel_score}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hitrate = one_user_rel_score.mean()\n",
    "assert np.isclose(hitrate, 0.33333)\n",
    "print(f\"HitRate: {hitrate}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from catalyst.metrics import hitrate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "one_user_t_rel_score = t_rel_score // 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"HitRate: {metrics.hitrate(t_our_score, one_user_t_rel_score)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Movie Lens Dataset\n",
    "\n",
    "\n",
    "MovieLens Dataset contains users score of some movies. `0` means that an user hasn't set raiting. An user can set raiting from `1` to `5`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from catalyst.contrib.datasets import MovieLens\n",
    "\n",
    "\n",
    "train_dataset = MovieLens(root=\".\", train=True, download=True)\n",
    "test_dataset = MovieLens(root=\".\", train=False, download=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll try to train model to find high scored unseed movies. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import typing as tp\n",
    "\n",
    "from catalyst.utils import get_loader\n",
    "\n",
    "\n",
    "def dist_transform(row: tp.Dict[str, tp.Any]) -> tp.Dict[str, tp.Any]:\n",
    "    raitings = row[\"raitings\"]\n",
    "    movie_ids = torch.arange(raitings.size(0))[raitings > 0]\n",
    "    user_ids = (\n",
    "        torch.zeros_like(movie_ids).type(torch.LongTensor) + row[\"user_id\"]\n",
    "    )\n",
    "    targets = (raitings[raitings > 0] / 5.0).type(torch.FloatTensor)\n",
    "    return {\"user_ids\": user_ids, \"movie_ids\": movie_ids, \"targets\": targets}\n",
    "\n",
    "\n",
    "def collate_fn(\n",
    "    batch: tp.Sequence[tp.Dict[str, torch.Tensor]]\n",
    ") -> tp.Dict[str, torch.Tensor]:\n",
    "    user_ids = torch.cat([b[\"user_ids\"] for b in batch])\n",
    "    movie_ids = torch.cat([b[\"movie_ids\"] for b in batch])\n",
    "    targets = torch.cat([b[\"targets\"] for b in batch])\n",
    "    return {\"user_ids\": user_ids, \"movie_ids\": movie_ids, \"targets\": targets}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_indexes = torch.arange(len(train_dataset))\n",
    "\n",
    "train_dataloader = get_loader(\n",
    "    user_indexes,\n",
    "    open_fn=lambda x: {\"user_id\": x, \"raitings\": train_dataset[x]},\n",
    "    dict_transform=dist_transform,\n",
    "    batch_size=1,\n",
    "    num_workers=4,\n",
    "    shuffle=True,\n",
    "    drop_last=True,\n",
    "    collate_fn=collate_fn\n",
    ")\n",
    "\n",
    "valid_dataloader = get_loader(\n",
    "    user_indexes,\n",
    "    open_fn=lambda x: {\"user_id\": x, \"raitings\": test_dataset[x]},\n",
    "    dict_transform=dist_transform,\n",
    "    batch_size=1,\n",
    "    num_workers=4,\n",
    "    shuffle=True,\n",
    "    drop_last=True,\n",
    "    collate_fn=collate_fn\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Funk SVD\n",
    "\n",
    "\n",
    "A first method it's SVD base. Instead of calculating true SVD matrices, we will find them by fitting!\n",
    "\n",
    "These implementation based on this [medium post](https://medium.com/datadriveninvestor/how-funk-singular-value-decomposition-algorithm-work-in-recommendation-engines-36f2fbf62cac)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "\n",
    "class FunkSVD(nn.Module):\n",
    "    def __init__(self, user_num: int, item_num: int, embedding_dim: int):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.user_embeddings = nn.Embedding(user_num, embedding_dim)\n",
    "        self.item_embeddings = nn.Embedding(item_num, embedding_dim)\n",
    "        \n",
    "        self.user_bias = nn.Embedding(user_num, 1)\n",
    "        self.item_bias = nn.Embedding(item_num, 1)\n",
    "        \n",
    "        self.bias = torch.nn.Parameter(torch.zeros(1))\n",
    "        self.embedding_dim = embedding_dim\n",
    "            \n",
    "    def forward(\n",
    "        self, user_ids: torch.Tensor, movie_ids: torch.Tensor\n",
    "    ) -> torch.Tensor:\n",
    "        user_embedding = self.user_embeddings(user_ids)\n",
    "        user_bias = self.user_bias(user_ids).reshape(-1)\n",
    "        item_embedding = self.item_embeddings(movie_ids)\n",
    "        item_bias = self.item_bias(movie_ids).reshape(-1)\n",
    "        dot = torch.einsum(\"oi,oj->o\", user_embedding, item_embedding)\n",
    "        output = dot + user_bias + item_bias + self.bias\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from catalyst.contrib.nn import RAdam\n",
    "\n",
    "\n",
    "model = FunkSVD(len(train_dataset), len(train_dataset[0]), 16)\n",
    "optimizer = RAdam(model.parameters(), lr=1e-1)\n",
    "criterion = nn.BCEWithLogitsLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from catalyst.dl import SupervisedRunner, Callback, CallbackOrder, SupervisedRunner, IRunner\n",
    "\n",
    "\n",
    "class RankMetricCallback(Callback):\n",
    "    def __init__(self):\n",
    "        super().__init__(CallbackOrder.Metric)\n",
    "        \n",
    "        self.top_k = [1, 3, 5]\n",
    "\n",
    "    def on_batch_end(self, runner: IRunner):\n",
    "        # In every batch we have only one user.\n",
    "        targets = runner.input[\"targets\"].to(\"cpu\")\n",
    "        logits = runner.output[\"logits\"].to(\"cpu\")\n",
    "        \n",
    "        sorted_indeces = torch.argsort(logits, descending=True)\n",
    "        targets = torch.take(targets, sorted_indeces).reshape(1, -1)\n",
    "        logits = torch.take(logits, sorted_indeces).reshape(1, -1)\n",
    "        \n",
    "        ndcg_values = metrics.ndcg(logits, targets, top_k=self.top_k)\n",
    "        runner.batch_metrics.update(\n",
    "            {f\"ndcg_{k}\": v for k, v in zip(self.top_k, ndcg_values)}\n",
    "        )\n",
    "\n",
    "        \n",
    "runner = SupervisedRunner(input_key=[\"user_ids\", \"movie_ids\"], device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from catalyst.callbacks import ControlFlowCallback, OptimizerCallback\n",
    "\n",
    "\n",
    "callbacks = [\n",
    "    RankMetricCallback(),\n",
    "    OptimizerCallback(\n",
    "        accumulation_steps=64\n",
    "    ),\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "\n",
    "runner.train(\n",
    "    model=model,\n",
    "    optimizer=optimizer,\n",
    "    loaders={\"train\": train_dataloader, \"valid\": valid_dataloader},\n",
    "    criterion=criterion,\n",
    "    callbacks=callbacks,\n",
    "    logdir=Path(\"logs\") / datetime.now().strftime(\"%Y%m%d-%H%M%S\"),\n",
    "    num_epochs=10,\n",
    "    verbose=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural Collaborative Filtering\n",
    "\n",
    "\n",
    "Second method it's calculating user and item embeddings. To score user-item pair relevance, we aare going to concatinating vectors and pass forward through a neural network.\n",
    "\n",
    "This method based on NCF article: [arxiv](https://arxiv.org/pdf/1708.05031.pdf). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NСF(nn.Module):\n",
    "    def __init__(\n",
    "        self, user_num: int, item_num: int, embedding_dim: int, hidden_dim: int\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        self.user_embeddings = nn.Embedding(user_num, embedding_dim)\n",
    "        self.item_embeddings = nn.Embedding(item_num, embedding_dim)\n",
    "\n",
    "        self.layers = nn.Sequential(\n",
    "            nn.Linear(2 * embedding_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, 1),\n",
    "        )\n",
    "\n",
    "    def forward(\n",
    "        self, user_ids: torch.Tensor, movie_ids: torch.Tensor\n",
    "    ) -> torch.Tensor:\n",
    "        user_embedding = self.user_embeddings(user_ids)\n",
    "        item_embedding = self.item_embeddings(movie_ids)\n",
    "        concat = torch.cat((user_embedding, item_embedding), -1)\n",
    "        return self.layers(concat).view(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from catalyst.contrib.nn import RAdam\n",
    "\n",
    "\n",
    "model = NСF(len(train_dataset), len(train_dataset[0]), 64, 64)\n",
    "optimizer = RAdam(model.parameters(), lr=1e-2)\n",
    "criterion = nn.BCEWithLogitsLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from catalyst.dl import SupervisedRunner\n",
    "\n",
    "\n",
    "runner = SupervisedRunner(input_key=[\"user_ids\", \"movie_ids\"], device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "runner.train(\n",
    "    model=model,\n",
    "    optimizer=optimizer,\n",
    "    loaders={\"train\": train_dataloader, \"valid\": valid_dataloader},\n",
    "    criterion=criterion,\n",
    "    callbacks=callbacks,\n",
    "    logdir=Path(\"logs\") / datetime.now().strftime(\"%Y%m%d-%H%M%S\"),\n",
    "    num_epochs=5,\n",
    "    verbose=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}