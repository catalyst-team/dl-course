{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "gans",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.7"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CtYNPhQaoCT_"
      },
      "source": [
        "# Generative Models\n",
        "\n",
        "Hi! Today we are going to learn about generative models. We'll continue to work with handwritten numbers, but we will generate with different models.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "db7ZGBuvoIKw"
      },
      "source": [
        "!pip install catalyst"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h2qZ-FsboCT_"
      },
      "source": [
        "from catalyst.utils import set_global_seed, get_device\n",
        "\n",
        "\n",
        "set_global_seed(42)\n",
        "device = get_device()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cFBzcQWwoCT_"
      },
      "source": [
        "We'll work with `MNIST` dataset. Download it, show examples of the writting and prepare the dataset to be loaded into models."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wL8RED6koCUA"
      },
      "source": [
        "from catalyst.contrib.datasets import mnist\n",
        "\n",
        "\n",
        "train = mnist.MNIST('.', train=True, download=True)\n",
        "valid = mnist.MNIST('.', train=False, download=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t_AYY5SzoCUA"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "_, axs = plt.subplots(4, 4, figsize=(10, 10))\n",
        "\n",
        "for i in range(16):\n",
        "    axs[i // 4][i % 4].imshow(train[100 * i + i][0])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4zNLPpljoCUA"
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qhSi2aedoCUA"
      },
      "source": [
        "import numpy as np\n",
        "import typing as tp\n",
        "from catalyst.utils import get_loader\n",
        "\n",
        "\n",
        "batch_size = 1024\n",
        "num_workers = 4\n",
        "\n",
        "def transform(x: np.array) -> tp.Dict[str, torch.Tensor]:\n",
        "    image = torch.FloatTensor(x['image'])\n",
        "    image = torch.where(image > 127, torch.ones(image.shape), torch.zeros(image.shape))\n",
        "    return {'image': image, 'targets': x['targets']}\n",
        "\n",
        "\n",
        "train_data_loader = get_loader(\n",
        "    train,\n",
        "    open_fn=lambda x : {'image': x[0].reshape(1, 28, 28), 'targets': x[1]},\n",
        "    dict_transform=transform,\n",
        "    batch_size=batch_size,\n",
        "    num_workers=num_workers,\n",
        "    shuffle=True,\n",
        "    sampler=None,\n",
        "    drop_last=True,\n",
        ")\n",
        "\n",
        "valid_data_loader = get_loader(\n",
        "    valid,\n",
        "    open_fn=lambda x : {'image': x[0].reshape(1, 28, 28), 'targets': x[1]},\n",
        "    dict_transform=transform,\n",
        "    batch_size=batch_size,\n",
        "    num_workers=num_workers,\n",
        "    shuffle=False,\n",
        "    sampler=None,\n",
        "    drop_last=False,\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cwaRij52oCUB"
      },
      "source": [
        "## GAN\n",
        "\n",
        "For GAN model, we need a Discriminator and a Generator. The Discriminator will judge generated images, how do they like real one. The Generator tries to fool the Discriminator.\n",
        "\n",
        "Notice that Generator is similar to Decoder in AE/VAE perspective. It will required later."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QppAiFFRoCUB"
      },
      "source": [
        "class Discriminator(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.feature_extractor = nn.Sequential(\n",
        "            nn.Conv2d(1, 4, kernel_size=3, padding=1),\n",
        "            nn.LeakyReLU(0.1),\n",
        "            nn.MaxPool2d(2,2),\n",
        "            nn.Conv2d(4, 16, kernel_size=3, padding=1),\n",
        "            nn.LeakyReLU(0.1),\n",
        "            nn.MaxPool2d(2,2),\n",
        "            nn.Conv2d(16, 64, kernel_size=3, padding=1),\n",
        "            nn.LeakyReLU(0.1),\n",
        "            nn.AdaptiveMaxPool2d((1,1)),\n",
        "            nn.Flatten())\n",
        "        self.clf = nn.Linear(64, 1)\n",
        "        \n",
        "    def forward(self, images: torch.Tensor) -> torch.Tensor:\n",
        "        features = self.feature_extractor(images)\n",
        "        return self.clf(features)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C1cAWstQoCUB"
      },
      "source": [
        "from catalyst.contrib.nn.modules import Lambda\n",
        "\n",
        "\n",
        "class Generator(nn.Module):\n",
        "    def __init__(\n",
        "        self,\n",
        "        image_size: tp.Tuple[int, int] = (28, 28),\n",
        "        latent_size: int = 10,\n",
        "    ):\n",
        "        super().__init__()\n",
        "        \n",
        "        self.image_size = image_size\n",
        "        self.latent_size = latent_size\n",
        "        \n",
        "        self.map_generator = nn.Sequential(\n",
        "            nn.Linear(latent_size, 64 * 49),\n",
        "            Lambda(lambda x: x.view(x.size(0), 64, 7, 7)),\n",
        "        )\n",
        "        self.deconv = nn.Sequential(\n",
        "            self.make_up_layer_(64, 16), # 7 -> 14\n",
        "            self.make_up_layer_(16, 4), # 14 -> 28\n",
        "        )\n",
        "            \n",
        "        self.output = nn.Sequential(\n",
        "            nn.Conv2d(4, 1, 3, padding=1),\n",
        "            nn.Sigmoid()\n",
        "        )\n",
        "            \n",
        "    def forward(self, points: torch.Tensor) -> torch.Tensor:\n",
        "        feature_map = self.map_generator(points)\n",
        "        feature_map = self.deconv(feature_map)\n",
        "        return self.output(feature_map)\n",
        "            \n",
        "    def make_up_layer_(\n",
        "        self,\n",
        "        in_channels: int,\n",
        "        out_channels: int\n",
        "    ) -> nn.Module:\n",
        "        return nn.Sequential(\n",
        "            nn.ConvTranspose2d(\n",
        "                    in_channels,\n",
        "                    out_channels,\n",
        "                    kernel_size=3,\n",
        "                    stride=2,\n",
        "                    padding=1,\n",
        "                    output_padding=1,\n",
        "            ),\n",
        "            nn.BatchNorm2d(out_channels),\n",
        "            nn.LeakyReLU(0.2),\n",
        "            nn.Conv2d(\n",
        "                    out_channels,\n",
        "                    out_channels,\n",
        "                    kernel_size=3,\n",
        "                    padding=1,\n",
        "            ),\n",
        "            nn.BatchNorm2d(out_channels),\n",
        "            nn.LeakyReLU(0.2),\n",
        "        )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SbHTsVWnoCUC"
      },
      "source": [
        "To monitor decoded images, we have to write a new callback function. It will log image into the tensorboard."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sBPP_oM5oCUC"
      },
      "source": [
        "from catalyst import dl\n",
        "from catalyst.core import Callback, CallbackOrder\n",
        "\n",
        "\n",
        "class LogFigureCallback(Callback):\n",
        "    def __init__(self):\n",
        "        super().__init__(CallbackOrder.External)\n",
        "\n",
        "    def on_epoch_end(self, runner):\n",
        "      logger = runner.loggers['_tensorboard']\n",
        "      logger = logger.loggers[runner.loader_key]\n",
        "      logger.add_images(f'image/epoch', runner.output['generated_images'][:64], global_step = runner.global_epoch_step)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ewegvr2BoCUC"
      },
      "source": [
        "Create model, criterion, optimizer. Train model!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E8LnTH0koCUD"
      },
      "source": [
        "from catalyst.contrib.nn.optimizers import RAdam\n",
        "\n",
        "\n",
        "generator_1 = Generator()\n",
        "discriminator = Discriminator()\n",
        "\n",
        "model = {'generator': generator_1, 'discriminator': discriminator} # set models\n",
        "optimizer = {\n",
        "    'generator': RAdam(generator_1.parameters(), lr=3e-3, betas=(0.5, 0.999)),\n",
        "    'discriminator': RAdam(discriminator.parameters(), lr=3e-3, betas=(0.5, 0.999)),\n",
        "} # set optimizers\n",
        "criterion = nn.BCEWithLogitsLoss()\n",
        "loaders = {\n",
        "    'train': train_data_loader,\n",
        "}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vt7Yj5eSoCUD"
      },
      "source": [
        "callbacks = [\n",
        "    dl.OptimizerCallback(\n",
        "        optimizer_key='generator', \n",
        "        metric_key='loss_generator',\n",
        "        model_key = 'generator'\n",
        "    ),\n",
        "    dl.OptimizerCallback(\n",
        "        optimizer_key='discriminator', \n",
        "        metric_key='loss_discriminator',\n",
        "        model_key = 'discriminator'\n",
        "    ),\n",
        "    LogFigureCallback()\n",
        "]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-VuQTlbdoCUD"
      },
      "source": [
        "Training process consists in two phases: discriminator and generator parts. The discriminator is differential metrics of 'fakeness'. So, it's trained to discriminate objects by `BinaryCrossEntropyLoss`. Because the discriminator is differential, we can pass knowledge about real images by backpropagation."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dS0vYz05oCUD"
      },
      "source": [
        "class GANRunner(dl.Runner):\n",
        "\n",
        "    def handle_batch(self, batch: tp.Dict[str, torch.Tensor]):\n",
        "        real_images = batch['image']\n",
        "        batch_metrics = {}\n",
        "        latent_size = self.model['generator'].latent_size\n",
        "        \n",
        "        # Sample random points in the latent space\n",
        "        batch_size = real_images.shape[0]\n",
        "        random_latent_vectors = torch.randn(batch_size, latent_size).to(self.device)\n",
        "        \n",
        "        # Generate fake images by random points\n",
        "        generated_images = self.model['generator'](random_latent_vectors).detach()\n",
        "        # Combine them with real images\n",
        "        combined_images = torch.cat([generated_images, real_images])\n",
        "        \n",
        "        # Assemble labels discriminating real from fake images\n",
        "        labels = torch.cat([\n",
        "            torch.ones((batch_size, 1)), torch.zeros((batch_size, 1))\n",
        "        ]).to(self.device)\n",
        "        \n",
        "        # Train the discriminator\n",
        "        predictions =  self.model['discriminator'](combined_images)\n",
        "        batch_metrics['loss_discriminator'] = self.criterion(predictions, labels)\n",
        "        \n",
        "        # Sample random points in the latent space\n",
        "        random_latent_vectors = torch.randn(batch_size, latent_size).to(self.device)\n",
        "        # Assemble labels that say 'all real images'\n",
        "        misleading_labels = torch.zeros((batch_size, 1)).to(self.device)\n",
        "        \n",
        "        # Train the generator\n",
        "        generated_images = self.model['generator'](random_latent_vectors)\n",
        "        self.output = {'generated_images': generated_images}\n",
        "        predictions = self.model['discriminator'](generated_images)\n",
        "        batch_metrics['loss_generator'] = self.criterion(predictions, misleading_labels)\n",
        "        \n",
        "        self.batch_metrics.update(**batch_metrics)\n",
        "\n",
        "\n",
        "runner = GANRunner()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zfiDKf3VoCUE"
      },
      "source": [
        "from datetime import datetime\n",
        "from pathlib import Path\n",
        "\n",
        "\n",
        "logdir = Path('logs') / datetime.now().strftime('%Y%m%d-%H%M%S')\n",
        "\n",
        "%reload_ext tensorboard\n",
        "%tensorboard --logdir logs"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2GgsFngXoCUE"
      },
      "source": [
        "runner.train(\n",
        "    model=model,\n",
        "    optimizer=optimizer,\n",
        "    criterion=criterion,\n",
        "    loaders=loaders,\n",
        "    callbacks=callbacks,\n",
        "    num_epochs=30,\n",
        "    verbose=True,\n",
        "    logdir=logdir\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ACatHF8ioCUE"
      },
      "source": [
        "## VAE-GAN\n",
        "\n",
        "Remember that the Discriminator is a Decoder? Let's add an Encoder in our training rutin:\n",
        "\n",
        "![](https://habrastorage.org/web/7a1/8db/d39/7a18dbd3969048c2b085cc707e539f0c.png)\n",
        "\n",
        "It will make latent space meaningfull.\n",
        "\n",
        "To train all these models, we need new loss function for encoder model. We need to compare results from the Discriminator with the real images. Instead from comparing images in the original sizes, we can compare feature maps from the Discriminator.\n",
        "\n",
        "\n",
        "https://arxiv.org/pdf/1512.09300.pdf"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AHcfHPcNoCUE"
      },
      "source": [
        "class Encoder(nn.Module):\n",
        "    def __init__(self, latent_size: int = 10):\n",
        "        super().__init__()\n",
        "        self.feature_extractor = nn.Sequential(\n",
        "            nn.Conv2d(1, 4, kernel_size=3, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
        "            nn.Conv2d(4, 16, kernel_size=3, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
        "            nn.Conv2d(16, 64, kernel_size=3, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.AdaptiveMaxPool2d((1, 1)),\n",
        "            nn.Flatten()\n",
        "        )\n",
        "        self.latent_space = nn.Linear(64, 2 * latent_size)\n",
        "        \n",
        "        self.latent_size = latent_size\n",
        "        \n",
        "    def forward(\n",
        "        self,\n",
        "        images: torch.Tensor\n",
        "    ) -> tp.Tuple[torch.Tensor, torch.Tensor]:\n",
        "        features = self.feature_extractor(images)\n",
        "        latent = self.latent_space(features)\n",
        "        return latent[:, :self.latent_size], latent[:, self.latent_size:]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LnbyWCPJoCUF"
      },
      "source": [
        "class Discriminator_v2(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.feature_extractor = nn.Sequential(\n",
        "        nn.Conv2d(1, 4, kernel_size=3, padding=1),\n",
        "        nn.LeakyReLU(0.1),\n",
        "        nn.MaxPool2d(2,2),\n",
        "        nn.Conv2d(4, 16, kernel_size=3, padding=1),\n",
        "        nn.LeakyReLU(0.1),\n",
        "        nn.MaxPool2d(2,2),\n",
        "        nn.Conv2d(16, 64, kernel_size=3, padding=1),\n",
        "        nn.LeakyReLU(0.1),\n",
        "        nn.AdaptiveMaxPool2d((1,1)),\n",
        "        nn.Flatten())\n",
        "        self.clf = nn.Linear(64, 1)\n",
        "        \n",
        "    def forward(\n",
        "        self,\n",
        "        images: torch.Tensor\n",
        "    ) -> tp.Tuple[torch.Tensor, torch.Tensor]:\n",
        "        features = self.feature_extractor(images)\n",
        "        return self.clf(features), features"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CA9FQ8EkoCUF"
      },
      "source": [
        "class KLVAELoss(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "    \n",
        "    def forward(\n",
        "        self,\n",
        "        loc: torch.Tensor,\n",
        "        log_scale: torch.Tensor\n",
        "    ) -> torch.Tensor:\n",
        "        return (-0.5 * torch.sum(log_scale - loc.pow(2) - log_scale.exp(), dim=1)).mean()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WFTSUSxToCUF"
      },
      "source": [
        "encoder = Encoder()\n",
        "generator_2 = Generator()\n",
        "discriminator = Discriminator_v2()\n",
        "\n",
        "model = {'generator': generator_2, 'discriminator': discriminator, 'encoder': encoder}# set models\n",
        "optimizer = {\n",
        "    'generator': RAdam(generator_2.parameters(), lr=3e-3, betas=(0.5, 0.999)),\n",
        "    'discriminator': RAdam(discriminator.parameters(), lr=3e-3, betas=(0.5, 0.999)),\n",
        "    'encoder': RAdam(encoder.parameters(), lr=3e-3, betas=(0.5, 0.999)),\n",
        "} # set optimizers\n",
        "criterion = {\n",
        "    'bce': nn.BCEWithLogitsLoss(),\n",
        "    'mse': nn.MSELoss(),\n",
        "    'vae': KLVAELoss()\n",
        "}\n",
        "loaders = {\n",
        "    'train': train_data_loader\n",
        "}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "evcNxQuHoCUG"
      },
      "source": [
        "\n",
        "callbacks = [\n",
        "    LogFigureCallback(),\n",
        "]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IgYPwZ58oCUG"
      },
      "source": [
        "Look the architecture and parameter update rutine:\n",
        "\n",
        "Model Architecture:\n",
        "![image](https://habrastorage.org/web/701/bbb/212/701bbb21273045fc9ed4aab7e0529764.png)\n",
        "\n",
        "Parameter Updates:\n",
        "![image](https://habrastorage.org/getpro/habr/post_images/07a/d0b/dc0/07ad0bdc0524f17cd4ae6c6e1be3c36d.svg)\n",
        "\n",
        "What we need to code?\n",
        "\n",
        "- Encoding training: compact latent space (KL Loss) and object reconstruction\n",
        "- Discriminator training: discriminate real from fake and reconstructed\n",
        "- Generator/decoder training: object reconstruction and fake object generator"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tfbOQXVBoCUG"
      },
      "source": [
        "LOG_SCALE_MAX = 2\n",
        "LOG_SCALE_MIN = -10\n",
        "\n",
        "\n",
        "def normal_sample(\n",
        "        loc: torch.Tensor,\n",
        "        log_scale: torch.Tensor\n",
        "    ) -> torch.Tensor:\n",
        "    scale = torch.exp(0.5 * log_scale)\n",
        "    return loc + scale * torch.randn_like(scale)\n",
        "\n",
        "\n",
        "class VAEGANRunner(dl.Runner):\n",
        "    def _zero_grad(self):\n",
        "        for optimizer in self.optimizer.values():\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "    def handle_batch(self, batch: tp.Dict[str, torch.Tensor]):\n",
        "        real_images = batch['image']\n",
        "        batch_metrics = {}\n",
        "        latent_size = self.model['generator'].latent_size\n",
        "        \n",
        "        # encoder-decoder part\n",
        "        latent_loc, latent_log_scale = self.model['encoder'](real_images)\n",
        "        loss_prior = self.criterion['vae'](latent_loc, latent_log_scale)\n",
        "            \n",
        "        latent_log_scale = torch.clamp(latent_log_scale, LOG_SCALE_MIN, LOG_SCALE_MAX)\n",
        "        latent_vectors = normal_sample(latent_loc, latent_log_scale)\n",
        "        \n",
        "        decoded_images = self.model['generator'](latent_vectors)\n",
        "        \n",
        "        _, predictions_decoded = self.model['discriminator'](decoded_images)\n",
        "        _, predictions_real = self.model['discriminator'](real_images)\n",
        "        loss_dislike = self.criterion['mse'](decoded_images, real_images)\n",
        "        \n",
        "        # generator part\n",
        "        \n",
        "        batch_size = real_images.shape[0]\n",
        "        random_latent_vectors = torch.randn(batch_size, latent_size).to(self.device)\n",
        "        \n",
        "        generated_images = self.model['generator'](random_latent_vectors)\n",
        "        \n",
        "        combined_images = torch.cat([generated_images, decoded_images])\n",
        "        labels = torch.zeros((2 * batch_size, 1)).to(self.device)\n",
        "        \n",
        "        self.output = {'generated_images': generated_images}\n",
        "        predictions, _ = self.model['discriminator'](combined_images)\n",
        "        loss_generator = self.criterion['bce'](predictions, labels)\n",
        "        \n",
        "        # discriminator part\n",
        "        \n",
        "        latent_vectors = normal_sample(latent_loc, latent_log_scale)\n",
        "        random_latent_vectors = torch.randn(batch_size, latent_size).to(self.device)\n",
        "        \n",
        "        decoded_images = self.model['generator'](latent_vectors)\n",
        "        generated_images = self.model['generator'](random_latent_vectors)\n",
        "        \n",
        "        combined_images = torch.cat([generated_images, decoded_images, real_images]).detach()\n",
        "        labels = torch.cat([\n",
        "            torch.ones((2 * batch_size, 1)), torch.zeros((batch_size, 1))\n",
        "        ]).to(self.device)\n",
        "        predictions, _ = self.model['discriminator'](combined_images)\n",
        "        loss_discriminator = self.criterion['bce'](predictions, labels)\n",
        "        \n",
        "        # closely look at the picture above and sum up losses for all part of VAE-GAN\n",
        "        batch_metrics['loss_encoder'] = loss_prior + loss_dislike\n",
        "        batch_metrics['loss_generator'] = loss_dislike + loss_generator\n",
        "        batch_metrics['loss_discriminator'] = loss_discriminator\n",
        "        if self.is_train_loader:\n",
        "            batch_metrics['loss_encoder'].backward(retain_graph=True)\n",
        "            optimizer['encoder'].step()\n",
        "            self._zero_grad()\n",
        "            \n",
        "            batch_metrics['loss_generator'].backward(retain_graph=True)\n",
        "            optimizer['generator'].step()\n",
        "            self._zero_grad()\n",
        "            \n",
        "            batch_metrics['loss_discriminator'].backward()\n",
        "            optimizer['discriminator'].step()\n",
        "            self._zero_grad()\n",
        "        \n",
        "        self.batch_metrics.update(**batch_metrics)\n",
        "\n",
        "\n",
        "runner = VAEGANRunner()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T8LB8pYAoCUG"
      },
      "source": [
        "logdir = Path('logs') / datetime.now().strftime('%Y%m%d-%H%M%S')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YNr63bFroCUG"
      },
      "source": [
        "runner.train(\n",
        "    model=model,\n",
        "    optimizer=optimizer,\n",
        "    criterion=criterion,\n",
        "    loaders=loaders,\n",
        "    callbacks=callbacks,\n",
        "    num_epochs=30,\n",
        "    verbose=True,\n",
        "    logdir=logdir\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7on_tBploCUH"
      },
      "source": [
        "How to compare models? Usually reseacher just closely look at the pictures. But they can use computer brains and eyes to compare generative models. Let's look at the one of them."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Cj4XqOvgoCUH"
      },
      "source": [
        "## Inception score\n",
        "\n",
        "Wait, wat? We can create numerical metric for generative models? Yes, we can. The metric called Inception score. To calculate it, we need the image classificator and generated images. Let's do it.\n",
        "\n",
        "\\*Inception score usually calculates for generators, trained on ImageNet/CIFAR. But we have no time to train GAN on these datasets. So, we will work with MNIST."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "24BjDF_ooCUH"
      },
      "source": [
        "clf_model = nn.Sequential(nn.Flatten(), nn.Linear(28 * 28, 10)).to(device)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = RAdam(clf_model.parameters(), lr=0.05)\n",
        "\n",
        "runner = dl.SupervisedRunner(\n",
        "    input_key='image',\n",
        "    output_key='logits',\n",
        "    target_key='targets', \n",
        ")\n",
        "runner.train(\n",
        "    model=clf_model, \n",
        "    optimizer=optimizer,\n",
        "    criterion=criterion,\n",
        "    callbacks=[\n",
        "        dl.AccuracyCallback(input_key='logits',\n",
        "                            target_key='targets')\n",
        "    ],\n",
        "    loaders={\n",
        "        'train': train_data_loader,\n",
        "        'valid': valid_data_loader\n",
        "    },\n",
        "    num_epochs=20,\n",
        "    verbose=False,\n",
        "    load_best_on_end = True\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ql-XzGKsoCUH"
      },
      "source": [
        "Inception Score fomulating by this:\n",
        "\n",
        "![](https://www.oreilly.com/library/view/generative-adversarial-networks/9781789136678/assets/0d33c46a-0a5f-4027-919c-30b910e6d93b.png)\n",
        "\n",
        "where $p(y)$ – is probability of class in a dataset, $p(y|x)$ – is probability of class of object."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_eYbSw36oCUH"
      },
      "source": [
        "from tqdm.notebook import trange\n",
        "\n",
        "\n",
        "def inception_score(model: nn.Module):\n",
        "    p_y = torch.ones(10).to(device) / 10\n",
        "    log_is = 0\n",
        "    num_images = 50000\n",
        "    batch_size = 100\n",
        "\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        for _ in trange(num_images // batch_size):\n",
        "            latent_size = model.latent_size\n",
        "            random_latent_points = torch.randn((batch_size, latent_size)).to(device)\n",
        "            generated = model(random_latent_points)\n",
        "            logits = clf_model(generated)\n",
        "            p_y_x = torch.softmax(logits, 1)\n",
        "            log_is += (torch.log(p_y_x) * p_y_x - torch.log(p_y) * p_y_x).sum(1)\n",
        "\n",
        "    log_is /= num_images\n",
        "    return torch.exp(log_is.mean())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yqumYMweoCUH"
      },
      "source": [
        "inception_score(generator_1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ilxhkEV5oCUH"
      },
      "source": [
        "inception_score(generator_2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9Vf3mk7_oCUI"
      },
      "source": [
        "## (Additional) Another point in VAE - GAN: AAE\n",
        "\n",
        "AAE stands for Adversarial Autoencoders. It is like a inverted VAE-GAN. A main model become AE, but we will use Discriminator to compare points in a latent space. Let's code it."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DUoMIwCKoCUI"
      },
      "source": [
        "class Disciriminator_v3(nn.Module):\n",
        "    def __init__(self, latent_size: int = 4):\n",
        "        super().__init__()\n",
        "        \n",
        "        self.latent_size = latent_size\n",
        "        \n",
        "        self.model = nn.Sequential(\n",
        "            nn.Linear(latent_size, 100),\n",
        "            nn.Dropout(0.1),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(100, 1),\n",
        "            nn.Sigmoid()\n",
        "        )\n",
        "        \n",
        "    def forward(self, z: torch.Tensor) -> torch.Tensor:\n",
        "        return self.model(z)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C37kyUopoCUI"
      },
      "source": [
        "class Encoder(nn.Module):\n",
        "    def __init__(self, latent_size: int = 10):\n",
        "        super().__init__()\n",
        "        self.feature_extractor = nn.Sequential(\n",
        "            nn.Conv2d(1, 4, kernel_size=3, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
        "            nn.Conv2d(4, 16, kernel_size=3, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
        "            nn.Conv2d(16, 64, kernel_size=3, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.AdaptiveMaxPool2d((1, 1)),\n",
        "            nn.Flatten()\n",
        "        )\n",
        "        self.latent_space = nn.Linear(64, latent_size)\n",
        "        \n",
        "        self.latent_size = latent_size\n",
        "        \n",
        "    def forward(self, images: torch.Tensor) -> torch.Tensor:\n",
        "        features = self.feature_extractor(images)\n",
        "        latent = self.latent_space(features)\n",
        "        # Instead of VAE, AAE use simple encoder\n",
        "        # because it can be trained by samples, not distributions.\n",
        "        return latent"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GUFNvefVoCUI"
      },
      "source": [
        "class Decoder(nn.Module):\n",
        "    def __init__(\n",
        "        self,\n",
        "        image_size: tp.Tuple[int, int] = (28, 28),\n",
        "        latent_size: int = 10\n",
        "    ):\n",
        "        super().__init__()\n",
        "        \n",
        "        self.image_size = image_size\n",
        "        self.latent_size = latent_size\n",
        "        \n",
        "        self.map_generator = nn.Sequential(\n",
        "            nn.Linear(latent_size, 64 * 49),\n",
        "            Lambda(lambda x: x.view(x.size(0), 64, 7, 7)),\n",
        "        )\n",
        "        self.deconv = nn.Sequential(\n",
        "            self.make_up_layer_(64, 16), # 7 -> 14\n",
        "            self.make_up_layer_(16, 4), # 14 -> 28\n",
        "        )\n",
        "            \n",
        "        self.output = nn.Sequential(\n",
        "            nn.Conv2d(4, 1, 3, padding=1),\n",
        "            nn.Sigmoid()\n",
        "        )\n",
        "            \n",
        "    def forward(self, points: torch.Tensor) -> torch.Tensor:\n",
        "        feature_map = self.map_generator(points)\n",
        "        feature_map = self.deconv(feature_map)\n",
        "        return self.output(feature_map)\n",
        "            \n",
        "    def make_up_layer_(\n",
        "        self,\n",
        "        in_channels: int,\n",
        "        out_channels: int\n",
        "    ) -> nn.Module:\n",
        "        return nn.Sequential(nn.ConvTranspose2d(\n",
        "                    in_channels,\n",
        "                    out_channels,\n",
        "                    kernel_size=3,\n",
        "                    stride=2,\n",
        "                    padding=1,\n",
        "                    output_padding=1,\n",
        "            ),\n",
        "            nn.BatchNorm2d(out_channels),\n",
        "            nn.Dropout(0.1),\n",
        "            nn.LeakyReLU(0.2),\n",
        "            )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IA6VwjqUoCUI"
      },
      "source": [
        "class AE(nn.Module):\n",
        "    def __init__(\n",
        "        self,\n",
        "        image_size: tp.Tuple[int, int] = (28, 28),\n",
        "        latent_size: int = 4\n",
        "    ):\n",
        "        super().__init__()\n",
        "\n",
        "        self.encoder = Encoder(latent_size)\n",
        "        self.decoder = Decoder(image_size, latent_size)\n",
        "        \n",
        "    def forward(self, images: torch.Tensor) -> tp.Dict[str, torch.Tensor]:\n",
        "        latent = self.encoder(images)\n",
        "        x_ = self.decoder(latent)\n",
        "\n",
        "        return {\n",
        "            'decoder_result': x_,\n",
        "            'latent': latent\n",
        "        }"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wBQltexPoCUI"
      },
      "source": [
        "autoencoder = AE()\n",
        "discriminator = Disciriminator_v3()\n",
        "model = {'autoencoder': autoencoder, 'discriminator': discriminator}\n",
        "optimizer = {\n",
        "    'autoencoder': RAdam(autoencoder.parameters(), lr=1e-3), \n",
        "    'discriminator': RAdam(discriminator.parameters(), lr=1e-4),\n",
        "}\n",
        "criterion = nn.BCELoss()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7Jfa-fxGoCUI"
      },
      "source": [
        "callbacks = [\n",
        "      LogFigureCallback()\n",
        "    ]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f9p40vppoCUJ"
      },
      "source": [
        "class AAERunner(dl.Runner):\n",
        "    def _zero_grad(self):\n",
        "        for optimizer in self.optimizer.values():\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "    def handle_batch(self, batch: tp.Dict[str, torch.Tensor]):\n",
        "        real_images = batch['image']\n",
        "        batch_metrics = {}\n",
        "        latent_size = self.model['discriminator'].latent_size\n",
        "        batch_size = real_images.size(0)\n",
        "        \n",
        "        output = self.model['autoencoder'](real_images)\n",
        "        fake_latent = output['latent']\n",
        "        decoded_images = output['decoder_result']\n",
        "        \n",
        "        self.output = {'generated_images': decoded_images}\n",
        "        normal_latent = torch.randn(fake_latent.size()).to(self.device)\n",
        "        \n",
        "        loss_ae = self.criterion(\n",
        "            decoded_images.reshape(batch_size, -1), \n",
        "            real_images.reshape(batch_size, -1)\n",
        "        )\n",
        "        \n",
        "        concat_latent = torch.cat([normal_latent, fake_latent]).detach()\n",
        "        target = torch.cat(\n",
        "            [torch.ones(batch_size), torch.zeros(batch_size)]\n",
        "        ).to(self.device)\n",
        "        pred = self.model['discriminator'](concat_latent)\n",
        "        batch_metrics['loss_discriminator'] = self.criterion(\n",
        "            pred.reshape(-1), target\n",
        "        )\n",
        "        \n",
        "        fake_pred = self.model['discriminator'](fake_latent)\n",
        "        fake_target = torch.ones(batch_size).to(self.device)\n",
        "        loss_fake_latent = self.criterion(\n",
        "            fake_pred.reshape(-1), fake_target\n",
        "        )\n",
        "        \n",
        "        batch_metrics['loss_autoencoder'] = loss_ae + loss_fake_latent\n",
        "        \n",
        "        if self.is_train_loader:\n",
        "            optimizer['discriminator'].zero_grad()\n",
        "            batch_metrics['loss_discriminator'].backward(retain_graph=True)\n",
        "            optimizer['discriminator'].step()\n",
        "            \n",
        "            optimizer['autoencoder'].zero_grad()\n",
        "            batch_metrics['loss_autoencoder'].backward()\n",
        "            optimizer['autoencoder'].step()\n",
        "        \n",
        "        self.batch_metrics.update(**batch_metrics)\n",
        "\n",
        "\n",
        "runner = AAERunner()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uNb7eTqwoCUJ"
      },
      "source": [
        "logdir = Path('logs') / datetime.now().strftime('%Y%m%d-%H%M%S')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "yryrE53voCUJ"
      },
      "source": [
        "runner.train(\n",
        "    model=model,\n",
        "    optimizer=optimizer,\n",
        "    criterion=criterion,\n",
        "    loaders=loaders,\n",
        "    callbacks=callbacks,\n",
        "    num_epochs=1000,\n",
        "    verbose=True,\n",
        "    logdir=logdir\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "jl9Oz9M_gRF7"
      },
      "source": [
        "%reload_ext tensorboard\n",
        "%tensorboard --logdir logs"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "mDBtBlHnrpWn"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}