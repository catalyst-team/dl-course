{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "v43NNTrWcmBB"
   },
   "outputs": [],
   "source": [
    "!pip install -qU datasets transformers catalyst==20.12 sacrebleu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "f8fyR2ZFcmBQ"
   },
   "outputs": [],
   "source": [
    "from catalyst.utils import set_global_seed, get_device\n",
    "\n",
    "\n",
    "set_global_seed(42)\n",
    "device = get_device()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jOoYLE_KcmBc"
   },
   "source": [
    "# Homework\n",
    "\n",
    "Hi! Today we are going to create Neural Machine Translation system! It will read english sentences and write their tranlsations on German. This is the Seq2Seq task. So, we need an Encoder and a Decoder models. You can use any architecture and pretrained models. But you have several rules:\n",
    "\n",
    "- Not copy your classmates works\n",
    "- If you will use pretrained weights, check that this weights wasn't gotten by solving Machine Translation task.\n",
    "- Your model score on BLEU must be higher that `0.15` points in a testing pipeline.\n",
    "\n",
    "To get 10 points (full score), your model have to get `0.20` points BLEU on a test dataset: test part from WMT14.\n",
    "\n",
    "\n",
    "!! **WARNING** !!\n",
    "\n",
    "You need several **HOURS** to train models for translation task!\n",
    "\n",
    "!! **WARNING** !!\n",
    "\n",
    "\n",
    "There is a basic model, that you can use as a starting point to create your solution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Y_igMe04tBDW"
   },
   "outputs": [],
   "source": [
    "# You can use google drive to save data and best models\n",
    "# Only work in colab\n",
    "\n",
    "# from google.colab import drive\n",
    "# drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IWHpEHGBcmBe"
   },
   "outputs": [],
   "source": [
    "from datasets import load_dataset, load_metric"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's basic datasets: WMT14. For a training process, you can add any additional samples. But you need to leave `wmt14_test` and metric `sacrebleu` unchangable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ZL9dGeVRcmBl",
    "outputId": "c721ad78-85f5-4fdb-d2f8-7b172040d36f"
   },
   "outputs": [],
   "source": [
    "cache_dir = None\n",
    "\n",
    "wmt14_train = load_dataset(\n",
    "    \"wmt14\", \"de-en\", split=\"train[:20%]\", cache_dir=cache_dir\n",
    ")\n",
    "wmt14_valid = load_dataset(\n",
    "    \"wmt14\", \"de-en\", split=\"validation\", cache_dir=cache_dir\n",
    ")\n",
    "\n",
    "# DO NOT TOUCH!\n",
    "wmt14_test = load_dataset(\"wmt14\", \"de-en\", split=\"test\", cache_dir=cache_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ITccYsTicmBp",
    "outputId": "124679f9-14f5-4546-86cc-281ce4172286"
   },
   "outputs": [],
   "source": [
    "bleu = load_metric('sacrebleu')\n",
    "print(bleu)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this baseline, we encorage you to use pretrained models. The easiest way to get them: by HuggingFaceðŸ¤— library. Next code blocks contain models preparation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "amLlH8NOcmBw"
   },
   "outputs": [],
   "source": [
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    EncoderDecoderModel,\n",
    "    DataCollatorForLanguageModeling,\n",
    "    DataCollatorWithPadding,\n",
    "    default_data_collator\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Regardless of the fact that our target language is German, we will use a model trained on English texts. These models are easier to find and add in the pipeline. And German and English languages are very similiar, too."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "S-s_PzCeaCeu"
   },
   "outputs": [],
   "source": [
    "source_model = \"google/bert_uncased_L-6_H-128_A-2\"\n",
    "target_model = \"google/bert_uncased_L-6_H-128_A-2\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NR_ueYSdcmBy"
   },
   "outputs": [],
   "source": [
    "tokenizer_source = AutoTokenizer.from_pretrained(source_model)\n",
    "tokenizer_target = AutoTokenizer.from_pretrained(target_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PgIkaNIB4cgm"
   },
   "outputs": [],
   "source": [
    "import typing as tp\n",
    "\n",
    "from catalyst.utils import get_loader\n",
    "\n",
    "\n",
    "collate_fn_source = DataCollatorWithPadding(tokenizer_source)\n",
    "collate_fn_target = DataCollatorForLanguageModeling(\n",
    "    tokenizer_target, mlm=False\n",
    ")\n",
    "\n",
    "def collate_fn(\n",
    "        batch: tp.Sequence[tp.Dict[str, tp.Any]]\n",
    ") -> tp.Tuple[tp.Dict[str, tp.Any], tp.Dict[str, tp.Any]]:\n",
    "    batch_source = collate_fn_source([b[\"source\"] for b in batch])\n",
    "    batch_target = collate_fn_source([b[\"target\"] for b in batch])\n",
    "    return batch_source, batch_target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HxrGpfzGcmB2"
   },
   "outputs": [],
   "source": [
    "max_length = 64\n",
    "\n",
    "def text_data_transforms(\n",
    "        row: tp.Dict[str, tp.Any]\n",
    ") -> tp.Dict[str, tp.Dict[str, tp.Any]]:\n",
    "    source = row[\"translation\"][\"en\"]\n",
    "    target = row[\"translation\"][\"de\"]\n",
    "    source_tokens = tokenizer_source.encode_plus(\n",
    "        source, max_length=max_length, truncation=True, padding=\"max_length\"\n",
    "    )\n",
    "    target_tokens = tokenizer_target.encode_plus(\n",
    "        target, max_length=max_length, truncation=True,\n",
    "    )\n",
    "    return {\"source\": source_tokens, \"target\": target_tokens}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wQTL7VBCcmCC"
   },
   "outputs": [],
   "source": [
    "train_dataloader = get_loader(\n",
    "    wmt14_train,\n",
    "    open_fn=lambda x: x,\n",
    "    dict_transform=text_data_transforms,\n",
    "    batch_size=128,\n",
    "    num_workers=4,\n",
    "    shuffle=True,\n",
    "    drop_last=True,\n",
    "    collate_fn=collate_fn,\n",
    ")\n",
    "\n",
    "valid_dataloader = get_loader(\n",
    "    wmt14_valid,\n",
    "    open_fn=lambda x: x,\n",
    "    dict_transform=text_data_transforms,\n",
    "    batch_size=128,\n",
    "    num_workers=4,\n",
    "    shuffle=True,\n",
    "    drop_last=True,\n",
    "    collate_fn=collate_fn,\n",
    ")\n",
    "\n",
    "test_dataloader = get_loader(\n",
    "    wmt14_test,\n",
    "    open_fn=lambda x: x,\n",
    "    dict_transform=text_data_transforms,\n",
    "    batch_size=128,\n",
    "    num_workers=4,\n",
    "    shuffle=True,\n",
    "    drop_last=True,\n",
    "    collate_fn=collate_fn,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Two `BERT` models are used to create one Seq2Seq model. HuggingFace gave us elegant way to do so. More explanation in [docs](https://huggingface.co/transformers/model_doc/encoderdecoder.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3QAp_YHUcmCF",
    "outputId": "4360fce7-bf54-4856-f21e-b0939639af69"
   },
   "outputs": [],
   "source": [
    "model = EncoderDecoderModel.from_encoder_decoder_pretrained(\n",
    "    source_model, target_model\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "P6hICooWfe-X"
   },
   "outputs": [],
   "source": [
    "# from torch.utils set_requires_grad\n",
    "\n",
    "# You can freeze some parameters in models to speed up training loops.\n",
    "\n",
    "# set_requires_grad(model.encoder.embeddings, False)\n",
    "# set_requires_grad(model.decoder.bert.embeddings, False)\n",
    "\n",
    "# set_requires_grad(model.encoder.encoder.layer[3:], False)\n",
    "# set_requires_grad(model.decoder.bert.encoder.layer[3:], False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next there are some not commented example of the model API:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HCQUfGSBcmCI"
   },
   "outputs": [],
   "source": [
    "batch = next(iter(train_dataloader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-PygJtlUcmCL"
   },
   "outputs": [],
   "source": [
    "outputs = model(\n",
    "    input_ids=batch[0][\"input_ids\"], \n",
    "    attention_mask=batch[0][\"attention_mask\"],\n",
    "    decoder_input_ids=batch[1][\"input_ids\"],\n",
    "    decoder_attention_mask=batch[1][\"attention_mask\"],\n",
    "    labels=batch[1][\"input_ids\"],\n",
    "    return_dict=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "NLwHzLpBcmCN",
    "outputId": "2a3fb758-dead-4f4a-faa4-8f9327f35370"
   },
   "outputs": [],
   "source": [
    "outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QIztQgegcmCP"
   },
   "outputs": [],
   "source": [
    "logits = outputs[\"logits\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fptng485cmCS"
   },
   "outputs": [],
   "source": [
    "decoded_reference = tokenizer_target.decode(batch[1][\"input_ids\"][0])\n",
    "decoded_hypothesis = tokenizer_target.decode(logits[0].max(0)[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our main metric: BLEU. To show your performance, we will use ScareBLEU as common implementation.\n",
    "It's very slow, that's why we haven't added this is the training loop. And it's API works like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ptoZ7_gwcmCW"
   },
   "outputs": [],
   "source": [
    "bleu.add_batch(\n",
    "    predictions=[decoded_hypothesis], references=[[decoded_reference]],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2c9-FEtHcmCZ",
    "outputId": "15bc23c2-75d9-40b5-c6a2-18f7717e37ee"
   },
   "outputs": [],
   "source": [
    "bleu.compute()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Typical Catalyst routune located here:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bpQP4gplcmCd"
   },
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "from catalyst.dl import SchedulerCallback\n",
    "from catalyst.contrib.nn import RAdam\n",
    "\n",
    "from transformers import get_linear_schedule_with_warmup\n",
    "\n",
    "\n",
    "optimizer = RAdam(model.parameters(), 1e-3)\n",
    "scheduler = get_linear_schedule_with_warmup(\n",
    "    optimizer,\n",
    "    num_warmup_steps=100,\n",
    "    num_training_steps=1 * len(train_dataloader),\n",
    ")\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "callbacks = [SchedulerCallback(mode=\"batch\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Hgzb_QbgcmCg"
   },
   "outputs": [],
   "source": [
    "import typing as tp\n",
    "\n",
    "import torch\n",
    "\n",
    "from catalyst.dl import Runner\n",
    "\n",
    "\n",
    "class TranslationRunner(Runner):\n",
    "    # This function will be used to test you models. Don't forget about it!\n",
    "    def predict_batch(\n",
    "        self,\n",
    "        batch: tp.Tuple[\n",
    "            tp.Dict[str, torch.Tensor], tp.Dict[str, torch.Tensor]\n",
    "        ],\n",
    "    ) -> None:\n",
    "        output = model.generate(\n",
    "            input_ids=batch[0][\"input_ids\"].to(self.device),\n",
    "            decoder_start_token_id=tokenizer_target.cls_token_id,\n",
    "            max_length=max_length,\n",
    "        )\n",
    "        # output: not the logits, but token ids, already have been chosen by the model\n",
    "        # target: ground truth\n",
    "        return {\"output\": output, \"target\": batch[1][\"input_ids\"]}\n",
    "\n",
    "    def _handle_batch(\n",
    "        self,\n",
    "        batch: tp.Tuple[\n",
    "            tp.Dict[str, torch.Tensor], tp.Dict[str, torch.Tensor]\n",
    "        ],\n",
    "    ) -> None:\n",
    "        source = batch[0]\n",
    "        target = batch[1]\n",
    "\n",
    "        output = model(\n",
    "            input_ids=source[\"input_ids\"],\n",
    "            attention_mask=source[\"attention_mask\"],\n",
    "            decoder_input_ids=target[\"input_ids\"],\n",
    "            decoder_attention_mask=target[\"attention_mask\"],\n",
    "            labels=target[\"input_ids\"],\n",
    "            return_dict=True,\n",
    "        )\n",
    "        loss, logits = output.loss, output.logits\n",
    "\n",
    "        self.input = {\"source\": source, \"target\": target}\n",
    "        self.output = logits\n",
    "\n",
    "        if self.is_train_loader:\n",
    "            self.optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            self.optimizer.step()\n",
    "\n",
    "        self.batch_metrics.update({\"loss\": loss})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4gL4_liQcmCi",
    "outputId": "2cd431da-abb5-4dbc-a328-fcd417094bd6"
   },
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "\n",
    "\n",
    "runner = TranslationRunner()\n",
    "runner.train(\n",
    "    model=model,\n",
    "    optimizer=optimizer,\n",
    "    scheduler=scheduler,\n",
    "    loaders={\"train\": train_dataloader, \"valid\": valid_dataloader},\n",
    "    criterion=criterion,\n",
    "    callbacks=callbacks,\n",
    "    logdir=Path(\"logs\") / datetime.now().strftime(\"%Y%m%d-%H%M%S\"),\n",
    "    num_epochs=1,\n",
    "    verbose=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next block contains testing pipeline. You can't change it, but you can use it to check your model performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 424,
     "referenced_widgets": [
      "de7479e2ac0040c1b39a3ee3c0182b80",
      "b9f784d0f93c4dcfb2a00259da1cb44d",
      "f912af1eb2cc4ab789d8ee518bf8a8ee",
      "cac01781f8444734a6734738629fa090",
      "25af2d1bced14f9fb6f63b7687311999",
      "4c03eac16eea4295b906f867544884a8",
      "475bc16928704742bd0b0b7f3680691c",
      "5668900e0dab4acb87f00f0304ff0eeb"
     ]
    },
    "id": "h-VajXsNcmCj",
    "outputId": "8f0acd00-7131-4696-c338-3367ae5dffb3"
   },
   "outputs": [],
   "source": [
    "from tqdm.notebook import tqdm\n",
    "\n",
    "\n",
    "for output in tqdm(\n",
    "    runner.predict_loader(loader=test_dataloader), total=len(test_dataloader)\n",
    "):\n",
    "    # If you change tokenizer type, you will be allowed to change these code\n",
    "    ####\n",
    "    hypothesis = [\n",
    "        tokenizer_target.decode(o, skip_special_tokens=True)  # here\n",
    "        for o in output[\"output\"]\n",
    "    ]\n",
    "    references = [\n",
    "        [tokenizer_target.decode(o, skip_special_tokens=True)]  # and here\n",
    "        for o in output[\"target\"]\n",
    "    ]\n",
    "    ####\n",
    "    bleu.add_batch(\n",
    "        predictions=hypothesis, references=references,\n",
    "    )\n",
    "print(f\"Test BLEU: {bleu.compute()['score']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Several ideas, that you can use to upgrade your model:\n",
    "\n",
    "- Change model type/architecture/config\n",
    "- Tune hyperparameters\n",
    "- Change generate process to BEAM search. link: https://en.wikipedia.org/wiki/Beam_search\n",
    "\n",
    "Feel free to delete almost entire the notebook. However, you **have no permission** to change the testing pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "name": "Untitled-2.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "25af2d1bced14f9fb6f63b7687311999": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": "initial"
     }
    },
    "475bc16928704742bd0b0b7f3680691c": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "4c03eac16eea4295b906f867544884a8": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "5668900e0dab4acb87f00f0304ff0eeb": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "b9f784d0f93c4dcfb2a00259da1cb44d": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "cac01781f8444734a6734738629fa090": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_5668900e0dab4acb87f00f0304ff0eeb",
      "placeholder": "â€‹",
      "style": "IPY_MODEL_475bc16928704742bd0b0b7f3680691c",
      "value": " 0/93 [00:01&lt;?, ?it/s]"
     }
    },
    "de7479e2ac0040c1b39a3ee3c0182b80": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_f912af1eb2cc4ab789d8ee518bf8a8ee",
       "IPY_MODEL_cac01781f8444734a6734738629fa090"
      ],
      "layout": "IPY_MODEL_b9f784d0f93c4dcfb2a00259da1cb44d"
     }
    },
    "f912af1eb2cc4ab789d8ee518bf8a8ee": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "danger",
      "description": "  0%",
      "description_tooltip": null,
      "layout": "IPY_MODEL_4c03eac16eea4295b906f867544884a8",
      "max": 93,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_25af2d1bced14f9fb6f63b7687311999",
      "value": 0
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}